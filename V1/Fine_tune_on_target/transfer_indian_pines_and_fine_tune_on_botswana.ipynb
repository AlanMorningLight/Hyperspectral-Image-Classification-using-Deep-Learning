{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_indian_pines_and_fine_tune_on_botswana.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STppFn_nhvYi","executionInfo":{"status":"ok","timestamp":1608529225758,"user_tz":480,"elapsed":6283,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"8e6faeb8-e076-45f4-f5ce-af0b00cb3beb"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7NsWimZvhqNW","executionInfo":{"status":"ok","timestamp":1608532147169,"user_tz":480,"elapsed":510,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["import os\n","os.chdir('/content/drive/My Drive/Hyperspectral_Image_Classification')"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBRyZQ0MhrSs","executionInfo":{"status":"ok","timestamp":1608532150834,"user_tz":480,"elapsed":3089,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["from sample_extraction_V1_utils import *\n","import scipy.io as sio"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hAWER3kezHmg"},"source":["#  Load Target Dataset - Botswana"]},{"cell_type":"code","metadata":{"id":"6cRTmsNsymp3","executionInfo":{"status":"ok","timestamp":1608532153268,"user_tz":480,"elapsed":1572,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["uBotswana = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/Botswana.mat')\n","gt_Botswana = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/Botswana_gt.mat')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Egv2O79ry9fd","executionInfo":{"status":"ok","timestamp":1608532153269,"user_tz":480,"elapsed":1131,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["data = uBotswana['Botswana']\n","ground_truth = gt_Botswana['Botswana_gt']"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4hhJBE3_zxI","executionInfo":{"status":"ok","timestamp":1608532153270,"user_tz":480,"elapsed":672,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"353ae31b-9429-4587-e8c4-d619c81b4108"},"source":["data.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1476, 256, 145)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grlLMWu8zSu5","executionInfo":{"status":"ok","timestamp":1608532153271,"user_tz":480,"elapsed":288,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"b515a395-3481-4e2c-e910-2bc23f4ea245"},"source":["ground_truth.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1476, 256)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"et9yKgNo4j2b"},"source":["# Distribution of Samples in target dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"id":"AYdydfZF4gwC","executionInfo":{"status":"ok","timestamp":1608532156839,"user_tz":480,"elapsed":801,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"de3eca23-035b-49ec-bc21-82a4d7662288"},"source":["class_distribution = pd.DataFrame(np.unique(ground_truth, return_counts = True))\n","class_distribution = class_distribution.transpose()\n","class_distribution.columns = ['class','samples']\n","class_distribution"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>374608</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>270</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>101</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>251</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>215</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>269</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>269</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>259</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>203</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>314</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>248</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>305</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>181</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>268</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>95</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    class  samples\n","0       0   374608\n","1       1      270\n","2       2      101\n","3       3      251\n","4       4      215\n","5       5      269\n","6       6      269\n","7       7      259\n","8       8      203\n","9       9      314\n","10     10      248\n","11     11      305\n","12     12      181\n","13     13      268\n","14     14       95"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PsaQ_1FH4pa8","executionInfo":{"status":"ok","timestamp":1608532158103,"user_tz":480,"elapsed":352,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"0692987d-e0ec-4396-b521-08fe56bb25eb"},"source":["classes , counts = np.unique(ground_truth, return_counts = True)\n","classes = classes[1:] ## Not considering background\n","classes"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14],\n","      dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"HCUWkZcyzVK9"},"source":["# Load saved source model trained on Indian Pines"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkvxK5MgCa0s","executionInfo":{"status":"ok","timestamp":1608532620610,"user_tz":480,"elapsed":413575,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"7b9ceb31-ac04-43c2-9114-d6352657ee52"},"source":["transfer_results, confustion_matrixes = transfer_learning(source_dataset = 'indian_pines',\n","                                                          target_dataset = 'botswana',\n","                                                          data = data,\n","                                                          ground_truth = ground_truth,\n","                                                          training_samples_from_each_class = [30,40,50,60,70],\n","                                                          source_training_size = [200, 250, 300],\n","                                                          classes = classes,\n","                                                          overlap_ratio = 1,\n","                                                          channels = 64,\n","                                                          cube_size = 20,\n","                                                          learning_rate = 0.0001,\n","                                                          epochs = 100,\n","                                                          batch_size = 20,\n","                                                          test_accuracies = [])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","28/28 [==============================] - 0s 7ms/step - loss: 1.0016 - categorical_accuracy: 0.9796 - val_loss: 1.5466 - val_categorical_accuracy: 0.5724\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.59636\n","Epoch 80/100\n","28/28 [==============================] - 0s 7ms/step - loss: 1.0326 - categorical_accuracy: 0.9886 - val_loss: 1.5470 - val_categorical_accuracy: 0.5747\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.59636\n","Epoch 81/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.9724 - categorical_accuracy: 0.9752 - val_loss: 1.5438 - val_categorical_accuracy: 0.5882\n","\n","Epoch 00081: val_categorical_accuracy did not improve from 0.59636\n","Epoch 82/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.9883 - categorical_accuracy: 0.9843 - val_loss: 1.5381 - val_categorical_accuracy: 0.5759\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.59636\n","Epoch 83/100\n","28/28 [==============================] - 0s 7ms/step - loss: 1.0131 - categorical_accuracy: 0.9732 - val_loss: 1.5328 - val_categorical_accuracy: 0.5952\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.59636\n","Epoch 84/100\n","28/28 [==============================] - 0s 8ms/step - loss: 1.0095 - categorical_accuracy: 0.9714 - val_loss: 1.5349 - val_categorical_accuracy: 0.5774\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.59636\n","Epoch 85/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.9522 - categorical_accuracy: 0.9768 - val_loss: 1.5293 - val_categorical_accuracy: 0.5782\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.59636\n","Epoch 86/100\n","28/28 [==============================] - 0s 9ms/step - loss: 0.9697 - categorical_accuracy: 0.9825 - val_loss: 1.5264 - val_categorical_accuracy: 0.5871\n","\n","Epoch 00086: val_categorical_accuracy did not improve from 0.59636\n","Epoch 87/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.9132 - categorical_accuracy: 0.9901 - val_loss: 1.5260 - val_categorical_accuracy: 0.5875\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.59636\n","Epoch 88/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.9575 - categorical_accuracy: 0.9708 - val_loss: 1.5177 - val_categorical_accuracy: 0.5844\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.59636\n","Epoch 89/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.9481 - categorical_accuracy: 0.9826 - val_loss: 1.5178 - val_categorical_accuracy: 0.5948\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.59636\n","Epoch 90/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.9134 - categorical_accuracy: 0.9801 - val_loss: 1.5146 - val_categorical_accuracy: 0.5797\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.59636\n","Epoch 91/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.9343 - categorical_accuracy: 0.9913 - val_loss: 1.5140 - val_categorical_accuracy: 0.5909\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.59636\n","Epoch 92/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.9260 - categorical_accuracy: 0.9893 - val_loss: 1.5100 - val_categorical_accuracy: 0.5786\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.59636\n","Epoch 93/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.9000 - categorical_accuracy: 0.9856 - val_loss: 1.5078 - val_categorical_accuracy: 0.5975\n","\n","Epoch 00093: val_categorical_accuracy improved from 0.59636 to 0.59752, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_40_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 94/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.9216 - categorical_accuracy: 0.9892 - val_loss: 1.5038 - val_categorical_accuracy: 0.5983\n","\n","Epoch 00094: val_categorical_accuracy improved from 0.59752 to 0.59830, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_40_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 95/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.9267 - categorical_accuracy: 0.9824 - val_loss: 1.5043 - val_categorical_accuracy: 0.5960\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.59830\n","Epoch 96/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.9179 - categorical_accuracy: 0.9819 - val_loss: 1.5032 - val_categorical_accuracy: 0.5956\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.59830\n","Epoch 97/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.9008 - categorical_accuracy: 0.9687 - val_loss: 1.4962 - val_categorical_accuracy: 0.5867\n","\n","Epoch 00097: val_categorical_accuracy did not improve from 0.59830\n","Epoch 98/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.8806 - categorical_accuracy: 0.9789 - val_loss: 1.4935 - val_categorical_accuracy: 0.5909\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.59830\n","Epoch 99/100\n","28/28 [==============================] - 0s 7ms/step - loss: 0.8739 - categorical_accuracy: 0.9877 - val_loss: 1.4921 - val_categorical_accuracy: 0.5789\n","\n","Epoch 00099: val_categorical_accuracy did not improve from 0.59830\n","Epoch 100/100\n","28/28 [==============================] - 0s 8ms/step - loss: 0.8580 - categorical_accuracy: 0.9889 - val_loss: 1.4898 - val_categorical_accuracy: 0.6118\n","\n","Epoch 00100: val_categorical_accuracy improved from 0.59830 to 0.61184, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_40_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","81/81 [==============================] - 0s 2ms/step - loss: 1.4898 - categorical_accuracy: 0.6118\n","81/81 [==============================] - 0s 1ms/step\n","\n","================================================================================================================================\n","Model training starts for data with 50 samples from each class in training set\n","=================================================================================================================================\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         concatenate[0][0]                \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 5, 5, 8, 16)  0           activation_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 5, 5, 16, 8)  0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 5, 5, 128)    0           tf.reshape_1[0][0]               \n","                                                                 activation_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 700.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [50 50 50 50 50 50 50 50 50 50 50 50 50 50]\n","\n","Total number of samples in test set 2444.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [220  51 201 165 219 219 209 153 264 198 226  83 191  45]\n","\n","X_train_transfer => (700, 128)\n","X_test_transfer  => (2444, 128)\n","y_train => (700, 14)\n","y_test  => (2444, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_7 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","35/35 [==============================] - 1s 10ms/step - loss: 2.7333 - categorical_accuracy: 0.0046 - val_loss: 2.6894 - val_categorical_accuracy: 0.0266\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.02660, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.6180 - categorical_accuracy: 0.0788 - val_loss: 2.6172 - val_categorical_accuracy: 0.0282\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.02660 to 0.02823, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.5380 - categorical_accuracy: 0.0639 - val_loss: 2.5710 - val_categorical_accuracy: 0.0315\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.02823 to 0.03151, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.4868 - categorical_accuracy: 0.0920 - val_loss: 2.5334 - val_categorical_accuracy: 0.0716\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.03151 to 0.07160, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.4454 - categorical_accuracy: 0.1424 - val_loss: 2.5027 - val_categorical_accuracy: 0.1403\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.07160 to 0.14034, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 6/100\n","35/35 [==============================] - 0s 5ms/step - loss: 2.3837 - categorical_accuracy: 0.2155 - val_loss: 2.4821 - val_categorical_accuracy: 0.1686\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.14034 to 0.16858, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.3789 - categorical_accuracy: 0.2413 - val_loss: 2.4553 - val_categorical_accuracy: 0.1764\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.16858 to 0.17635, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 8/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.3385 - categorical_accuracy: 0.2672 - val_loss: 2.4341 - val_categorical_accuracy: 0.1792\n","\n","Epoch 00008: val_categorical_accuracy improved from 0.17635 to 0.17921, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 9/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.2940 - categorical_accuracy: 0.2884 - val_loss: 2.4188 - val_categorical_accuracy: 0.1866\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.17921 to 0.18658, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 10/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.2813 - categorical_accuracy: 0.2694 - val_loss: 2.3974 - val_categorical_accuracy: 0.1939\n","\n","Epoch 00010: val_categorical_accuracy improved from 0.18658 to 0.19394, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 11/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.2508 - categorical_accuracy: 0.2791 - val_loss: 2.3774 - val_categorical_accuracy: 0.2029\n","\n","Epoch 00011: val_categorical_accuracy improved from 0.19394 to 0.20295, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 12/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.2361 - categorical_accuracy: 0.2929 - val_loss: 2.3603 - val_categorical_accuracy: 0.2140\n","\n","Epoch 00012: val_categorical_accuracy improved from 0.20295 to 0.21399, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 13/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.1814 - categorical_accuracy: 0.3201 - val_loss: 2.3456 - val_categorical_accuracy: 0.2189\n","\n","Epoch 00013: val_categorical_accuracy improved from 0.21399 to 0.21890, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 14/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.1894 - categorical_accuracy: 0.3078 - val_loss: 2.3292 - val_categorical_accuracy: 0.2308\n","\n","Epoch 00014: val_categorical_accuracy improved from 0.21890 to 0.23077, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 15/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.1808 - categorical_accuracy: 0.2832 - val_loss: 2.3129 - val_categorical_accuracy: 0.2287\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.23077\n","Epoch 16/100\n","35/35 [==============================] - 0s 7ms/step - loss: 2.1456 - categorical_accuracy: 0.3068 - val_loss: 2.2995 - val_categorical_accuracy: 0.2324\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.23077 to 0.23241, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 17/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.1511 - categorical_accuracy: 0.2822 - val_loss: 2.2861 - val_categorical_accuracy: 0.2361\n","\n","Epoch 00017: val_categorical_accuracy improved from 0.23241 to 0.23609, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 18/100\n","35/35 [==============================] - 0s 7ms/step - loss: 2.1236 - categorical_accuracy: 0.3151 - val_loss: 2.2735 - val_categorical_accuracy: 0.2340\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.23609\n","Epoch 19/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.1433 - categorical_accuracy: 0.2735 - val_loss: 2.2638 - val_categorical_accuracy: 0.2390\n","\n","Epoch 00019: val_categorical_accuracy improved from 0.23609 to 0.23895, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 20/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.0955 - categorical_accuracy: 0.3372 - val_loss: 2.2523 - val_categorical_accuracy: 0.2418\n","\n","Epoch 00020: val_categorical_accuracy improved from 0.23895 to 0.24182, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 21/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.0642 - categorical_accuracy: 0.3208 - val_loss: 2.2407 - val_categorical_accuracy: 0.2455\n","\n","Epoch 00021: val_categorical_accuracy improved from 0.24182 to 0.24550, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 22/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.0427 - categorical_accuracy: 0.3094 - val_loss: 2.2306 - val_categorical_accuracy: 0.2467\n","\n","Epoch 00022: val_categorical_accuracy improved from 0.24550 to 0.24673, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 23/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.0477 - categorical_accuracy: 0.3224 - val_loss: 2.2183 - val_categorical_accuracy: 0.2557\n","\n","Epoch 00023: val_categorical_accuracy improved from 0.24673 to 0.25573, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 24/100\n","35/35 [==============================] - 0s 5ms/step - loss: 2.0446 - categorical_accuracy: 0.3007 - val_loss: 2.2089 - val_categorical_accuracy: 0.2570\n","\n","Epoch 00024: val_categorical_accuracy improved from 0.25573 to 0.25696, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 25/100\n","35/35 [==============================] - 0s 5ms/step - loss: 2.0229 - categorical_accuracy: 0.3757 - val_loss: 2.2000 - val_categorical_accuracy: 0.2647\n","\n","Epoch 00025: val_categorical_accuracy improved from 0.25696 to 0.26473, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 26/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.0066 - categorical_accuracy: 0.3690 - val_loss: 2.1912 - val_categorical_accuracy: 0.2619\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.26473\n","Epoch 27/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.9944 - categorical_accuracy: 0.3959 - val_loss: 2.1806 - val_categorical_accuracy: 0.2827\n","\n","Epoch 00027: val_categorical_accuracy improved from 0.26473 to 0.28273, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 28/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.0005 - categorical_accuracy: 0.4162 - val_loss: 2.1726 - val_categorical_accuracy: 0.2729\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.28273\n","Epoch 29/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.9742 - categorical_accuracy: 0.3842 - val_loss: 2.1649 - val_categorical_accuracy: 0.2705\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.28273\n","Epoch 30/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.9646 - categorical_accuracy: 0.4253 - val_loss: 2.1564 - val_categorical_accuracy: 0.2786\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.28273\n","Epoch 31/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.9601 - categorical_accuracy: 0.4256 - val_loss: 2.1473 - val_categorical_accuracy: 0.2848\n","\n","Epoch 00031: val_categorical_accuracy improved from 0.28273 to 0.28478, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 32/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.9386 - categorical_accuracy: 0.4619 - val_loss: 2.1394 - val_categorical_accuracy: 0.2860\n","\n","Epoch 00032: val_categorical_accuracy improved from 0.28478 to 0.28601, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 33/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8934 - categorical_accuracy: 0.4744 - val_loss: 2.1323 - val_categorical_accuracy: 0.2917\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.28601 to 0.29173, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 34/100\n","35/35 [==============================] - 0s 5ms/step - loss: 1.9291 - categorical_accuracy: 0.4911 - val_loss: 2.1246 - val_categorical_accuracy: 0.3020\n","\n","Epoch 00034: val_categorical_accuracy improved from 0.29173 to 0.30196, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 35/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.9283 - categorical_accuracy: 0.5105 - val_loss: 2.1199 - val_categorical_accuracy: 0.3138\n","\n","Epoch 00035: val_categorical_accuracy improved from 0.30196 to 0.31383, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 36/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.9189 - categorical_accuracy: 0.5629 - val_loss: 2.1106 - val_categorical_accuracy: 0.3236\n","\n","Epoch 00036: val_categorical_accuracy improved from 0.31383 to 0.32365, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 37/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8853 - categorical_accuracy: 0.5492 - val_loss: 2.1046 - val_categorical_accuracy: 0.3163\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.32365\n","Epoch 38/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8450 - categorical_accuracy: 0.5815 - val_loss: 2.0967 - val_categorical_accuracy: 0.3367\n","\n","Epoch 00038: val_categorical_accuracy improved from 0.32365 to 0.33674, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 39/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8906 - categorical_accuracy: 0.5566 - val_loss: 2.0892 - val_categorical_accuracy: 0.3486\n","\n","Epoch 00039: val_categorical_accuracy improved from 0.33674 to 0.34861, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 40/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8670 - categorical_accuracy: 0.5685 - val_loss: 2.0842 - val_categorical_accuracy: 0.3507\n","\n","Epoch 00040: val_categorical_accuracy improved from 0.34861 to 0.35065, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 41/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8418 - categorical_accuracy: 0.5725 - val_loss: 2.0763 - val_categorical_accuracy: 0.3650\n","\n","Epoch 00041: val_categorical_accuracy improved from 0.35065 to 0.36498, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 42/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8418 - categorical_accuracy: 0.5938 - val_loss: 2.0713 - val_categorical_accuracy: 0.3682\n","\n","Epoch 00042: val_categorical_accuracy improved from 0.36498 to 0.36825, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 43/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8429 - categorical_accuracy: 0.6083 - val_loss: 2.0651 - val_categorical_accuracy: 0.3666\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.36825\n","Epoch 44/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8140 - categorical_accuracy: 0.6060 - val_loss: 2.0561 - val_categorical_accuracy: 0.3756\n","\n","Epoch 00044: val_categorical_accuracy improved from 0.36825 to 0.37561, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 45/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8399 - categorical_accuracy: 0.6062 - val_loss: 2.0489 - val_categorical_accuracy: 0.3875\n","\n","Epoch 00045: val_categorical_accuracy improved from 0.37561 to 0.38748, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 46/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7795 - categorical_accuracy: 0.6714 - val_loss: 2.0457 - val_categorical_accuracy: 0.3793\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.38748\n","Epoch 47/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8026 - categorical_accuracy: 0.6256 - val_loss: 2.0358 - val_categorical_accuracy: 0.3973\n","\n","Epoch 00047: val_categorical_accuracy improved from 0.38748 to 0.39730, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 48/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7900 - categorical_accuracy: 0.6383 - val_loss: 2.0314 - val_categorical_accuracy: 0.3973\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.39730\n","Epoch 49/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7414 - categorical_accuracy: 0.6583 - val_loss: 2.0279 - val_categorical_accuracy: 0.3895\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.39730\n","Epoch 50/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7546 - categorical_accuracy: 0.6805 - val_loss: 2.0216 - val_categorical_accuracy: 0.4034\n","\n","Epoch 00050: val_categorical_accuracy improved from 0.39730 to 0.40344, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 51/100\n","35/35 [==============================] - 0s 10ms/step - loss: 1.7444 - categorical_accuracy: 0.6821 - val_loss: 2.0162 - val_categorical_accuracy: 0.4030\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.40344\n","Epoch 52/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7619 - categorical_accuracy: 0.6918 - val_loss: 2.0085 - val_categorical_accuracy: 0.4182\n","\n","Epoch 00052: val_categorical_accuracy improved from 0.40344 to 0.41817, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 53/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7335 - categorical_accuracy: 0.6979 - val_loss: 2.0052 - val_categorical_accuracy: 0.4071\n","\n","Epoch 00053: val_categorical_accuracy did not improve from 0.41817\n","Epoch 54/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7762 - categorical_accuracy: 0.6687 - val_loss: 2.0007 - val_categorical_accuracy: 0.4137\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.41817\n","Epoch 55/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7517 - categorical_accuracy: 0.6961 - val_loss: 1.9926 - val_categorical_accuracy: 0.4296\n","\n","Epoch 00055: val_categorical_accuracy improved from 0.41817 to 0.42962, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 56/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7159 - categorical_accuracy: 0.7124 - val_loss: 1.9881 - val_categorical_accuracy: 0.4223\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.42962\n","Epoch 57/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7146 - categorical_accuracy: 0.6890 - val_loss: 1.9841 - val_categorical_accuracy: 0.4259\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.42962\n","Epoch 58/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7440 - categorical_accuracy: 0.6782 - val_loss: 1.9791 - val_categorical_accuracy: 0.4264\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.42962\n","Epoch 59/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7242 - categorical_accuracy: 0.7091 - val_loss: 1.9744 - val_categorical_accuracy: 0.4333\n","\n","Epoch 00059: val_categorical_accuracy improved from 0.42962 to 0.43331, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 60/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6733 - categorical_accuracy: 0.7440 - val_loss: 1.9675 - val_categorical_accuracy: 0.4435\n","\n","Epoch 00060: val_categorical_accuracy improved from 0.43331 to 0.44354, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 61/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6900 - categorical_accuracy: 0.7656 - val_loss: 1.9647 - val_categorical_accuracy: 0.4386\n","\n","Epoch 00061: val_categorical_accuracy did not improve from 0.44354\n","Epoch 62/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6715 - categorical_accuracy: 0.7539 - val_loss: 1.9599 - val_categorical_accuracy: 0.4390\n","\n","Epoch 00062: val_categorical_accuracy did not improve from 0.44354\n","Epoch 63/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6381 - categorical_accuracy: 0.7584 - val_loss: 1.9527 - val_categorical_accuracy: 0.4509\n","\n","Epoch 00063: val_categorical_accuracy improved from 0.44354 to 0.45090, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 64/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6426 - categorical_accuracy: 0.7738 - val_loss: 1.9492 - val_categorical_accuracy: 0.4529\n","\n","Epoch 00064: val_categorical_accuracy improved from 0.45090 to 0.45295, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 65/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6448 - categorical_accuracy: 0.7462 - val_loss: 1.9453 - val_categorical_accuracy: 0.4554\n","\n","Epoch 00065: val_categorical_accuracy improved from 0.45295 to 0.45540, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 66/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6313 - categorical_accuracy: 0.7893 - val_loss: 1.9396 - val_categorical_accuracy: 0.4632\n","\n","Epoch 00066: val_categorical_accuracy improved from 0.45540 to 0.46318, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 67/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.6689 - categorical_accuracy: 0.7595 - val_loss: 1.9376 - val_categorical_accuracy: 0.4583\n","\n","Epoch 00067: val_categorical_accuracy did not improve from 0.46318\n","Epoch 68/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.6309 - categorical_accuracy: 0.8140 - val_loss: 1.9306 - val_categorical_accuracy: 0.4669\n","\n","Epoch 00068: val_categorical_accuracy improved from 0.46318 to 0.46686, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 69/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6212 - categorical_accuracy: 0.7947 - val_loss: 1.9257 - val_categorical_accuracy: 0.4640\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.46686\n","Epoch 70/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6172 - categorical_accuracy: 0.8036 - val_loss: 1.9217 - val_categorical_accuracy: 0.4714\n","\n","Epoch 00070: val_categorical_accuracy improved from 0.46686 to 0.47136, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 71/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6417 - categorical_accuracy: 0.8162 - val_loss: 1.9190 - val_categorical_accuracy: 0.4648\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.47136\n","Epoch 72/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6187 - categorical_accuracy: 0.8181 - val_loss: 1.9145 - val_categorical_accuracy: 0.4709\n","\n","Epoch 00072: val_categorical_accuracy did not improve from 0.47136\n","Epoch 73/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6145 - categorical_accuracy: 0.8241 - val_loss: 1.9104 - val_categorical_accuracy: 0.4746\n","\n","Epoch 00073: val_categorical_accuracy improved from 0.47136 to 0.47463, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 74/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5642 - categorical_accuracy: 0.8343 - val_loss: 1.9063 - val_categorical_accuracy: 0.4726\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.47463\n","Epoch 75/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5993 - categorical_accuracy: 0.8244 - val_loss: 1.9002 - val_categorical_accuracy: 0.4840\n","\n","Epoch 00075: val_categorical_accuracy improved from 0.47463 to 0.48404, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 76/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.6146 - categorical_accuracy: 0.8402 - val_loss: 1.8963 - val_categorical_accuracy: 0.4845\n","\n","Epoch 00076: val_categorical_accuracy improved from 0.48404 to 0.48445, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 77/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5921 - categorical_accuracy: 0.8299 - val_loss: 1.8921 - val_categorical_accuracy: 0.4820\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.48445\n","Epoch 78/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6025 - categorical_accuracy: 0.8176 - val_loss: 1.8878 - val_categorical_accuracy: 0.4898\n","\n","Epoch 00078: val_categorical_accuracy improved from 0.48445 to 0.48977, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 79/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5685 - categorical_accuracy: 0.8462 - val_loss: 1.8864 - val_categorical_accuracy: 0.4975\n","\n","Epoch 00079: val_categorical_accuracy improved from 0.48977 to 0.49755, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 80/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5701 - categorical_accuracy: 0.8304 - val_loss: 1.8803 - val_categorical_accuracy: 0.4861\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.49755\n","Epoch 81/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5314 - categorical_accuracy: 0.8211 - val_loss: 1.8775 - val_categorical_accuracy: 0.5053\n","\n","Epoch 00081: val_categorical_accuracy improved from 0.49755 to 0.50532, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 82/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5397 - categorical_accuracy: 0.8312 - val_loss: 1.8740 - val_categorical_accuracy: 0.4849\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.50532\n","Epoch 83/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5534 - categorical_accuracy: 0.8561 - val_loss: 1.8695 - val_categorical_accuracy: 0.4832\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.50532\n","Epoch 84/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5455 - categorical_accuracy: 0.8442 - val_loss: 1.8657 - val_categorical_accuracy: 0.4996\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.50532\n","Epoch 85/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5610 - categorical_accuracy: 0.8727 - val_loss: 1.8634 - val_categorical_accuracy: 0.4943\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.50532\n","Epoch 86/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5187 - categorical_accuracy: 0.8536 - val_loss: 1.8584 - val_categorical_accuracy: 0.4984\n","\n","Epoch 00086: val_categorical_accuracy did not improve from 0.50532\n","Epoch 87/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5059 - categorical_accuracy: 0.8525 - val_loss: 1.8530 - val_categorical_accuracy: 0.5016\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.50532\n","Epoch 88/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4901 - categorical_accuracy: 0.8476 - val_loss: 1.8511 - val_categorical_accuracy: 0.5115\n","\n","Epoch 00088: val_categorical_accuracy improved from 0.50532 to 0.51146, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 89/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5105 - categorical_accuracy: 0.8699 - val_loss: 1.8468 - val_categorical_accuracy: 0.5045\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.51146\n","Epoch 90/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5135 - categorical_accuracy: 0.8628 - val_loss: 1.8437 - val_categorical_accuracy: 0.5053\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.51146\n","Epoch 91/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5144 - categorical_accuracy: 0.8565 - val_loss: 1.8400 - val_categorical_accuracy: 0.5053\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.51146\n","Epoch 92/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.4845 - categorical_accuracy: 0.8662 - val_loss: 1.8380 - val_categorical_accuracy: 0.4959\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.51146\n","Epoch 93/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4716 - categorical_accuracy: 0.8653 - val_loss: 1.8345 - val_categorical_accuracy: 0.5262\n","\n","Epoch 00093: val_categorical_accuracy improved from 0.51146 to 0.52619, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 94/100\n","35/35 [==============================] - 0s 5ms/step - loss: 1.4829 - categorical_accuracy: 0.8589 - val_loss: 1.8292 - val_categorical_accuracy: 0.4996\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.52619\n","Epoch 95/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4819 - categorical_accuracy: 0.8608 - val_loss: 1.8257 - val_categorical_accuracy: 0.5070\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.52619\n","Epoch 96/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4635 - categorical_accuracy: 0.8595 - val_loss: 1.8234 - val_categorical_accuracy: 0.5029\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.52619\n","Epoch 97/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4586 - categorical_accuracy: 0.8673 - val_loss: 1.8192 - val_categorical_accuracy: 0.5348\n","\n","Epoch 00097: val_categorical_accuracy improved from 0.52619 to 0.53478, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 98/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.4559 - categorical_accuracy: 0.8734 - val_loss: 1.8155 - val_categorical_accuracy: 0.4975\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.53478\n","Epoch 99/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4388 - categorical_accuracy: 0.8720 - val_loss: 1.8134 - val_categorical_accuracy: 0.5397\n","\n","Epoch 00099: val_categorical_accuracy improved from 0.53478 to 0.53969, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 100/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.4291 - categorical_accuracy: 0.8878 - val_loss: 1.8098 - val_categorical_accuracy: 0.5135\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.53969\n","77/77 [==============================] - 0s 1ms/step - loss: 1.8134 - categorical_accuracy: 0.5397\n","77/77 [==============================] - 0s 1ms/step\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 5, 5, 64)     256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 5, 5, 64)     0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 5, 5, 8)      584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 5, 5, 8)      584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 5, 5, 8)      584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 5, 5, 8)      584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 5, 5, 8)      584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 5, 5, 8)      584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 5, 5, 8)      584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 5, 5, 8)      584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 5, 5, 8)      584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 5, 5, 8)      584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 5, 5, 8)      584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 5, 5, 8)      584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 5, 5, 8)      584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 5, 5, 8)      584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 5, 5, 8)      584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 5, 5, 8)      584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 5, 5, 64)     0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 5, 5, 64)     256         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 5, 5, 64)     0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 5, 5, 128)    8320        activation_6[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 5, 5, 128)    512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 5, 5, 128)    0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 5, 5, 8, 16)  0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 5, 5, 16, 8)  0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 5, 5, 128)    512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 5, 5, 128)    0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 5, 5, 128)    0           tf.reshape_3[0][0]               \n","                                                                 activation_8[0][0]               \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 5, 5, 128)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_9[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 700.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [50 50 50 50 50 50 50 50 50 50 50 50 50 50]\n","\n","Total number of samples in test set 2444.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [220  51 201 165 219 219 209 153 264 198 226  83 191  45]\n","\n","X_train_transfer => (700, 128)\n","X_test_transfer  => (2444, 128)\n","y_train => (700, 14)\n","y_test  => (2444, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_8 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","35/35 [==============================] - 1s 10ms/step - loss: 3.7753 - categorical_accuracy: 0.1228 - val_loss: 2.5407 - val_categorical_accuracy: 0.0495\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.04951, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.2677 - categorical_accuracy: 0.1567 - val_loss: 2.3111 - val_categorical_accuracy: 0.2369\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.04951 to 0.23691, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.9529 - categorical_accuracy: 0.4659 - val_loss: 2.1820 - val_categorical_accuracy: 0.2565\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.23691 to 0.25655, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.7646 - categorical_accuracy: 0.6443 - val_loss: 2.0931 - val_categorical_accuracy: 0.3146\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.25655 to 0.31465, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6500 - categorical_accuracy: 0.6164 - val_loss: 1.9855 - val_categorical_accuracy: 0.3433\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.31465 to 0.34329, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 6/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5835 - categorical_accuracy: 0.6455 - val_loss: 1.9076 - val_categorical_accuracy: 0.4902\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.34329 to 0.49018, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4767 - categorical_accuracy: 0.7364 - val_loss: 1.8618 - val_categorical_accuracy: 0.4767\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.49018\n","Epoch 8/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4103 - categorical_accuracy: 0.7228 - val_loss: 1.8046 - val_categorical_accuracy: 0.4767\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.49018\n","Epoch 9/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.3742 - categorical_accuracy: 0.7178 - val_loss: 1.7611 - val_categorical_accuracy: 0.5286\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.49018 to 0.52864, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 10/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.3169 - categorical_accuracy: 0.7334 - val_loss: 1.7562 - val_categorical_accuracy: 0.4026\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.52864\n","Epoch 11/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.2793 - categorical_accuracy: 0.7793 - val_loss: 1.6950 - val_categorical_accuracy: 0.5475\n","\n","Epoch 00011: val_categorical_accuracy improved from 0.52864 to 0.54746, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 12/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.2549 - categorical_accuracy: 0.7854 - val_loss: 1.6615 - val_categorical_accuracy: 0.5712\n","\n","Epoch 00012: val_categorical_accuracy improved from 0.54746 to 0.57119, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 13/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.1833 - categorical_accuracy: 0.8811 - val_loss: 1.6039 - val_categorical_accuracy: 0.5344\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.57119\n","Epoch 14/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1226 - categorical_accuracy: 0.8556 - val_loss: 1.6194 - val_categorical_accuracy: 0.5773\n","\n","Epoch 00014: val_categorical_accuracy improved from 0.57119 to 0.57733, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 15/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.1246 - categorical_accuracy: 0.9225 - val_loss: 1.5682 - val_categorical_accuracy: 0.5724\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.57733\n","Epoch 16/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.0621 - categorical_accuracy: 0.8968 - val_loss: 1.5540 - val_categorical_accuracy: 0.5872\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.57733 to 0.58715, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 17/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0518 - categorical_accuracy: 0.9530 - val_loss: 1.5523 - val_categorical_accuracy: 0.6174\n","\n","Epoch 00017: val_categorical_accuracy improved from 0.58715 to 0.61743, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 18/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.0081 - categorical_accuracy: 0.9636 - val_loss: 1.5151 - val_categorical_accuracy: 0.5978\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.61743\n","Epoch 19/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.9777 - categorical_accuracy: 0.9672 - val_loss: 1.4885 - val_categorical_accuracy: 0.6215\n","\n","Epoch 00019: val_categorical_accuracy improved from 0.61743 to 0.62152, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 20/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.9606 - categorical_accuracy: 0.9775 - val_loss: 1.4894 - val_categorical_accuracy: 0.5773\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.62152\n","Epoch 21/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.9516 - categorical_accuracy: 0.9426 - val_loss: 1.4497 - val_categorical_accuracy: 0.6481\n","\n","Epoch 00021: val_categorical_accuracy improved from 0.62152 to 0.64812, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 22/100\n","35/35 [==============================] - 0s 5ms/step - loss: 0.9043 - categorical_accuracy: 0.9794 - val_loss: 1.4580 - val_categorical_accuracy: 0.6747\n","\n","Epoch 00022: val_categorical_accuracy improved from 0.64812 to 0.67471, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 23/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.9050 - categorical_accuracy: 0.9725 - val_loss: 1.4283 - val_categorical_accuracy: 0.6383\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.67471\n","Epoch 24/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8782 - categorical_accuracy: 0.9650 - val_loss: 1.4102 - val_categorical_accuracy: 0.6522\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.67471\n","Epoch 25/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8360 - categorical_accuracy: 0.9650 - val_loss: 1.3944 - val_categorical_accuracy: 0.6072\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.67471\n","Epoch 26/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8203 - categorical_accuracy: 0.9745 - val_loss: 1.4129 - val_categorical_accuracy: 0.6788\n","\n","Epoch 00026: val_categorical_accuracy improved from 0.67471 to 0.67881, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 27/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.7954 - categorical_accuracy: 0.9875 - val_loss: 1.3764 - val_categorical_accuracy: 0.6559\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.67881\n","Epoch 28/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.7977 - categorical_accuracy: 0.9763 - val_loss: 1.3721 - val_categorical_accuracy: 0.6755\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.67881\n","Epoch 29/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.8084 - categorical_accuracy: 0.9890 - val_loss: 1.3699 - val_categorical_accuracy: 0.6739\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.67881\n","Epoch 30/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.7837 - categorical_accuracy: 0.9946 - val_loss: 1.3472 - val_categorical_accuracy: 0.6653\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.67881\n","Epoch 31/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.7686 - categorical_accuracy: 0.9743 - val_loss: 1.3371 - val_categorical_accuracy: 0.6326\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.67881\n","Epoch 32/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.7307 - categorical_accuracy: 0.9706 - val_loss: 1.3513 - val_categorical_accuracy: 0.6747\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.67881\n","Epoch 33/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.7420 - categorical_accuracy: 0.9814 - val_loss: 1.3144 - val_categorical_accuracy: 0.6543\n","\n","Epoch 00033: val_categorical_accuracy did not improve from 0.67881\n","Epoch 34/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.6973 - categorical_accuracy: 0.9976 - val_loss: 1.2945 - val_categorical_accuracy: 0.6813\n","\n","Epoch 00034: val_categorical_accuracy improved from 0.67881 to 0.68126, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 35/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.6879 - categorical_accuracy: 0.9950 - val_loss: 1.3016 - val_categorical_accuracy: 0.6739\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.68126\n","Epoch 36/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.6812 - categorical_accuracy: 0.9956 - val_loss: 1.2797 - val_categorical_accuracy: 0.7070\n","\n","Epoch 00036: val_categorical_accuracy improved from 0.68126 to 0.70704, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 37/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.6781 - categorical_accuracy: 0.9945 - val_loss: 1.2853 - val_categorical_accuracy: 0.6538\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.70704\n","Epoch 38/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.6430 - categorical_accuracy: 0.9955 - val_loss: 1.2530 - val_categorical_accuracy: 0.6944\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.70704\n","Epoch 39/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.6470 - categorical_accuracy: 0.9902 - val_loss: 1.2517 - val_categorical_accuracy: 0.6665\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.70704\n","Epoch 40/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.6378 - categorical_accuracy: 0.9965 - val_loss: 1.2547 - val_categorical_accuracy: 0.6678\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.70704\n","Epoch 41/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5947 - categorical_accuracy: 0.9984 - val_loss: 1.2586 - val_categorical_accuracy: 0.6608\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.70704\n","Epoch 42/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5931 - categorical_accuracy: 0.9961 - val_loss: 1.2311 - val_categorical_accuracy: 0.6882\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.70704\n","Epoch 43/100\n","35/35 [==============================] - 0s 10ms/step - loss: 0.5988 - categorical_accuracy: 0.9988 - val_loss: 1.2231 - val_categorical_accuracy: 0.6792\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.70704\n","Epoch 44/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.5635 - categorical_accuracy: 1.0000 - val_loss: 1.2287 - val_categorical_accuracy: 0.6903\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.70704\n","Epoch 45/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5911 - categorical_accuracy: 0.9995 - val_loss: 1.2334 - val_categorical_accuracy: 0.6849\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.70704\n","Epoch 46/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5610 - categorical_accuracy: 0.9986 - val_loss: 1.2137 - val_categorical_accuracy: 0.6899\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.70704\n","Epoch 47/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5610 - categorical_accuracy: 0.9984 - val_loss: 1.2165 - val_categorical_accuracy: 0.6506\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.70704\n","Epoch 48/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.5540 - categorical_accuracy: 0.9999 - val_loss: 1.2229 - val_categorical_accuracy: 0.6678\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.70704\n","Epoch 49/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5382 - categorical_accuracy: 0.9916 - val_loss: 1.2121 - val_categorical_accuracy: 0.6800\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.70704\n","Epoch 50/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5217 - categorical_accuracy: 1.0000 - val_loss: 1.1814 - val_categorical_accuracy: 0.6702\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.70704\n","Epoch 51/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4978 - categorical_accuracy: 0.9987 - val_loss: 1.2009 - val_categorical_accuracy: 0.6596\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.70704\n","Epoch 52/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5045 - categorical_accuracy: 0.9879 - val_loss: 1.1698 - val_categorical_accuracy: 0.6628\n","\n","Epoch 00052: val_categorical_accuracy did not improve from 0.70704\n","Epoch 53/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.5042 - categorical_accuracy: 0.9947 - val_loss: 1.2056 - val_categorical_accuracy: 0.6657\n","\n","Epoch 00053: val_categorical_accuracy did not improve from 0.70704\n","Epoch 54/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4879 - categorical_accuracy: 0.9962 - val_loss: 1.1656 - val_categorical_accuracy: 0.6976\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.70704\n","Epoch 55/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.5030 - categorical_accuracy: 0.9991 - val_loss: 1.2046 - val_categorical_accuracy: 0.6780\n","\n","Epoch 00055: val_categorical_accuracy did not improve from 0.70704\n","Epoch 56/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4788 - categorical_accuracy: 0.9956 - val_loss: 1.1685 - val_categorical_accuracy: 0.6817\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.70704\n","Epoch 57/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4795 - categorical_accuracy: 1.0000 - val_loss: 1.1876 - val_categorical_accuracy: 0.6579\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.70704\n","Epoch 58/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4579 - categorical_accuracy: 0.9974 - val_loss: 1.1849 - val_categorical_accuracy: 0.6718\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.70704\n","Epoch 59/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4723 - categorical_accuracy: 0.9959 - val_loss: 1.1364 - val_categorical_accuracy: 0.6866\n","\n","Epoch 00059: val_categorical_accuracy did not improve from 0.70704\n","Epoch 60/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4523 - categorical_accuracy: 1.0000 - val_loss: 1.1669 - val_categorical_accuracy: 0.6821\n","\n","Epoch 00060: val_categorical_accuracy did not improve from 0.70704\n","Epoch 61/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4559 - categorical_accuracy: 0.9989 - val_loss: 1.1400 - val_categorical_accuracy: 0.6866\n","\n","Epoch 00061: val_categorical_accuracy did not improve from 0.70704\n","Epoch 62/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4379 - categorical_accuracy: 1.0000 - val_loss: 1.1535 - val_categorical_accuracy: 0.6702\n","\n","Epoch 00062: val_categorical_accuracy did not improve from 0.70704\n","Epoch 63/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4402 - categorical_accuracy: 1.0000 - val_loss: 1.1204 - val_categorical_accuracy: 0.6989\n","\n","Epoch 00063: val_categorical_accuracy did not improve from 0.70704\n","Epoch 64/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4259 - categorical_accuracy: 1.0000 - val_loss: 1.1553 - val_categorical_accuracy: 0.6878\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.70704\n","Epoch 65/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4185 - categorical_accuracy: 1.0000 - val_loss: 1.1327 - val_categorical_accuracy: 0.6768\n","\n","Epoch 00065: val_categorical_accuracy did not improve from 0.70704\n","Epoch 66/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.4217 - categorical_accuracy: 1.0000 - val_loss: 1.1167 - val_categorical_accuracy: 0.6796\n","\n","Epoch 00066: val_categorical_accuracy did not improve from 0.70704\n","Epoch 67/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.4248 - categorical_accuracy: 1.0000 - val_loss: 1.1310 - val_categorical_accuracy: 0.6866\n","\n","Epoch 00067: val_categorical_accuracy did not improve from 0.70704\n","Epoch 68/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3922 - categorical_accuracy: 0.9991 - val_loss: 1.1254 - val_categorical_accuracy: 0.6919\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.70704\n","Epoch 69/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.3936 - categorical_accuracy: 1.0000 - val_loss: 1.1298 - val_categorical_accuracy: 0.6837\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.70704\n","Epoch 70/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.3878 - categorical_accuracy: 0.9998 - val_loss: 1.1220 - val_categorical_accuracy: 0.6755\n","\n","Epoch 00070: val_categorical_accuracy did not improve from 0.70704\n","Epoch 71/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3845 - categorical_accuracy: 1.0000 - val_loss: 1.1064 - val_categorical_accuracy: 0.6854\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.70704\n","Epoch 72/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.3807 - categorical_accuracy: 1.0000 - val_loss: 1.1140 - val_categorical_accuracy: 0.6899\n","\n","Epoch 00072: val_categorical_accuracy did not improve from 0.70704\n","Epoch 73/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3894 - categorical_accuracy: 1.0000 - val_loss: 1.1277 - val_categorical_accuracy: 0.6759\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.70704\n","Epoch 74/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3721 - categorical_accuracy: 1.0000 - val_loss: 1.0969 - val_categorical_accuracy: 0.6927\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.70704\n","Epoch 75/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3790 - categorical_accuracy: 1.0000 - val_loss: 1.1072 - val_categorical_accuracy: 0.6964\n","\n","Epoch 00075: val_categorical_accuracy did not improve from 0.70704\n","Epoch 76/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3578 - categorical_accuracy: 1.0000 - val_loss: 1.1195 - val_categorical_accuracy: 0.6804\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.70704\n","Epoch 77/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3557 - categorical_accuracy: 1.0000 - val_loss: 1.0953 - val_categorical_accuracy: 0.7087\n","\n","Epoch 00077: val_categorical_accuracy improved from 0.70704 to 0.70867, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 78/100\n","35/35 [==============================] - 0s 8ms/step - loss: 0.3602 - categorical_accuracy: 1.0000 - val_loss: 1.1043 - val_categorical_accuracy: 0.6768\n","\n","Epoch 00078: val_categorical_accuracy did not improve from 0.70867\n","Epoch 79/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3572 - categorical_accuracy: 1.0000 - val_loss: 1.1059 - val_categorical_accuracy: 0.6870\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.70867\n","Epoch 80/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3456 - categorical_accuracy: 1.0000 - val_loss: 1.1022 - val_categorical_accuracy: 0.6833\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.70867\n","Epoch 81/100\n","35/35 [==============================] - 0s 8ms/step - loss: 0.3548 - categorical_accuracy: 1.0000 - val_loss: 1.0947 - val_categorical_accuracy: 0.6813\n","\n","Epoch 00081: val_categorical_accuracy did not improve from 0.70867\n","Epoch 82/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3424 - categorical_accuracy: 1.0000 - val_loss: 1.0929 - val_categorical_accuracy: 0.6882\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.70867\n","Epoch 83/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3471 - categorical_accuracy: 1.0000 - val_loss: 1.0929 - val_categorical_accuracy: 0.6813\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.70867\n","Epoch 84/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3297 - categorical_accuracy: 1.0000 - val_loss: 1.0870 - val_categorical_accuracy: 0.6837\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.70867\n","Epoch 85/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3384 - categorical_accuracy: 1.0000 - val_loss: 1.0881 - val_categorical_accuracy: 0.6911\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.70867\n","Epoch 86/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3299 - categorical_accuracy: 1.0000 - val_loss: 1.0818 - val_categorical_accuracy: 0.6919\n","\n","Epoch 00086: val_categorical_accuracy did not improve from 0.70867\n","Epoch 87/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3223 - categorical_accuracy: 1.0000 - val_loss: 1.1119 - val_categorical_accuracy: 0.6849\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.70867\n","Epoch 88/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3153 - categorical_accuracy: 1.0000 - val_loss: 1.0709 - val_categorical_accuracy: 0.6788\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.70867\n","Epoch 89/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3058 - categorical_accuracy: 1.0000 - val_loss: 1.0760 - val_categorical_accuracy: 0.6772\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.70867\n","Epoch 90/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3088 - categorical_accuracy: 0.9981 - val_loss: 1.0620 - val_categorical_accuracy: 0.7042\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.70867\n","Epoch 91/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3010 - categorical_accuracy: 1.0000 - val_loss: 1.0811 - val_categorical_accuracy: 0.6935\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.70867\n","Epoch 92/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.3110 - categorical_accuracy: 1.0000 - val_loss: 1.0827 - val_categorical_accuracy: 0.6776\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.70867\n","Epoch 93/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.2955 - categorical_accuracy: 1.0000 - val_loss: 1.0635 - val_categorical_accuracy: 0.6890\n","\n","Epoch 00093: val_categorical_accuracy did not improve from 0.70867\n","Epoch 94/100\n","35/35 [==============================] - 0s 8ms/step - loss: 0.2892 - categorical_accuracy: 1.0000 - val_loss: 1.0511 - val_categorical_accuracy: 0.7144\n","\n","Epoch 00094: val_categorical_accuracy improved from 0.70867 to 0.71440, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 95/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.2948 - categorical_accuracy: 1.0000 - val_loss: 1.0714 - val_categorical_accuracy: 0.6804\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.71440\n","Epoch 96/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.2865 - categorical_accuracy: 1.0000 - val_loss: 1.0651 - val_categorical_accuracy: 0.6939\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.71440\n","Epoch 97/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.2710 - categorical_accuracy: 1.0000 - val_loss: 1.0627 - val_categorical_accuracy: 0.6862\n","\n","Epoch 00097: val_categorical_accuracy did not improve from 0.71440\n","Epoch 98/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.2761 - categorical_accuracy: 1.0000 - val_loss: 1.0609 - val_categorical_accuracy: 0.6984\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.71440\n","Epoch 99/100\n","35/35 [==============================] - 0s 10ms/step - loss: 0.2729 - categorical_accuracy: 1.0000 - val_loss: 1.0649 - val_categorical_accuracy: 0.6911\n","\n","Epoch 00099: val_categorical_accuracy did not improve from 0.71440\n","Epoch 100/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.2853 - categorical_accuracy: 1.0000 - val_loss: 1.0569 - val_categorical_accuracy: 0.6956\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.71440\n","77/77 [==============================] - 0s 2ms/step - loss: 1.0511 - categorical_accuracy: 0.7144\n","77/77 [==============================] - 0s 1ms/step\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 5, 5, 64)     256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 5, 5, 64)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 5, 5, 8)      584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 5, 5, 8)      584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 5, 5, 8)      584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 5, 5, 8)      584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 5, 5, 8)      584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 5, 5, 8)      584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 5, 5, 8)      584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 5, 5, 8)      584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 5, 5, 8)      584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 5, 5, 8)      584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 5, 5, 8)      584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 5, 5, 8)      584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 5, 5, 8)      584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 5, 5, 8)      584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 5, 5, 8)      584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 5, 5, 8)      584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 5, 5, 64)     0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 5, 5, 64)     256         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 5, 5, 64)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 5, 5, 128)    8320        activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 5, 5, 128)    0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 5, 5, 8, 16)  0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 5, 5, 16, 8)  0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 5, 5, 128)    512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 5, 5, 128)    0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 5, 5, 128)    0           tf.reshape_5[0][0]               \n","                                                                 activation_13[0][0]              \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 5, 5, 128)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_14[0][0]              \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 700.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [50 50 50 50 50 50 50 50 50 50 50 50 50 50]\n","\n","Total number of samples in test set 2444.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [220  51 201 165 219 219 209 153 264 198 226  83 191  45]\n","\n","X_train_transfer => (700, 128)\n","X_test_transfer  => (2444, 128)\n","y_train => (700, 14)\n","y_test  => (2444, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_9 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","35/35 [==============================] - 1s 10ms/step - loss: 3.2232 - categorical_accuracy: 0.0649 - val_loss: 2.6129 - val_categorical_accuracy: 0.1882\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.18822, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.4421 - categorical_accuracy: 0.2220 - val_loss: 2.4396 - val_categorical_accuracy: 0.2529\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.18822 to 0.25286, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","35/35 [==============================] - 0s 7ms/step - loss: 2.2985 - categorical_accuracy: 0.3322 - val_loss: 2.3555 - val_categorical_accuracy: 0.3482\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.25286 to 0.34820, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","35/35 [==============================] - 0s 7ms/step - loss: 2.2265 - categorical_accuracy: 0.4180 - val_loss: 2.2972 - val_categorical_accuracy: 0.3793\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.34820 to 0.37930, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","35/35 [==============================] - 0s 7ms/step - loss: 2.1605 - categorical_accuracy: 0.4331 - val_loss: 2.2544 - val_categorical_accuracy: 0.4059\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.37930 to 0.40589, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 6/100\n","35/35 [==============================] - 0s 7ms/step - loss: 2.0793 - categorical_accuracy: 0.4856 - val_loss: 2.2127 - val_categorical_accuracy: 0.4681\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.40589 to 0.46809, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","35/35 [==============================] - 0s 6ms/step - loss: 2.0249 - categorical_accuracy: 0.5351 - val_loss: 2.1809 - val_categorical_accuracy: 0.4484\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.46809\n","Epoch 8/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.9966 - categorical_accuracy: 0.5824 - val_loss: 2.1489 - val_categorical_accuracy: 0.4460\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.46809\n","Epoch 9/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.9497 - categorical_accuracy: 0.5930 - val_loss: 2.1207 - val_categorical_accuracy: 0.4714\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.46809 to 0.47136, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 10/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.9415 - categorical_accuracy: 0.5944 - val_loss: 2.0977 - val_categorical_accuracy: 0.4583\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.47136\n","Epoch 11/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8694 - categorical_accuracy: 0.5800 - val_loss: 2.0751 - val_categorical_accuracy: 0.4476\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.47136\n","Epoch 12/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.8744 - categorical_accuracy: 0.5733 - val_loss: 2.0524 - val_categorical_accuracy: 0.4296\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.47136\n","Epoch 13/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.8251 - categorical_accuracy: 0.5770 - val_loss: 2.0295 - val_categorical_accuracy: 0.4820\n","\n","Epoch 00013: val_categorical_accuracy improved from 0.47136 to 0.48200, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 14/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.7562 - categorical_accuracy: 0.6747 - val_loss: 2.0094 - val_categorical_accuracy: 0.4464\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.48200\n","Epoch 15/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.7792 - categorical_accuracy: 0.5803 - val_loss: 1.9925 - val_categorical_accuracy: 0.4517\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.48200\n","Epoch 16/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.7428 - categorical_accuracy: 0.6144 - val_loss: 1.9739 - val_categorical_accuracy: 0.4632\n","\n","Epoch 00016: val_categorical_accuracy did not improve from 0.48200\n","Epoch 17/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.7174 - categorical_accuracy: 0.6514 - val_loss: 1.9585 - val_categorical_accuracy: 0.4963\n","\n","Epoch 00017: val_categorical_accuracy improved from 0.48200 to 0.49632, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 18/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.6890 - categorical_accuracy: 0.6984 - val_loss: 1.9402 - val_categorical_accuracy: 0.4652\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.49632\n","Epoch 19/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6645 - categorical_accuracy: 0.6620 - val_loss: 1.9262 - val_categorical_accuracy: 0.4652\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.49632\n","Epoch 20/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.6593 - categorical_accuracy: 0.6565 - val_loss: 1.9092 - val_categorical_accuracy: 0.5090\n","\n","Epoch 00020: val_categorical_accuracy improved from 0.49632 to 0.50900, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 21/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5751 - categorical_accuracy: 0.7096 - val_loss: 1.8939 - val_categorical_accuracy: 0.4705\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.50900\n","Epoch 22/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5844 - categorical_accuracy: 0.6988 - val_loss: 1.8752 - val_categorical_accuracy: 0.5041\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.50900\n","Epoch 23/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.6029 - categorical_accuracy: 0.6999 - val_loss: 1.8613 - val_categorical_accuracy: 0.4853\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.50900\n","Epoch 24/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5774 - categorical_accuracy: 0.7179 - val_loss: 1.8476 - val_categorical_accuracy: 0.5086\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.50900\n","Epoch 25/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.5182 - categorical_accuracy: 0.7127 - val_loss: 1.8368 - val_categorical_accuracy: 0.4709\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.50900\n","Epoch 26/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5103 - categorical_accuracy: 0.7259 - val_loss: 1.8179 - val_categorical_accuracy: 0.5070\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.50900\n","Epoch 27/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.5128 - categorical_accuracy: 0.7435 - val_loss: 1.8093 - val_categorical_accuracy: 0.5110\n","\n","Epoch 00027: val_categorical_accuracy improved from 0.50900 to 0.51105, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 28/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4539 - categorical_accuracy: 0.7527 - val_loss: 1.7983 - val_categorical_accuracy: 0.5037\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.51105\n","Epoch 29/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4489 - categorical_accuracy: 0.7455 - val_loss: 1.7856 - val_categorical_accuracy: 0.5291\n","\n","Epoch 00029: val_categorical_accuracy improved from 0.51105 to 0.52905, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 30/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.4511 - categorical_accuracy: 0.7811 - val_loss: 1.7721 - val_categorical_accuracy: 0.5196\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.52905\n","Epoch 31/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.4492 - categorical_accuracy: 0.7506 - val_loss: 1.7627 - val_categorical_accuracy: 0.5110\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.52905\n","Epoch 32/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.3797 - categorical_accuracy: 0.7794 - val_loss: 1.7524 - val_categorical_accuracy: 0.5065\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.52905\n","Epoch 33/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.3551 - categorical_accuracy: 0.7921 - val_loss: 1.7438 - val_categorical_accuracy: 0.5123\n","\n","Epoch 00033: val_categorical_accuracy did not improve from 0.52905\n","Epoch 34/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.3718 - categorical_accuracy: 0.8154 - val_loss: 1.7285 - val_categorical_accuracy: 0.5258\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.52905\n","Epoch 35/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.3745 - categorical_accuracy: 0.7811 - val_loss: 1.7256 - val_categorical_accuracy: 0.5110\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.52905\n","Epoch 36/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.3595 - categorical_accuracy: 0.7973 - val_loss: 1.7135 - val_categorical_accuracy: 0.5155\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.52905\n","Epoch 37/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.3359 - categorical_accuracy: 0.8130 - val_loss: 1.7020 - val_categorical_accuracy: 0.5282\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.52905\n","Epoch 38/100\n","35/35 [==============================] - 0s 12ms/step - loss: 1.3048 - categorical_accuracy: 0.7974 - val_loss: 1.6953 - val_categorical_accuracy: 0.5299\n","\n","Epoch 00038: val_categorical_accuracy improved from 0.52905 to 0.52987, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 39/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.2863 - categorical_accuracy: 0.8267 - val_loss: 1.6865 - val_categorical_accuracy: 0.5254\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.52987\n","Epoch 40/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.2668 - categorical_accuracy: 0.8462 - val_loss: 1.6758 - val_categorical_accuracy: 0.5241\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.52987\n","Epoch 41/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.2543 - categorical_accuracy: 0.8226 - val_loss: 1.6740 - val_categorical_accuracy: 0.5344\n","\n","Epoch 00041: val_categorical_accuracy improved from 0.52987 to 0.53437, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 42/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.2933 - categorical_accuracy: 0.8468 - val_loss: 1.6641 - val_categorical_accuracy: 0.5323\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.53437\n","Epoch 43/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.2236 - categorical_accuracy: 0.8754 - val_loss: 1.6562 - val_categorical_accuracy: 0.5364\n","\n","Epoch 00043: val_categorical_accuracy improved from 0.53437 to 0.53642, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 44/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.2162 - categorical_accuracy: 0.8807 - val_loss: 1.6524 - val_categorical_accuracy: 0.5086\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.53642\n","Epoch 45/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.2218 - categorical_accuracy: 0.8755 - val_loss: 1.6452 - val_categorical_accuracy: 0.5516\n","\n","Epoch 00045: val_categorical_accuracy improved from 0.53642 to 0.55155, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 46/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1880 - categorical_accuracy: 0.8609 - val_loss: 1.6366 - val_categorical_accuracy: 0.5499\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.55155\n","Epoch 47/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.2406 - categorical_accuracy: 0.9299 - val_loss: 1.6277 - val_categorical_accuracy: 0.5393\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.55155\n","Epoch 48/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1714 - categorical_accuracy: 0.8982 - val_loss: 1.6225 - val_categorical_accuracy: 0.5438\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.55155\n","Epoch 49/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.1932 - categorical_accuracy: 0.8934 - val_loss: 1.6195 - val_categorical_accuracy: 0.5454\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.55155\n","Epoch 50/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1700 - categorical_accuracy: 0.9107 - val_loss: 1.6145 - val_categorical_accuracy: 0.5450\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.55155\n","Epoch 51/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1600 - categorical_accuracy: 0.9225 - val_loss: 1.6047 - val_categorical_accuracy: 0.5401\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.55155\n","Epoch 52/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1620 - categorical_accuracy: 0.9105 - val_loss: 1.5979 - val_categorical_accuracy: 0.5487\n","\n","Epoch 00052: val_categorical_accuracy did not improve from 0.55155\n","Epoch 53/100\n","35/35 [==============================] - 0s 6ms/step - loss: 1.1535 - categorical_accuracy: 0.9183 - val_loss: 1.5974 - val_categorical_accuracy: 0.5364\n","\n","Epoch 00053: val_categorical_accuracy did not improve from 0.55155\n","Epoch 54/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1041 - categorical_accuracy: 0.9336 - val_loss: 1.5869 - val_categorical_accuracy: 0.5471\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.55155\n","Epoch 55/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1140 - categorical_accuracy: 0.9546 - val_loss: 1.5827 - val_categorical_accuracy: 0.5446\n","\n","Epoch 00055: val_categorical_accuracy did not improve from 0.55155\n","Epoch 56/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1271 - categorical_accuracy: 0.9603 - val_loss: 1.5810 - val_categorical_accuracy: 0.5716\n","\n","Epoch 00056: val_categorical_accuracy improved from 0.55155 to 0.57160, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 57/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.1252 - categorical_accuracy: 0.9483 - val_loss: 1.5717 - val_categorical_accuracy: 0.5495\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.57160\n","Epoch 58/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0663 - categorical_accuracy: 0.9664 - val_loss: 1.5654 - val_categorical_accuracy: 0.5499\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.57160\n","Epoch 59/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0662 - categorical_accuracy: 0.9482 - val_loss: 1.5647 - val_categorical_accuracy: 0.5483\n","\n","Epoch 00059: val_categorical_accuracy did not improve from 0.57160\n","Epoch 60/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0502 - categorical_accuracy: 0.9628 - val_loss: 1.5594 - val_categorical_accuracy: 0.5634\n","\n","Epoch 00060: val_categorical_accuracy did not improve from 0.57160\n","Epoch 61/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0638 - categorical_accuracy: 0.9637 - val_loss: 1.5535 - val_categorical_accuracy: 0.5638\n","\n","Epoch 00061: val_categorical_accuracy did not improve from 0.57160\n","Epoch 62/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0386 - categorical_accuracy: 0.9497 - val_loss: 1.5520 - val_categorical_accuracy: 0.5618\n","\n","Epoch 00062: val_categorical_accuracy did not improve from 0.57160\n","Epoch 63/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0779 - categorical_accuracy: 0.9607 - val_loss: 1.5454 - val_categorical_accuracy: 0.5646\n","\n","Epoch 00063: val_categorical_accuracy did not improve from 0.57160\n","Epoch 64/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0138 - categorical_accuracy: 0.9634 - val_loss: 1.5406 - val_categorical_accuracy: 0.5577\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.57160\n","Epoch 65/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0129 - categorical_accuracy: 0.9597 - val_loss: 1.5366 - val_categorical_accuracy: 0.5696\n","\n","Epoch 00065: val_categorical_accuracy did not improve from 0.57160\n","Epoch 66/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0116 - categorical_accuracy: 0.9686 - val_loss: 1.5309 - val_categorical_accuracy: 0.5691\n","\n","Epoch 00066: val_categorical_accuracy did not improve from 0.57160\n","Epoch 67/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0176 - categorical_accuracy: 0.9550 - val_loss: 1.5284 - val_categorical_accuracy: 0.5839\n","\n","Epoch 00067: val_categorical_accuracy improved from 0.57160 to 0.58388, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 68/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0196 - categorical_accuracy: 0.9608 - val_loss: 1.5250 - val_categorical_accuracy: 0.5577\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.58388\n","Epoch 69/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0135 - categorical_accuracy: 0.9447 - val_loss: 1.5216 - val_categorical_accuracy: 0.5814\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.58388\n","Epoch 70/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9874 - categorical_accuracy: 0.9477 - val_loss: 1.5159 - val_categorical_accuracy: 0.5675\n","\n","Epoch 00070: val_categorical_accuracy did not improve from 0.58388\n","Epoch 71/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0019 - categorical_accuracy: 0.9725 - val_loss: 1.5113 - val_categorical_accuracy: 0.5597\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.58388\n","Epoch 72/100\n","35/35 [==============================] - 0s 7ms/step - loss: 1.0017 - categorical_accuracy: 0.9750 - val_loss: 1.5127 - val_categorical_accuracy: 0.5622\n","\n","Epoch 00072: val_categorical_accuracy did not improve from 0.58388\n","Epoch 73/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9512 - categorical_accuracy: 0.9562 - val_loss: 1.5087 - val_categorical_accuracy: 0.5659\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.58388\n","Epoch 74/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.9407 - categorical_accuracy: 0.9680 - val_loss: 1.5049 - val_categorical_accuracy: 0.5814\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.58388\n","Epoch 75/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9441 - categorical_accuracy: 0.9591 - val_loss: 1.4967 - val_categorical_accuracy: 0.5818\n","\n","Epoch 00075: val_categorical_accuracy did not improve from 0.58388\n","Epoch 76/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9525 - categorical_accuracy: 0.9757 - val_loss: 1.4972 - val_categorical_accuracy: 0.5622\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.58388\n","Epoch 77/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9099 - categorical_accuracy: 0.9753 - val_loss: 1.4954 - val_categorical_accuracy: 0.5790\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.58388\n","Epoch 78/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9164 - categorical_accuracy: 0.9754 - val_loss: 1.4891 - val_categorical_accuracy: 0.5720\n","\n","Epoch 00078: val_categorical_accuracy did not improve from 0.58388\n","Epoch 79/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9587 - categorical_accuracy: 0.9781 - val_loss: 1.4888 - val_categorical_accuracy: 0.5900\n","\n","Epoch 00079: val_categorical_accuracy improved from 0.58388 to 0.59002, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 80/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8879 - categorical_accuracy: 0.9727 - val_loss: 1.4854 - val_categorical_accuracy: 0.5618\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.59002\n","Epoch 81/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9368 - categorical_accuracy: 0.9677 - val_loss: 1.4837 - val_categorical_accuracy: 0.5945\n","\n","Epoch 00081: val_categorical_accuracy improved from 0.59002 to 0.59452, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 82/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9196 - categorical_accuracy: 0.9611 - val_loss: 1.4793 - val_categorical_accuracy: 0.5818\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.59452\n","Epoch 83/100\n","35/35 [==============================] - 0s 8ms/step - loss: 0.8657 - categorical_accuracy: 0.9829 - val_loss: 1.4737 - val_categorical_accuracy: 0.5777\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.59452\n","Epoch 84/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.9097 - categorical_accuracy: 0.9742 - val_loss: 1.4724 - val_categorical_accuracy: 0.5724\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.59452\n","Epoch 85/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8612 - categorical_accuracy: 0.9833 - val_loss: 1.4724 - val_categorical_accuracy: 0.5847\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.59452\n","Epoch 86/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8604 - categorical_accuracy: 0.9756 - val_loss: 1.4700 - val_categorical_accuracy: 0.5876\n","\n","Epoch 00086: val_categorical_accuracy did not improve from 0.59452\n","Epoch 87/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8880 - categorical_accuracy: 0.9610 - val_loss: 1.4644 - val_categorical_accuracy: 0.5917\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.59452\n","Epoch 88/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8725 - categorical_accuracy: 0.9755 - val_loss: 1.4641 - val_categorical_accuracy: 0.5724\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.59452\n","Epoch 89/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8548 - categorical_accuracy: 0.9646 - val_loss: 1.4621 - val_categorical_accuracy: 0.6039\n","\n","Epoch 00089: val_categorical_accuracy improved from 0.59452 to 0.60393, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_50_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 90/100\n","35/35 [==============================] - 0s 6ms/step - loss: 0.8823 - categorical_accuracy: 0.9755 - val_loss: 1.4589 - val_categorical_accuracy: 0.5630\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.60393\n","Epoch 91/100\n","35/35 [==============================] - 0s 10ms/step - loss: 0.8170 - categorical_accuracy: 0.9701 - val_loss: 1.4558 - val_categorical_accuracy: 0.5773\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.60393\n","Epoch 92/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8357 - categorical_accuracy: 0.9747 - val_loss: 1.4515 - val_categorical_accuracy: 0.5912\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.60393\n","Epoch 93/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8396 - categorical_accuracy: 0.9736 - val_loss: 1.4520 - val_categorical_accuracy: 0.5765\n","\n","Epoch 00093: val_categorical_accuracy did not improve from 0.60393\n","Epoch 94/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.7935 - categorical_accuracy: 0.9741 - val_loss: 1.4489 - val_categorical_accuracy: 0.5855\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.60393\n","Epoch 95/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.7891 - categorical_accuracy: 0.9866 - val_loss: 1.4473 - val_categorical_accuracy: 0.5786\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.60393\n","Epoch 96/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8101 - categorical_accuracy: 0.9803 - val_loss: 1.4450 - val_categorical_accuracy: 0.5839\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.60393\n","Epoch 97/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8117 - categorical_accuracy: 0.9898 - val_loss: 1.4425 - val_categorical_accuracy: 0.5945\n","\n","Epoch 00097: val_categorical_accuracy did not improve from 0.60393\n","Epoch 98/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.7909 - categorical_accuracy: 0.9821 - val_loss: 1.4407 - val_categorical_accuracy: 0.5782\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.60393\n","Epoch 99/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8229 - categorical_accuracy: 0.9673 - val_loss: 1.4367 - val_categorical_accuracy: 0.5810\n","\n","Epoch 00099: val_categorical_accuracy did not improve from 0.60393\n","Epoch 100/100\n","35/35 [==============================] - 0s 7ms/step - loss: 0.8004 - categorical_accuracy: 0.9742 - val_loss: 1.4400 - val_categorical_accuracy: 0.5945\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.60393\n","77/77 [==============================] - 0s 2ms/step - loss: 1.4621 - categorical_accuracy: 0.6039\n","77/77 [==============================] - 0s 1ms/step\n","\n","================================================================================================================================\n","Model training starts for data with 60 samples from each class in training set\n","=================================================================================================================================\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         concatenate[0][0]                \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 5, 5, 8, 16)  0           activation_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 5, 5, 16, 8)  0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 5, 5, 128)    0           tf.reshape_1[0][0]               \n","                                                                 activation_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 840.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [60 60 60 60 60 60 60 60 60 60 60 60 60 60]\n","\n","Total number of samples in test set 2304.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [210  41 191 155 209 209 199 143 254 188 216  73 181  35]\n","\n","X_train_transfer => (840, 128)\n","X_test_transfer  => (2304, 128)\n","y_train => (840, 14)\n","y_test  => (2304, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_10 (InputLayer)        [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","42/42 [==============================] - 1s 9ms/step - loss: 2.7326 - categorical_accuracy: 0.0055 - val_loss: 2.6776 - val_categorical_accuracy: 0.0239\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.02387, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.5992 - categorical_accuracy: 0.0665 - val_loss: 2.6018 - val_categorical_accuracy: 0.0247\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.02387 to 0.02474, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.4896 - categorical_accuracy: 0.0947 - val_loss: 2.5496 - val_categorical_accuracy: 0.0577\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.02474 to 0.05773, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.4582 - categorical_accuracy: 0.1157 - val_loss: 2.5132 - val_categorical_accuracy: 0.1176\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.05773 to 0.11762, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.3971 - categorical_accuracy: 0.2130 - val_loss: 2.4824 - val_categorical_accuracy: 0.1675\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.11762 to 0.16753, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 6/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.3650 - categorical_accuracy: 0.2627 - val_loss: 2.4569 - val_categorical_accuracy: 0.1732\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.16753 to 0.17318, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.3280 - categorical_accuracy: 0.2853 - val_loss: 2.4328 - val_categorical_accuracy: 0.1806\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.17318 to 0.18056, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 8/100\n","42/42 [==============================] - 0s 7ms/step - loss: 2.3118 - categorical_accuracy: 0.2798 - val_loss: 2.4104 - val_categorical_accuracy: 0.1866\n","\n","Epoch 00008: val_categorical_accuracy improved from 0.18056 to 0.18663, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 9/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.2698 - categorical_accuracy: 0.2755 - val_loss: 2.3909 - val_categorical_accuracy: 0.1970\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.18663 to 0.19705, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 10/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.2535 - categorical_accuracy: 0.2777 - val_loss: 2.3693 - val_categorical_accuracy: 0.2166\n","\n","Epoch 00010: val_categorical_accuracy improved from 0.19705 to 0.21658, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 11/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.1976 - categorical_accuracy: 0.3179 - val_loss: 2.3508 - val_categorical_accuracy: 0.2218\n","\n","Epoch 00011: val_categorical_accuracy improved from 0.21658 to 0.22179, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 12/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.1671 - categorical_accuracy: 0.3056 - val_loss: 2.3289 - val_categorical_accuracy: 0.2274\n","\n","Epoch 00012: val_categorical_accuracy improved from 0.22179 to 0.22743, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 13/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.1624 - categorical_accuracy: 0.2794 - val_loss: 2.3125 - val_categorical_accuracy: 0.2305\n","\n","Epoch 00013: val_categorical_accuracy improved from 0.22743 to 0.23047, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 14/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.1379 - categorical_accuracy: 0.3165 - val_loss: 2.2970 - val_categorical_accuracy: 0.2313\n","\n","Epoch 00014: val_categorical_accuracy improved from 0.23047 to 0.23134, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 15/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.1433 - categorical_accuracy: 0.2958 - val_loss: 2.2803 - val_categorical_accuracy: 0.2378\n","\n","Epoch 00015: val_categorical_accuracy improved from 0.23134 to 0.23785, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 16/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.1077 - categorical_accuracy: 0.3074 - val_loss: 2.2662 - val_categorical_accuracy: 0.2405\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.23785 to 0.24045, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 17/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.0950 - categorical_accuracy: 0.2998 - val_loss: 2.2540 - val_categorical_accuracy: 0.2461\n","\n","Epoch 00017: val_categorical_accuracy improved from 0.24045 to 0.24609, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 18/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.1011 - categorical_accuracy: 0.2934 - val_loss: 2.2408 - val_categorical_accuracy: 0.2496\n","\n","Epoch 00018: val_categorical_accuracy improved from 0.24609 to 0.24957, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 19/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.0844 - categorical_accuracy: 0.3067 - val_loss: 2.2284 - val_categorical_accuracy: 0.2535\n","\n","Epoch 00019: val_categorical_accuracy improved from 0.24957 to 0.25347, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 20/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.0166 - categorical_accuracy: 0.3133 - val_loss: 2.2152 - val_categorical_accuracy: 0.2561\n","\n","Epoch 00020: val_categorical_accuracy improved from 0.25347 to 0.25608, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 21/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.0077 - categorical_accuracy: 0.3538 - val_loss: 2.2058 - val_categorical_accuracy: 0.2513\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.25608\n","Epoch 22/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.0168 - categorical_accuracy: 0.3524 - val_loss: 2.1941 - val_categorical_accuracy: 0.2717\n","\n","Epoch 00022: val_categorical_accuracy improved from 0.25608 to 0.27170, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 23/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.9986 - categorical_accuracy: 0.3863 - val_loss: 2.1845 - val_categorical_accuracy: 0.2669\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.27170\n","Epoch 24/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.0048 - categorical_accuracy: 0.3923 - val_loss: 2.1741 - val_categorical_accuracy: 0.2786\n","\n","Epoch 00024: val_categorical_accuracy improved from 0.27170 to 0.27865, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 25/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.9793 - categorical_accuracy: 0.4069 - val_loss: 2.1649 - val_categorical_accuracy: 0.2765\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.27865\n","Epoch 26/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.9732 - categorical_accuracy: 0.3897 - val_loss: 2.1542 - val_categorical_accuracy: 0.2977\n","\n","Epoch 00026: val_categorical_accuracy improved from 0.27865 to 0.29774, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 27/100\n","42/42 [==============================] - 0s 9ms/step - loss: 1.9678 - categorical_accuracy: 0.4596 - val_loss: 2.1475 - val_categorical_accuracy: 0.2808\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.29774\n","Epoch 28/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.9237 - categorical_accuracy: 0.4530 - val_loss: 2.1391 - val_categorical_accuracy: 0.2869\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.29774\n","Epoch 29/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.9005 - categorical_accuracy: 0.4800 - val_loss: 2.1271 - val_categorical_accuracy: 0.3012\n","\n","Epoch 00029: val_categorical_accuracy improved from 0.29774 to 0.30122, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 30/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8715 - categorical_accuracy: 0.5100 - val_loss: 2.1189 - val_categorical_accuracy: 0.2999\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.30122\n","Epoch 31/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8901 - categorical_accuracy: 0.5605 - val_loss: 2.1090 - val_categorical_accuracy: 0.3234\n","\n","Epoch 00031: val_categorical_accuracy improved from 0.30122 to 0.32335, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 32/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8819 - categorical_accuracy: 0.5843 - val_loss: 2.1028 - val_categorical_accuracy: 0.3281\n","\n","Epoch 00032: val_categorical_accuracy improved from 0.32335 to 0.32812, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 33/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8787 - categorical_accuracy: 0.5542 - val_loss: 2.0937 - val_categorical_accuracy: 0.3385\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.32812 to 0.33854, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 34/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.8838 - categorical_accuracy: 0.5518 - val_loss: 2.0854 - val_categorical_accuracy: 0.3477\n","\n","Epoch 00034: val_categorical_accuracy improved from 0.33854 to 0.34766, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 35/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8353 - categorical_accuracy: 0.6165 - val_loss: 2.0786 - val_categorical_accuracy: 0.3550\n","\n","Epoch 00035: val_categorical_accuracy improved from 0.34766 to 0.35503, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 36/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8297 - categorical_accuracy: 0.6074 - val_loss: 2.0716 - val_categorical_accuracy: 0.3615\n","\n","Epoch 00036: val_categorical_accuracy improved from 0.35503 to 0.36155, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 37/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.8275 - categorical_accuracy: 0.6149 - val_loss: 2.0661 - val_categorical_accuracy: 0.3628\n","\n","Epoch 00037: val_categorical_accuracy improved from 0.36155 to 0.36285, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 38/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8189 - categorical_accuracy: 0.6104 - val_loss: 2.0569 - val_categorical_accuracy: 0.3750\n","\n","Epoch 00038: val_categorical_accuracy improved from 0.36285 to 0.37500, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 39/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8256 - categorical_accuracy: 0.6310 - val_loss: 2.0505 - val_categorical_accuracy: 0.3741\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.37500\n","Epoch 40/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7558 - categorical_accuracy: 0.6456 - val_loss: 2.0425 - val_categorical_accuracy: 0.3815\n","\n","Epoch 00040: val_categorical_accuracy improved from 0.37500 to 0.38151, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 41/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7862 - categorical_accuracy: 0.6578 - val_loss: 2.0365 - val_categorical_accuracy: 0.3906\n","\n","Epoch 00041: val_categorical_accuracy improved from 0.38151 to 0.39062, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 42/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7742 - categorical_accuracy: 0.6410 - val_loss: 2.0302 - val_categorical_accuracy: 0.3885\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.39062\n","Epoch 43/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7426 - categorical_accuracy: 0.6696 - val_loss: 2.0225 - val_categorical_accuracy: 0.3958\n","\n","Epoch 00043: val_categorical_accuracy improved from 0.39062 to 0.39583, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 44/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7791 - categorical_accuracy: 0.6444 - val_loss: 2.0177 - val_categorical_accuracy: 0.3915\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.39583\n","Epoch 45/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7655 - categorical_accuracy: 0.6999 - val_loss: 2.0092 - val_categorical_accuracy: 0.4162\n","\n","Epoch 00045: val_categorical_accuracy improved from 0.39583 to 0.41623, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 46/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7397 - categorical_accuracy: 0.6948 - val_loss: 2.0044 - val_categorical_accuracy: 0.4002\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.41623\n","Epoch 47/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7720 - categorical_accuracy: 0.6655 - val_loss: 1.9975 - val_categorical_accuracy: 0.4106\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.41623\n","Epoch 48/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7141 - categorical_accuracy: 0.7187 - val_loss: 1.9931 - val_categorical_accuracy: 0.4106\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.41623\n","Epoch 49/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7214 - categorical_accuracy: 0.6964 - val_loss: 1.9844 - val_categorical_accuracy: 0.4306\n","\n","Epoch 00049: val_categorical_accuracy improved from 0.41623 to 0.43056, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 50/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7176 - categorical_accuracy: 0.6717 - val_loss: 1.9795 - val_categorical_accuracy: 0.4288\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.43056\n","Epoch 51/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7303 - categorical_accuracy: 0.7036 - val_loss: 1.9744 - val_categorical_accuracy: 0.4384\n","\n","Epoch 00051: val_categorical_accuracy improved from 0.43056 to 0.43837, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 52/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6723 - categorical_accuracy: 0.7439 - val_loss: 1.9693 - val_categorical_accuracy: 0.4293\n","\n","Epoch 00052: val_categorical_accuracy did not improve from 0.43837\n","Epoch 53/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6762 - categorical_accuracy: 0.7210 - val_loss: 1.9630 - val_categorical_accuracy: 0.4362\n","\n","Epoch 00053: val_categorical_accuracy did not improve from 0.43837\n","Epoch 54/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6557 - categorical_accuracy: 0.7131 - val_loss: 1.9570 - val_categorical_accuracy: 0.4475\n","\n","Epoch 00054: val_categorical_accuracy improved from 0.43837 to 0.44748, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 55/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.6916 - categorical_accuracy: 0.7344 - val_loss: 1.9506 - val_categorical_accuracy: 0.4570\n","\n","Epoch 00055: val_categorical_accuracy improved from 0.44748 to 0.45703, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 56/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.6552 - categorical_accuracy: 0.7218 - val_loss: 1.9445 - val_categorical_accuracy: 0.4527\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.45703\n","Epoch 57/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6677 - categorical_accuracy: 0.7571 - val_loss: 1.9423 - val_categorical_accuracy: 0.4484\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.45703\n","Epoch 58/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6628 - categorical_accuracy: 0.7417 - val_loss: 1.9348 - val_categorical_accuracy: 0.4531\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.45703\n","Epoch 59/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6322 - categorical_accuracy: 0.7685 - val_loss: 1.9295 - val_categorical_accuracy: 0.4774\n","\n","Epoch 00059: val_categorical_accuracy improved from 0.45703 to 0.47743, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 60/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6230 - categorical_accuracy: 0.8066 - val_loss: 1.9248 - val_categorical_accuracy: 0.4661\n","\n","Epoch 00060: val_categorical_accuracy did not improve from 0.47743\n","Epoch 61/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5889 - categorical_accuracy: 0.7664 - val_loss: 1.9199 - val_categorical_accuracy: 0.4618\n","\n","Epoch 00061: val_categorical_accuracy did not improve from 0.47743\n","Epoch 62/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6234 - categorical_accuracy: 0.7956 - val_loss: 1.9140 - val_categorical_accuracy: 0.4640\n","\n","Epoch 00062: val_categorical_accuracy did not improve from 0.47743\n","Epoch 63/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5883 - categorical_accuracy: 0.7943 - val_loss: 1.9081 - val_categorical_accuracy: 0.4696\n","\n","Epoch 00063: val_categorical_accuracy did not improve from 0.47743\n","Epoch 64/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5803 - categorical_accuracy: 0.7924 - val_loss: 1.9040 - val_categorical_accuracy: 0.4735\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.47743\n","Epoch 65/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5872 - categorical_accuracy: 0.8183 - val_loss: 1.8986 - val_categorical_accuracy: 0.4952\n","\n","Epoch 00065: val_categorical_accuracy improved from 0.47743 to 0.49523, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 66/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5943 - categorical_accuracy: 0.8194 - val_loss: 1.8939 - val_categorical_accuracy: 0.4740\n","\n","Epoch 00066: val_categorical_accuracy did not improve from 0.49523\n","Epoch 67/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5965 - categorical_accuracy: 0.8143 - val_loss: 1.8904 - val_categorical_accuracy: 0.4878\n","\n","Epoch 00067: val_categorical_accuracy did not improve from 0.49523\n","Epoch 68/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5578 - categorical_accuracy: 0.8015 - val_loss: 1.8856 - val_categorical_accuracy: 0.4709\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.49523\n","Epoch 69/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.5847 - categorical_accuracy: 0.7927 - val_loss: 1.8838 - val_categorical_accuracy: 0.4848\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.49523\n","Epoch 70/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5554 - categorical_accuracy: 0.8006 - val_loss: 1.8766 - val_categorical_accuracy: 0.4887\n","\n","Epoch 00070: val_categorical_accuracy did not improve from 0.49523\n","Epoch 71/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5292 - categorical_accuracy: 0.8295 - val_loss: 1.8717 - val_categorical_accuracy: 0.4744\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.49523\n","Epoch 72/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5175 - categorical_accuracy: 0.8196 - val_loss: 1.8685 - val_categorical_accuracy: 0.5113\n","\n","Epoch 00072: val_categorical_accuracy improved from 0.49523 to 0.51128, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 73/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.5518 - categorical_accuracy: 0.8430 - val_loss: 1.8637 - val_categorical_accuracy: 0.4913\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.51128\n","Epoch 74/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5008 - categorical_accuracy: 0.8223 - val_loss: 1.8592 - val_categorical_accuracy: 0.5000\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.51128\n","Epoch 75/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5353 - categorical_accuracy: 0.8314 - val_loss: 1.8539 - val_categorical_accuracy: 0.4861\n","\n","Epoch 00075: val_categorical_accuracy did not improve from 0.51128\n","Epoch 76/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5028 - categorical_accuracy: 0.8440 - val_loss: 1.8513 - val_categorical_accuracy: 0.4818\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.51128\n","Epoch 77/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.5254 - categorical_accuracy: 0.8378 - val_loss: 1.8454 - val_categorical_accuracy: 0.5039\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.51128\n","Epoch 78/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5418 - categorical_accuracy: 0.8356 - val_loss: 1.8424 - val_categorical_accuracy: 0.5122\n","\n","Epoch 00078: val_categorical_accuracy improved from 0.51128 to 0.51215, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 79/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4646 - categorical_accuracy: 0.8455 - val_loss: 1.8379 - val_categorical_accuracy: 0.5026\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.51215\n","Epoch 80/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5171 - categorical_accuracy: 0.8475 - val_loss: 1.8331 - val_categorical_accuracy: 0.5082\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.51215\n","Epoch 81/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4925 - categorical_accuracy: 0.8331 - val_loss: 1.8292 - val_categorical_accuracy: 0.5161\n","\n","Epoch 00081: val_categorical_accuracy improved from 0.51215 to 0.51606, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 82/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4838 - categorical_accuracy: 0.8528 - val_loss: 1.8270 - val_categorical_accuracy: 0.5109\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.51606\n","Epoch 83/100\n","42/42 [==============================] - 0s 9ms/step - loss: 1.4591 - categorical_accuracy: 0.8524 - val_loss: 1.8217 - val_categorical_accuracy: 0.5148\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.51606\n","Epoch 84/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.4408 - categorical_accuracy: 0.8625 - val_loss: 1.8168 - val_categorical_accuracy: 0.5187\n","\n","Epoch 00084: val_categorical_accuracy improved from 0.51606 to 0.51866, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 85/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4365 - categorical_accuracy: 0.8527 - val_loss: 1.8134 - val_categorical_accuracy: 0.5174\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.51866\n","Epoch 86/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4527 - categorical_accuracy: 0.8708 - val_loss: 1.8091 - val_categorical_accuracy: 0.5312\n","\n","Epoch 00086: val_categorical_accuracy improved from 0.51866 to 0.53125, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 87/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.4441 - categorical_accuracy: 0.8787 - val_loss: 1.8071 - val_categorical_accuracy: 0.5265\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.53125\n","Epoch 88/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.4162 - categorical_accuracy: 0.8923 - val_loss: 1.8031 - val_categorical_accuracy: 0.5382\n","\n","Epoch 00088: val_categorical_accuracy improved from 0.53125 to 0.53819, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 89/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4351 - categorical_accuracy: 0.8646 - val_loss: 1.7963 - val_categorical_accuracy: 0.5295\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.53819\n","Epoch 90/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4184 - categorical_accuracy: 0.8572 - val_loss: 1.7942 - val_categorical_accuracy: 0.5308\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.53819\n","Epoch 91/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4259 - categorical_accuracy: 0.8695 - val_loss: 1.7893 - val_categorical_accuracy: 0.5382\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.53819\n","Epoch 92/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4104 - categorical_accuracy: 0.9048 - val_loss: 1.7872 - val_categorical_accuracy: 0.5239\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.53819\n","Epoch 93/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3960 - categorical_accuracy: 0.8826 - val_loss: 1.7829 - val_categorical_accuracy: 0.5451\n","\n","Epoch 00093: val_categorical_accuracy improved from 0.53819 to 0.54514, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 94/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4248 - categorical_accuracy: 0.9034 - val_loss: 1.7809 - val_categorical_accuracy: 0.5443\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.54514\n","Epoch 95/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3555 - categorical_accuracy: 0.8891 - val_loss: 1.7764 - val_categorical_accuracy: 0.5508\n","\n","Epoch 00095: val_categorical_accuracy improved from 0.54514 to 0.55078, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 96/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3680 - categorical_accuracy: 0.9055 - val_loss: 1.7713 - val_categorical_accuracy: 0.5447\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.55078\n","Epoch 97/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3748 - categorical_accuracy: 0.9100 - val_loss: 1.7680 - val_categorical_accuracy: 0.5412\n","\n","Epoch 00097: val_categorical_accuracy did not improve from 0.55078\n","Epoch 98/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3865 - categorical_accuracy: 0.8924 - val_loss: 1.7664 - val_categorical_accuracy: 0.5473\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.55078\n","Epoch 99/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3989 - categorical_accuracy: 0.9180 - val_loss: 1.7615 - val_categorical_accuracy: 0.5560\n","\n","Epoch 00099: val_categorical_accuracy improved from 0.55078 to 0.55599, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 100/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.3635 - categorical_accuracy: 0.9093 - val_loss: 1.7583 - val_categorical_accuracy: 0.5521\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.55599\n","72/72 [==============================] - 0s 2ms/step - loss: 1.7615 - categorical_accuracy: 0.5560\n","72/72 [==============================] - 0s 1ms/step\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 5, 5, 64)     256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 5, 5, 64)     0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 5, 5, 8)      584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 5, 5, 8)      584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 5, 5, 8)      584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 5, 5, 8)      584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 5, 5, 8)      584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 5, 5, 8)      584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 5, 5, 8)      584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 5, 5, 8)      584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 5, 5, 8)      584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 5, 5, 8)      584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 5, 5, 8)      584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 5, 5, 8)      584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 5, 5, 8)      584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 5, 5, 8)      584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 5, 5, 8)      584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 5, 5, 8)      584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 5, 5, 64)     0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 5, 5, 64)     256         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 5, 5, 64)     0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 5, 5, 128)    8320        activation_6[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 5, 5, 128)    512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 5, 5, 128)    0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 5, 5, 8, 16)  0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 5, 5, 16, 8)  0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 5, 5, 128)    512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 5, 5, 128)    0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 5, 5, 128)    0           tf.reshape_3[0][0]               \n","                                                                 activation_8[0][0]               \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 5, 5, 128)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_9[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 840.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [60 60 60 60 60 60 60 60 60 60 60 60 60 60]\n","\n","Total number of samples in test set 2304.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [210  41 191 155 209 209 199 143 254 188 216  73 181  35]\n","\n","X_train_transfer => (840, 128)\n","X_test_transfer  => (2304, 128)\n","y_train => (840, 14)\n","y_test  => (2304, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_11 (InputLayer)        [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","42/42 [==============================] - 1s 8ms/step - loss: 3.7378 - categorical_accuracy: 0.0862 - val_loss: 2.5483 - val_categorical_accuracy: 0.0430\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.04297, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","42/42 [==============================] - 0s 5ms/step - loss: 2.1304 - categorical_accuracy: 0.2916 - val_loss: 2.2545 - val_categorical_accuracy: 0.2027\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.04297 to 0.20269, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.8672 - categorical_accuracy: 0.4958 - val_loss: 2.1575 - val_categorical_accuracy: 0.2860\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.20269 to 0.28602, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7395 - categorical_accuracy: 0.6179 - val_loss: 2.0126 - val_categorical_accuracy: 0.4514\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.28602 to 0.45139, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.6099 - categorical_accuracy: 0.6059 - val_loss: 1.9360 - val_categorical_accuracy: 0.4036\n","\n","Epoch 00005: val_categorical_accuracy did not improve from 0.45139\n","Epoch 6/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.4842 - categorical_accuracy: 0.6681 - val_loss: 1.8566 - val_categorical_accuracy: 0.4735\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.45139 to 0.47352, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4115 - categorical_accuracy: 0.7327 - val_loss: 1.8041 - val_categorical_accuracy: 0.5286\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.47352 to 0.52865, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 8/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3599 - categorical_accuracy: 0.7137 - val_loss: 1.7717 - val_categorical_accuracy: 0.4375\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.52865\n","Epoch 9/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3193 - categorical_accuracy: 0.7880 - val_loss: 1.7143 - val_categorical_accuracy: 0.4340\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.52865\n","Epoch 10/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.2411 - categorical_accuracy: 0.7679 - val_loss: 1.6925 - val_categorical_accuracy: 0.4952\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.52865\n","Epoch 11/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1886 - categorical_accuracy: 0.8399 - val_loss: 1.6206 - val_categorical_accuracy: 0.5113\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.52865\n","Epoch 12/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1391 - categorical_accuracy: 0.8284 - val_loss: 1.5880 - val_categorical_accuracy: 0.5859\n","\n","Epoch 00012: val_categorical_accuracy improved from 0.52865 to 0.58594, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 13/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0961 - categorical_accuracy: 0.8760 - val_loss: 1.5621 - val_categorical_accuracy: 0.6085\n","\n","Epoch 00013: val_categorical_accuracy improved from 0.58594 to 0.60851, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 14/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0638 - categorical_accuracy: 0.9537 - val_loss: 1.5429 - val_categorical_accuracy: 0.5352\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.60851\n","Epoch 15/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0450 - categorical_accuracy: 0.9568 - val_loss: 1.5158 - val_categorical_accuracy: 0.5668\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.60851\n","Epoch 16/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0345 - categorical_accuracy: 0.9693 - val_loss: 1.4821 - val_categorical_accuracy: 0.6076\n","\n","Epoch 00016: val_categorical_accuracy did not improve from 0.60851\n","Epoch 17/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9466 - categorical_accuracy: 0.9497 - val_loss: 1.4736 - val_categorical_accuracy: 0.6549\n","\n","Epoch 00017: val_categorical_accuracy improved from 0.60851 to 0.65495, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 18/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9704 - categorical_accuracy: 0.9707 - val_loss: 1.4502 - val_categorical_accuracy: 0.6502\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.65495\n","Epoch 19/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9319 - categorical_accuracy: 0.9823 - val_loss: 1.4238 - val_categorical_accuracy: 0.6120\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.65495\n","Epoch 20/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8894 - categorical_accuracy: 0.9581 - val_loss: 1.4060 - val_categorical_accuracy: 0.6602\n","\n","Epoch 00020: val_categorical_accuracy improved from 0.65495 to 0.66016, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 21/100\n","42/42 [==============================] - 0s 10ms/step - loss: 0.8536 - categorical_accuracy: 0.9821 - val_loss: 1.4027 - val_categorical_accuracy: 0.6259\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.66016\n","Epoch 22/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8365 - categorical_accuracy: 0.9785 - val_loss: 1.3987 - val_categorical_accuracy: 0.6523\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.66016\n","Epoch 23/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.8202 - categorical_accuracy: 0.9693 - val_loss: 1.3620 - val_categorical_accuracy: 0.6506\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.66016\n","Epoch 24/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8158 - categorical_accuracy: 0.9827 - val_loss: 1.3664 - val_categorical_accuracy: 0.6450\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.66016\n","Epoch 25/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7895 - categorical_accuracy: 0.9878 - val_loss: 1.3291 - val_categorical_accuracy: 0.6302\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.66016\n","Epoch 26/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7720 - categorical_accuracy: 0.9955 - val_loss: 1.3353 - val_categorical_accuracy: 0.6367\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.66016\n","Epoch 27/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.7630 - categorical_accuracy: 0.9832 - val_loss: 1.3252 - val_categorical_accuracy: 0.6654\n","\n","Epoch 00027: val_categorical_accuracy improved from 0.66016 to 0.66536, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 28/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7420 - categorical_accuracy: 0.9868 - val_loss: 1.3090 - val_categorical_accuracy: 0.6580\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.66536\n","Epoch 29/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7220 - categorical_accuracy: 0.9891 - val_loss: 1.2810 - val_categorical_accuracy: 0.6832\n","\n","Epoch 00029: val_categorical_accuracy improved from 0.66536 to 0.68316, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 30/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.6852 - categorical_accuracy: 0.9877 - val_loss: 1.2974 - val_categorical_accuracy: 0.6636\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.68316\n","Epoch 31/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7016 - categorical_accuracy: 0.9923 - val_loss: 1.2823 - val_categorical_accuracy: 0.6159\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.68316\n","Epoch 32/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.6824 - categorical_accuracy: 0.9626 - val_loss: 1.2555 - val_categorical_accuracy: 0.6441\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.68316\n","Epoch 33/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.6490 - categorical_accuracy: 0.9840 - val_loss: 1.2592 - val_categorical_accuracy: 0.6545\n","\n","Epoch 00033: val_categorical_accuracy did not improve from 0.68316\n","Epoch 34/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.6342 - categorical_accuracy: 0.9917 - val_loss: 1.2529 - val_categorical_accuracy: 0.6580\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.68316\n","Epoch 35/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.6277 - categorical_accuracy: 0.9908 - val_loss: 1.2277 - val_categorical_accuracy: 0.6658\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.68316\n","Epoch 36/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.6042 - categorical_accuracy: 0.9965 - val_loss: 1.2301 - val_categorical_accuracy: 0.6771\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.68316\n","Epoch 37/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.6173 - categorical_accuracy: 0.9956 - val_loss: 1.2594 - val_categorical_accuracy: 0.6732\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.68316\n","Epoch 38/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.5769 - categorical_accuracy: 0.9900 - val_loss: 1.1879 - val_categorical_accuracy: 0.6997\n","\n","Epoch 00038: val_categorical_accuracy improved from 0.68316 to 0.69965, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 39/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.5746 - categorical_accuracy: 0.9920 - val_loss: 1.1967 - val_categorical_accuracy: 0.6849\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.69965\n","Epoch 40/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.5460 - categorical_accuracy: 0.9959 - val_loss: 1.1912 - val_categorical_accuracy: 0.7049\n","\n","Epoch 00040: val_categorical_accuracy improved from 0.69965 to 0.70486, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 41/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.5601 - categorical_accuracy: 0.9974 - val_loss: 1.2164 - val_categorical_accuracy: 0.6840\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.70486\n","Epoch 42/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.5609 - categorical_accuracy: 0.9932 - val_loss: 1.1993 - val_categorical_accuracy: 0.6415\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.70486\n","Epoch 43/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.5126 - categorical_accuracy: 0.9969 - val_loss: 1.1779 - val_categorical_accuracy: 0.6415\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.70486\n","Epoch 44/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.5139 - categorical_accuracy: 0.9922 - val_loss: 1.1604 - val_categorical_accuracy: 0.6866\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.70486\n","Epoch 45/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.5345 - categorical_accuracy: 0.9980 - val_loss: 1.1706 - val_categorical_accuracy: 0.6762\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.70486\n","Epoch 46/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4906 - categorical_accuracy: 0.9994 - val_loss: 1.1706 - val_categorical_accuracy: 0.6606\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.70486\n","Epoch 47/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.5015 - categorical_accuracy: 0.9943 - val_loss: 1.1601 - val_categorical_accuracy: 0.6632\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.70486\n","Epoch 48/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4665 - categorical_accuracy: 0.9992 - val_loss: 1.1362 - val_categorical_accuracy: 0.6832\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.70486\n","Epoch 49/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4701 - categorical_accuracy: 0.9908 - val_loss: 1.1611 - val_categorical_accuracy: 0.6819\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.70486\n","Epoch 50/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4765 - categorical_accuracy: 0.9950 - val_loss: 1.1408 - val_categorical_accuracy: 0.6866\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.70486\n","Epoch 51/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.4510 - categorical_accuracy: 0.9990 - val_loss: 1.1215 - val_categorical_accuracy: 0.6871\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.70486\n","Epoch 52/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4670 - categorical_accuracy: 0.9976 - val_loss: 1.1288 - val_categorical_accuracy: 0.6723\n","\n","Epoch 00052: val_categorical_accuracy did not improve from 0.70486\n","Epoch 53/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4561 - categorical_accuracy: 0.9954 - val_loss: 1.1325 - val_categorical_accuracy: 0.7131\n","\n","Epoch 00053: val_categorical_accuracy improved from 0.70486 to 0.71311, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 54/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4406 - categorical_accuracy: 0.9948 - val_loss: 1.1246 - val_categorical_accuracy: 0.6736\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.71311\n","Epoch 55/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4209 - categorical_accuracy: 0.9983 - val_loss: 1.1231 - val_categorical_accuracy: 0.6740\n","\n","Epoch 00055: val_categorical_accuracy did not improve from 0.71311\n","Epoch 56/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4141 - categorical_accuracy: 0.9976 - val_loss: 1.0922 - val_categorical_accuracy: 0.6923\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.71311\n","Epoch 57/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4112 - categorical_accuracy: 0.9931 - val_loss: 1.0890 - val_categorical_accuracy: 0.6884\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.71311\n","Epoch 58/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.4127 - categorical_accuracy: 0.9968 - val_loss: 1.1193 - val_categorical_accuracy: 0.6675\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.71311\n","Epoch 59/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.4031 - categorical_accuracy: 0.9980 - val_loss: 1.1058 - val_categorical_accuracy: 0.6697\n","\n","Epoch 00059: val_categorical_accuracy did not improve from 0.71311\n","Epoch 60/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3924 - categorical_accuracy: 0.9931 - val_loss: 1.1079 - val_categorical_accuracy: 0.6649\n","\n","Epoch 00060: val_categorical_accuracy did not improve from 0.71311\n","Epoch 61/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3919 - categorical_accuracy: 0.9960 - val_loss: 1.0809 - val_categorical_accuracy: 0.6706\n","\n","Epoch 00061: val_categorical_accuracy did not improve from 0.71311\n","Epoch 62/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3842 - categorical_accuracy: 0.9998 - val_loss: 1.0976 - val_categorical_accuracy: 0.6910\n","\n","Epoch 00062: val_categorical_accuracy did not improve from 0.71311\n","Epoch 63/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3701 - categorical_accuracy: 0.9989 - val_loss: 1.0709 - val_categorical_accuracy: 0.7109\n","\n","Epoch 00063: val_categorical_accuracy did not improve from 0.71311\n","Epoch 64/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3684 - categorical_accuracy: 0.9991 - val_loss: 1.0838 - val_categorical_accuracy: 0.6836\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.71311\n","Epoch 65/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3567 - categorical_accuracy: 0.9986 - val_loss: 1.0845 - val_categorical_accuracy: 0.6736\n","\n","Epoch 00065: val_categorical_accuracy did not improve from 0.71311\n","Epoch 66/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3651 - categorical_accuracy: 0.9973 - val_loss: 1.0709 - val_categorical_accuracy: 0.6862\n","\n","Epoch 00066: val_categorical_accuracy did not improve from 0.71311\n","Epoch 67/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.3495 - categorical_accuracy: 0.9979 - val_loss: 1.0838 - val_categorical_accuracy: 0.6736\n","\n","Epoch 00067: val_categorical_accuracy did not improve from 0.71311\n","Epoch 68/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3441 - categorical_accuracy: 0.9991 - val_loss: 1.0703 - val_categorical_accuracy: 0.6819\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.71311\n","Epoch 69/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3557 - categorical_accuracy: 0.9976 - val_loss: 1.0729 - val_categorical_accuracy: 0.6875\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.71311\n","Epoch 70/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3337 - categorical_accuracy: 0.9978 - val_loss: 1.0536 - val_categorical_accuracy: 0.6797\n","\n","Epoch 00070: val_categorical_accuracy did not improve from 0.71311\n","Epoch 71/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.3352 - categorical_accuracy: 0.9991 - val_loss: 1.0541 - val_categorical_accuracy: 0.6836\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.71311\n","Epoch 72/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3349 - categorical_accuracy: 0.9992 - val_loss: 1.0596 - val_categorical_accuracy: 0.6884\n","\n","Epoch 00072: val_categorical_accuracy did not improve from 0.71311\n","Epoch 73/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3268 - categorical_accuracy: 0.9976 - val_loss: 1.0587 - val_categorical_accuracy: 0.6810\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.71311\n","Epoch 74/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.3139 - categorical_accuracy: 0.9996 - val_loss: 1.0498 - val_categorical_accuracy: 0.6827\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.71311\n","Epoch 75/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.3212 - categorical_accuracy: 0.9991 - val_loss: 1.0421 - val_categorical_accuracy: 0.6931\n","\n","Epoch 00075: val_categorical_accuracy did not improve from 0.71311\n","Epoch 76/100\n","42/42 [==============================] - 0s 8ms/step - loss: 0.3114 - categorical_accuracy: 0.9992 - val_loss: 1.0550 - val_categorical_accuracy: 0.6944\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.71311\n","Epoch 77/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3080 - categorical_accuracy: 0.9996 - val_loss: 1.0428 - val_categorical_accuracy: 0.6966\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.71311\n","Epoch 78/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.3082 - categorical_accuracy: 0.9975 - val_loss: 1.0479 - val_categorical_accuracy: 0.6962\n","\n","Epoch 00078: val_categorical_accuracy did not improve from 0.71311\n","Epoch 79/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2927 - categorical_accuracy: 0.9976 - val_loss: 1.0283 - val_categorical_accuracy: 0.6975\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.71311\n","Epoch 80/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2890 - categorical_accuracy: 0.9977 - val_loss: 1.0414 - val_categorical_accuracy: 0.6975\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.71311\n","Epoch 81/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2958 - categorical_accuracy: 0.9946 - val_loss: 1.0429 - val_categorical_accuracy: 0.6897\n","\n","Epoch 00081: val_categorical_accuracy did not improve from 0.71311\n","Epoch 82/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2802 - categorical_accuracy: 0.9991 - val_loss: 1.0276 - val_categorical_accuracy: 0.6866\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.71311\n","Epoch 83/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2887 - categorical_accuracy: 0.9987 - val_loss: 1.0251 - val_categorical_accuracy: 0.6892\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.71311\n","Epoch 84/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2763 - categorical_accuracy: 0.9972 - val_loss: 1.0199 - val_categorical_accuracy: 0.6858\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.71311\n","Epoch 85/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2728 - categorical_accuracy: 0.9976 - val_loss: 1.0269 - val_categorical_accuracy: 0.6923\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.71311\n","Epoch 86/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2685 - categorical_accuracy: 0.9991 - val_loss: 1.0218 - val_categorical_accuracy: 0.7023\n","\n","Epoch 00086: val_categorical_accuracy did not improve from 0.71311\n","Epoch 87/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2615 - categorical_accuracy: 0.9963 - val_loss: 1.0045 - val_categorical_accuracy: 0.7014\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.71311\n","Epoch 88/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2677 - categorical_accuracy: 0.9976 - val_loss: 1.0241 - val_categorical_accuracy: 0.6992\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.71311\n","Epoch 89/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2560 - categorical_accuracy: 0.9987 - val_loss: 1.0153 - val_categorical_accuracy: 0.6936\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.71311\n","Epoch 90/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2636 - categorical_accuracy: 0.9952 - val_loss: 1.0038 - val_categorical_accuracy: 0.7005\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.71311\n","Epoch 91/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2543 - categorical_accuracy: 0.9968 - val_loss: 1.0081 - val_categorical_accuracy: 0.6832\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.71311\n","Epoch 92/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2676 - categorical_accuracy: 0.9936 - val_loss: 1.0108 - val_categorical_accuracy: 0.6957\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.71311\n","Epoch 93/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2406 - categorical_accuracy: 0.9989 - val_loss: 1.0162 - val_categorical_accuracy: 0.6988\n","\n","Epoch 00093: val_categorical_accuracy did not improve from 0.71311\n","Epoch 94/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2443 - categorical_accuracy: 0.9976 - val_loss: 1.0186 - val_categorical_accuracy: 0.6949\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.71311\n","Epoch 95/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2550 - categorical_accuracy: 0.9975 - val_loss: 0.9960 - val_categorical_accuracy: 0.6888\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.71311\n","Epoch 96/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2390 - categorical_accuracy: 0.9994 - val_loss: 1.0158 - val_categorical_accuracy: 0.6988\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.71311\n","Epoch 97/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2400 - categorical_accuracy: 0.9989 - val_loss: 1.0019 - val_categorical_accuracy: 0.7031\n","\n","Epoch 00097: val_categorical_accuracy did not improve from 0.71311\n","Epoch 98/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2379 - categorical_accuracy: 0.9988 - val_loss: 1.0036 - val_categorical_accuracy: 0.7036\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.71311\n","Epoch 99/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.2295 - categorical_accuracy: 0.9990 - val_loss: 1.0096 - val_categorical_accuracy: 0.6871\n","\n","Epoch 00099: val_categorical_accuracy did not improve from 0.71311\n","Epoch 100/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.2349 - categorical_accuracy: 0.9961 - val_loss: 0.9942 - val_categorical_accuracy: 0.6910\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.71311\n","72/72 [==============================] - 0s 2ms/step - loss: 1.1325 - categorical_accuracy: 0.7131\n","72/72 [==============================] - 0s 1ms/step\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 5, 5, 64)     256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 5, 5, 64)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 5, 5, 8)      584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 5, 5, 8)      584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 5, 5, 8)      584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 5, 5, 8)      584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 5, 5, 8)      584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 5, 5, 8)      584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 5, 5, 8)      584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 5, 5, 8)      584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 5, 5, 8)      584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 5, 5, 8)      584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 5, 5, 8)      584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 5, 5, 8)      584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 5, 5, 8)      584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 5, 5, 8)      584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 5, 5, 8)      584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 5, 5, 8)      584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 5, 5, 64)     0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 5, 5, 64)     256         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 5, 5, 64)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 5, 5, 128)    8320        activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 5, 5, 128)    0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 5, 5, 8, 16)  0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 5, 5, 16, 8)  0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 5, 5, 128)    512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 5, 5, 128)    0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 5, 5, 128)    0           tf.reshape_5[0][0]               \n","                                                                 activation_13[0][0]              \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 5, 5, 128)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_14[0][0]              \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 840.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [60 60 60 60 60 60 60 60 60 60 60 60 60 60]\n","\n","Total number of samples in test set 2304.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [210  41 191 155 209 209 199 143 254 188 216  73 181  35]\n","\n","X_train_transfer => (840, 128)\n","X_test_transfer  => (2304, 128)\n","y_train => (840, 14)\n","y_test  => (2304, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_12 (InputLayer)        [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","42/42 [==============================] - 1s 9ms/step - loss: 3.0893 - categorical_accuracy: 0.0806 - val_loss: 2.5741 - val_categorical_accuracy: 0.1836\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.18359, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.4063 - categorical_accuracy: 0.2289 - val_loss: 2.4096 - val_categorical_accuracy: 0.3030\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.18359 to 0.30295, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.2697 - categorical_accuracy: 0.4538 - val_loss: 2.3164 - val_categorical_accuracy: 0.3624\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.30295 to 0.36241, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.1482 - categorical_accuracy: 0.4522 - val_loss: 2.2697 - val_categorical_accuracy: 0.4162\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.36241 to 0.41623, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.0905 - categorical_accuracy: 0.5248 - val_loss: 2.2185 - val_categorical_accuracy: 0.3976\n","\n","Epoch 00005: val_categorical_accuracy did not improve from 0.41623\n","Epoch 6/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.0520 - categorical_accuracy: 0.4986 - val_loss: 2.1831 - val_categorical_accuracy: 0.4401\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.41623 to 0.44010, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","42/42 [==============================] - 0s 6ms/step - loss: 2.0024 - categorical_accuracy: 0.5228 - val_loss: 2.1435 - val_categorical_accuracy: 0.4779\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.44010 to 0.47786, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 8/100\n","42/42 [==============================] - 0s 7ms/step - loss: 1.9094 - categorical_accuracy: 0.6064 - val_loss: 2.1080 - val_categorical_accuracy: 0.4679\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.47786\n","Epoch 9/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.9259 - categorical_accuracy: 0.6024 - val_loss: 2.0891 - val_categorical_accuracy: 0.4362\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.47786\n","Epoch 10/100\n","42/42 [==============================] - 0s 11ms/step - loss: 1.8503 - categorical_accuracy: 0.6036 - val_loss: 2.0544 - val_categorical_accuracy: 0.4414\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.47786\n","Epoch 11/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7903 - categorical_accuracy: 0.6113 - val_loss: 2.0331 - val_categorical_accuracy: 0.4592\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.47786\n","Epoch 12/100\n","42/42 [==============================] - 0s 7ms/step - loss: 1.7989 - categorical_accuracy: 0.6372 - val_loss: 2.0114 - val_categorical_accuracy: 0.4470\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.47786\n","Epoch 13/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7774 - categorical_accuracy: 0.5837 - val_loss: 1.9934 - val_categorical_accuracy: 0.4284\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.47786\n","Epoch 14/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6789 - categorical_accuracy: 0.6218 - val_loss: 1.9678 - val_categorical_accuracy: 0.4740\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.47786\n","Epoch 15/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.7059 - categorical_accuracy: 0.6505 - val_loss: 1.9478 - val_categorical_accuracy: 0.4618\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.47786\n","Epoch 16/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6567 - categorical_accuracy: 0.6659 - val_loss: 1.9324 - val_categorical_accuracy: 0.4336\n","\n","Epoch 00016: val_categorical_accuracy did not improve from 0.47786\n","Epoch 17/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6560 - categorical_accuracy: 0.6169 - val_loss: 1.9124 - val_categorical_accuracy: 0.4523\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.47786\n","Epoch 18/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.6498 - categorical_accuracy: 0.6848 - val_loss: 1.8956 - val_categorical_accuracy: 0.4562\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.47786\n","Epoch 19/100\n","42/42 [==============================] - 0s 7ms/step - loss: 1.6119 - categorical_accuracy: 0.6397 - val_loss: 1.8787 - val_categorical_accuracy: 0.4905\n","\n","Epoch 00019: val_categorical_accuracy improved from 0.47786 to 0.49045, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 20/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5239 - categorical_accuracy: 0.7271 - val_loss: 1.8591 - val_categorical_accuracy: 0.4991\n","\n","Epoch 00020: val_categorical_accuracy improved from 0.49045 to 0.49913, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 21/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5401 - categorical_accuracy: 0.7523 - val_loss: 1.8407 - val_categorical_accuracy: 0.4761\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.49913\n","Epoch 22/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5400 - categorical_accuracy: 0.7291 - val_loss: 1.8277 - val_categorical_accuracy: 0.4779\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.49913\n","Epoch 23/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.5094 - categorical_accuracy: 0.7153 - val_loss: 1.8145 - val_categorical_accuracy: 0.4926\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.49913\n","Epoch 24/100\n","42/42 [==============================] - 0s 7ms/step - loss: 1.4578 - categorical_accuracy: 0.7385 - val_loss: 1.7987 - val_categorical_accuracy: 0.5113\n","\n","Epoch 00024: val_categorical_accuracy improved from 0.49913 to 0.51128, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 25/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4365 - categorical_accuracy: 0.7575 - val_loss: 1.7869 - val_categorical_accuracy: 0.4913\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.51128\n","Epoch 26/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4110 - categorical_accuracy: 0.7611 - val_loss: 1.7728 - val_categorical_accuracy: 0.4983\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.51128\n","Epoch 27/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4372 - categorical_accuracy: 0.7770 - val_loss: 1.7646 - val_categorical_accuracy: 0.4961\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.51128\n","Epoch 28/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.4293 - categorical_accuracy: 0.7480 - val_loss: 1.7476 - val_categorical_accuracy: 0.5039\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.51128\n","Epoch 29/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3701 - categorical_accuracy: 0.8034 - val_loss: 1.7382 - val_categorical_accuracy: 0.5095\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.51128\n","Epoch 30/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3297 - categorical_accuracy: 0.8475 - val_loss: 1.7237 - val_categorical_accuracy: 0.5156\n","\n","Epoch 00030: val_categorical_accuracy improved from 0.51128 to 0.51562, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 31/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3519 - categorical_accuracy: 0.7968 - val_loss: 1.7169 - val_categorical_accuracy: 0.5156\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.51562\n","Epoch 32/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3157 - categorical_accuracy: 0.8377 - val_loss: 1.7030 - val_categorical_accuracy: 0.5152\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.51562\n","Epoch 33/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.3230 - categorical_accuracy: 0.8262 - val_loss: 1.6996 - val_categorical_accuracy: 0.5352\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.51562 to 0.53516, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 34/100\n","42/42 [==============================] - 0s 7ms/step - loss: 1.3057 - categorical_accuracy: 0.8377 - val_loss: 1.6842 - val_categorical_accuracy: 0.5260\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.53516\n","Epoch 35/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.2932 - categorical_accuracy: 0.8573 - val_loss: 1.6805 - val_categorical_accuracy: 0.5161\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.53516\n","Epoch 36/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.2492 - categorical_accuracy: 0.8477 - val_loss: 1.6687 - val_categorical_accuracy: 0.5317\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.53516\n","Epoch 37/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.2454 - categorical_accuracy: 0.8876 - val_loss: 1.6569 - val_categorical_accuracy: 0.5239\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.53516\n","Epoch 38/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.2429 - categorical_accuracy: 0.8696 - val_loss: 1.6525 - val_categorical_accuracy: 0.5269\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.53516\n","Epoch 39/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.2001 - categorical_accuracy: 0.8853 - val_loss: 1.6407 - val_categorical_accuracy: 0.5273\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.53516\n","Epoch 40/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.2232 - categorical_accuracy: 0.8869 - val_loss: 1.6319 - val_categorical_accuracy: 0.5269\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.53516\n","Epoch 41/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1668 - categorical_accuracy: 0.9189 - val_loss: 1.6266 - val_categorical_accuracy: 0.5386\n","\n","Epoch 00041: val_categorical_accuracy improved from 0.53516 to 0.53863, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 42/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.1709 - categorical_accuracy: 0.8945 - val_loss: 1.6149 - val_categorical_accuracy: 0.5308\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.53863\n","Epoch 43/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1670 - categorical_accuracy: 0.9094 - val_loss: 1.6126 - val_categorical_accuracy: 0.5273\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.53863\n","Epoch 44/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1372 - categorical_accuracy: 0.9111 - val_loss: 1.6066 - val_categorical_accuracy: 0.5347\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.53863\n","Epoch 45/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1847 - categorical_accuracy: 0.9372 - val_loss: 1.5979 - val_categorical_accuracy: 0.5417\n","\n","Epoch 00045: val_categorical_accuracy improved from 0.53863 to 0.54167, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 46/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1127 - categorical_accuracy: 0.9281 - val_loss: 1.5939 - val_categorical_accuracy: 0.5308\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.54167\n","Epoch 47/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.1220 - categorical_accuracy: 0.9116 - val_loss: 1.5851 - val_categorical_accuracy: 0.5373\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.54167\n","Epoch 48/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1271 - categorical_accuracy: 0.9585 - val_loss: 1.5814 - val_categorical_accuracy: 0.5312\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.54167\n","Epoch 49/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.1179 - categorical_accuracy: 0.9316 - val_loss: 1.5721 - val_categorical_accuracy: 0.5430\n","\n","Epoch 00049: val_categorical_accuracy improved from 0.54167 to 0.54297, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 50/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0779 - categorical_accuracy: 0.9392 - val_loss: 1.5671 - val_categorical_accuracy: 0.5412\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.54297\n","Epoch 51/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0532 - categorical_accuracy: 0.9403 - val_loss: 1.5587 - val_categorical_accuracy: 0.5369\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.54297\n","Epoch 52/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0482 - categorical_accuracy: 0.9558 - val_loss: 1.5563 - val_categorical_accuracy: 0.5365\n","\n","Epoch 00052: val_categorical_accuracy did not improve from 0.54297\n","Epoch 53/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0718 - categorical_accuracy: 0.9452 - val_loss: 1.5538 - val_categorical_accuracy: 0.5586\n","\n","Epoch 00053: val_categorical_accuracy improved from 0.54297 to 0.55859, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 54/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.0544 - categorical_accuracy: 0.9397 - val_loss: 1.5431 - val_categorical_accuracy: 0.5469\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.55859\n","Epoch 55/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.0584 - categorical_accuracy: 0.9571 - val_loss: 1.5340 - val_categorical_accuracy: 0.5590\n","\n","Epoch 00055: val_categorical_accuracy improved from 0.55859 to 0.55903, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 56/100\n","42/42 [==============================] - 0s 8ms/step - loss: 1.0031 - categorical_accuracy: 0.9664 - val_loss: 1.5378 - val_categorical_accuracy: 0.5395\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.55903\n","Epoch 57/100\n","42/42 [==============================] - 0s 5ms/step - loss: 1.0373 - categorical_accuracy: 0.9520 - val_loss: 1.5319 - val_categorical_accuracy: 0.5369\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.55903\n","Epoch 58/100\n","42/42 [==============================] - 0s 6ms/step - loss: 1.0221 - categorical_accuracy: 0.9388 - val_loss: 1.5275 - val_categorical_accuracy: 0.5399\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.55903\n","Epoch 59/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9862 - categorical_accuracy: 0.9582 - val_loss: 1.5216 - val_categorical_accuracy: 0.5634\n","\n","Epoch 00059: val_categorical_accuracy improved from 0.55903 to 0.56337, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 60/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.9619 - categorical_accuracy: 0.9584 - val_loss: 1.5170 - val_categorical_accuracy: 0.5521\n","\n","Epoch 00060: val_categorical_accuracy did not improve from 0.56337\n","Epoch 61/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9689 - categorical_accuracy: 0.9698 - val_loss: 1.5167 - val_categorical_accuracy: 0.5781\n","\n","Epoch 00061: val_categorical_accuracy improved from 0.56337 to 0.57812, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 62/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.9562 - categorical_accuracy: 0.9694 - val_loss: 1.5107 - val_categorical_accuracy: 0.5543\n","\n","Epoch 00062: val_categorical_accuracy did not improve from 0.57812\n","Epoch 63/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.9603 - categorical_accuracy: 0.9600 - val_loss: 1.5082 - val_categorical_accuracy: 0.5512\n","\n","Epoch 00063: val_categorical_accuracy did not improve from 0.57812\n","Epoch 64/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.9284 - categorical_accuracy: 0.9571 - val_loss: 1.5041 - val_categorical_accuracy: 0.5530\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.57812\n","Epoch 65/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9411 - categorical_accuracy: 0.9711 - val_loss: 1.4980 - val_categorical_accuracy: 0.5521\n","\n","Epoch 00065: val_categorical_accuracy did not improve from 0.57812\n","Epoch 66/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9081 - categorical_accuracy: 0.9650 - val_loss: 1.4959 - val_categorical_accuracy: 0.5612\n","\n","Epoch 00066: val_categorical_accuracy did not improve from 0.57812\n","Epoch 67/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9280 - categorical_accuracy: 0.9757 - val_loss: 1.4884 - val_categorical_accuracy: 0.5508\n","\n","Epoch 00067: val_categorical_accuracy did not improve from 0.57812\n","Epoch 68/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.9064 - categorical_accuracy: 0.9688 - val_loss: 1.4855 - val_categorical_accuracy: 0.5677\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.57812\n","Epoch 69/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8993 - categorical_accuracy: 0.9676 - val_loss: 1.4838 - val_categorical_accuracy: 0.5521\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.57812\n","Epoch 70/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8785 - categorical_accuracy: 0.9622 - val_loss: 1.4828 - val_categorical_accuracy: 0.5486\n","\n","Epoch 00070: val_categorical_accuracy did not improve from 0.57812\n","Epoch 71/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8840 - categorical_accuracy: 0.9740 - val_loss: 1.4798 - val_categorical_accuracy: 0.5508\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.57812\n","Epoch 72/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8814 - categorical_accuracy: 0.9645 - val_loss: 1.4777 - val_categorical_accuracy: 0.5612\n","\n","Epoch 00072: val_categorical_accuracy did not improve from 0.57812\n","Epoch 73/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8847 - categorical_accuracy: 0.9644 - val_loss: 1.4749 - val_categorical_accuracy: 0.5694\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.57812\n","Epoch 74/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.8748 - categorical_accuracy: 0.9630 - val_loss: 1.4717 - val_categorical_accuracy: 0.5586\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.57812\n","Epoch 75/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8551 - categorical_accuracy: 0.9805 - val_loss: 1.4653 - val_categorical_accuracy: 0.5872\n","\n","Epoch 00075: val_categorical_accuracy improved from 0.57812 to 0.58724, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 76/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.8602 - categorical_accuracy: 0.9733 - val_loss: 1.4609 - val_categorical_accuracy: 0.5681\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.58724\n","Epoch 77/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8677 - categorical_accuracy: 0.9690 - val_loss: 1.4633 - val_categorical_accuracy: 0.5690\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.58724\n","Epoch 78/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8507 - categorical_accuracy: 0.9703 - val_loss: 1.4551 - val_categorical_accuracy: 0.5716\n","\n","Epoch 00078: val_categorical_accuracy did not improve from 0.58724\n","Epoch 79/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8511 - categorical_accuracy: 0.9777 - val_loss: 1.4517 - val_categorical_accuracy: 0.5755\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.58724\n","Epoch 80/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8222 - categorical_accuracy: 0.9662 - val_loss: 1.4485 - val_categorical_accuracy: 0.5729\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.58724\n","Epoch 81/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8429 - categorical_accuracy: 0.9749 - val_loss: 1.4469 - val_categorical_accuracy: 0.5664\n","\n","Epoch 00081: val_categorical_accuracy did not improve from 0.58724\n","Epoch 82/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.8242 - categorical_accuracy: 0.9771 - val_loss: 1.4469 - val_categorical_accuracy: 0.5812\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.58724\n","Epoch 83/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8010 - categorical_accuracy: 0.9716 - val_loss: 1.4481 - val_categorical_accuracy: 0.5846\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.58724\n","Epoch 84/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8253 - categorical_accuracy: 0.9693 - val_loss: 1.4456 - val_categorical_accuracy: 0.5660\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.58724\n","Epoch 85/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.8179 - categorical_accuracy: 0.9667 - val_loss: 1.4363 - val_categorical_accuracy: 0.5872\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.58724\n","Epoch 86/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7944 - categorical_accuracy: 0.9655 - val_loss: 1.4331 - val_categorical_accuracy: 0.5881\n","\n","Epoch 00086: val_categorical_accuracy improved from 0.58724 to 0.58811, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 87/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7623 - categorical_accuracy: 0.9694 - val_loss: 1.4333 - val_categorical_accuracy: 0.5929\n","\n","Epoch 00087: val_categorical_accuracy improved from 0.58811 to 0.59288, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 88/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7805 - categorical_accuracy: 0.9718 - val_loss: 1.4331 - val_categorical_accuracy: 0.5829\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.59288\n","Epoch 89/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.7917 - categorical_accuracy: 0.9721 - val_loss: 1.4330 - val_categorical_accuracy: 0.5712\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.59288\n","Epoch 90/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7868 - categorical_accuracy: 0.9713 - val_loss: 1.4281 - val_categorical_accuracy: 0.5734\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.59288\n","Epoch 91/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7572 - categorical_accuracy: 0.9784 - val_loss: 1.4243 - val_categorical_accuracy: 0.5794\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.59288\n","Epoch 92/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.6996 - categorical_accuracy: 0.9808 - val_loss: 1.4191 - val_categorical_accuracy: 0.5877\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.59288\n","Epoch 93/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7467 - categorical_accuracy: 0.9732 - val_loss: 1.4194 - val_categorical_accuracy: 0.5916\n","\n","Epoch 00093: val_categorical_accuracy did not improve from 0.59288\n","Epoch 94/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.7345 - categorical_accuracy: 0.9809 - val_loss: 1.4168 - val_categorical_accuracy: 0.5911\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.59288\n","Epoch 95/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7509 - categorical_accuracy: 0.9819 - val_loss: 1.4161 - val_categorical_accuracy: 0.5938\n","\n","Epoch 00095: val_categorical_accuracy improved from 0.59288 to 0.59375, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 96/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.7274 - categorical_accuracy: 0.9750 - val_loss: 1.4141 - val_categorical_accuracy: 0.5812\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.59375\n","Epoch 97/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.7395 - categorical_accuracy: 0.9796 - val_loss: 1.4137 - val_categorical_accuracy: 0.5985\n","\n","Epoch 00097: val_categorical_accuracy improved from 0.59375 to 0.59852, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_60_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 98/100\n","42/42 [==============================] - 0s 5ms/step - loss: 0.7504 - categorical_accuracy: 0.9757 - val_loss: 1.4084 - val_categorical_accuracy: 0.5920\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.59852\n","Epoch 99/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7295 - categorical_accuracy: 0.9695 - val_loss: 1.4135 - val_categorical_accuracy: 0.5872\n","\n","Epoch 00099: val_categorical_accuracy did not improve from 0.59852\n","Epoch 100/100\n","42/42 [==============================] - 0s 6ms/step - loss: 0.7439 - categorical_accuracy: 0.9664 - val_loss: 1.4077 - val_categorical_accuracy: 0.5777\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.59852\n","72/72 [==============================] - 0s 1ms/step - loss: 1.4137 - categorical_accuracy: 0.5985\n","72/72 [==============================] - 0s 1ms/step\n","\n","================================================================================================================================\n","Model training starts for data with 70 samples from each class in training set\n","=================================================================================================================================\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         concatenate[0][0]                \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 5, 5, 8, 16)  0           activation_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 5, 5, 16, 8)  0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 5, 5, 128)    0           tf.reshape_1[0][0]               \n","                                                                 activation_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 980.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [70 70 70 70 70 70 70 70 70 70 70 70 70 70]\n","\n","Total number of samples in test set 2164.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [200  31 181 145 199 199 189 133 244 178 206  63 171  25]\n","\n","X_train_transfer => (980, 128)\n","X_test_transfer  => (2164, 128)\n","y_train => (980, 14)\n","y_test  => (2164, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_13 (InputLayer)        [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","49/49 [==============================] - 1s 7ms/step - loss: 2.7226 - categorical_accuracy: 0.0166 - val_loss: 2.6642 - val_categorical_accuracy: 0.0199\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.01987, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.5684 - categorical_accuracy: 0.0733 - val_loss: 2.5887 - val_categorical_accuracy: 0.0222\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.01987 to 0.02218, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.4879 - categorical_accuracy: 0.0897 - val_loss: 2.5386 - val_categorical_accuracy: 0.0809\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.02218 to 0.08087, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.4424 - categorical_accuracy: 0.1458 - val_loss: 2.5021 - val_categorical_accuracy: 0.1497\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.08087 to 0.14972, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.3882 - categorical_accuracy: 0.2562 - val_loss: 2.4696 - val_categorical_accuracy: 0.1631\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.14972 to 0.16312, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 6/100\n","49/49 [==============================] - 0s 4ms/step - loss: 2.3349 - categorical_accuracy: 0.2889 - val_loss: 2.4375 - val_categorical_accuracy: 0.1733\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.16312 to 0.17329, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.3110 - categorical_accuracy: 0.2756 - val_loss: 2.4130 - val_categorical_accuracy: 0.1848\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.17329 to 0.18484, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 8/100\n","49/49 [==============================] - 0s 4ms/step - loss: 2.2507 - categorical_accuracy: 0.3047 - val_loss: 2.3865 - val_categorical_accuracy: 0.2056\n","\n","Epoch 00008: val_categorical_accuracy improved from 0.18484 to 0.20564, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 9/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.2284 - categorical_accuracy: 0.2989 - val_loss: 2.3653 - val_categorical_accuracy: 0.2144\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.20564 to 0.21442, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 10/100\n","49/49 [==============================] - 0s 4ms/step - loss: 2.2065 - categorical_accuracy: 0.2992 - val_loss: 2.3436 - val_categorical_accuracy: 0.2218\n","\n","Epoch 00010: val_categorical_accuracy improved from 0.21442 to 0.22181, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 11/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.1896 - categorical_accuracy: 0.2831 - val_loss: 2.3222 - val_categorical_accuracy: 0.2278\n","\n","Epoch 00011: val_categorical_accuracy improved from 0.22181 to 0.22782, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 12/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.1458 - categorical_accuracy: 0.3104 - val_loss: 2.3030 - val_categorical_accuracy: 0.2292\n","\n","Epoch 00012: val_categorical_accuracy improved from 0.22782 to 0.22921, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 13/100\n","49/49 [==============================] - 0s 4ms/step - loss: 2.1262 - categorical_accuracy: 0.3144 - val_loss: 2.2856 - val_categorical_accuracy: 0.2352\n","\n","Epoch 00013: val_categorical_accuracy improved from 0.22921 to 0.23521, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 14/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.0991 - categorical_accuracy: 0.3386 - val_loss: 2.2650 - val_categorical_accuracy: 0.2408\n","\n","Epoch 00014: val_categorical_accuracy improved from 0.23521 to 0.24076, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 15/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.0956 - categorical_accuracy: 0.3323 - val_loss: 2.2531 - val_categorical_accuracy: 0.2421\n","\n","Epoch 00015: val_categorical_accuracy improved from 0.24076 to 0.24214, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 16/100\n","49/49 [==============================] - 0s 4ms/step - loss: 2.0979 - categorical_accuracy: 0.3075 - val_loss: 2.2352 - val_categorical_accuracy: 0.2426\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.24214 to 0.24261, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 17/100\n","49/49 [==============================] - 0s 4ms/step - loss: 2.0642 - categorical_accuracy: 0.3618 - val_loss: 2.2235 - val_categorical_accuracy: 0.2528\n","\n","Epoch 00017: val_categorical_accuracy improved from 0.24261 to 0.25277, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 18/100\n","49/49 [==============================] - 0s 4ms/step - loss: 2.0344 - categorical_accuracy: 0.3539 - val_loss: 2.2108 - val_categorical_accuracy: 0.2477\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.25277\n","Epoch 19/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.0110 - categorical_accuracy: 0.3746 - val_loss: 2.1995 - val_categorical_accuracy: 0.2528\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.25277\n","Epoch 20/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.0108 - categorical_accuracy: 0.3908 - val_loss: 2.1876 - val_categorical_accuracy: 0.2666\n","\n","Epoch 00020: val_categorical_accuracy improved from 0.25277 to 0.26664, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 21/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9782 - categorical_accuracy: 0.4175 - val_loss: 2.1753 - val_categorical_accuracy: 0.2625\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.26664\n","Epoch 22/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9852 - categorical_accuracy: 0.4257 - val_loss: 2.1631 - val_categorical_accuracy: 0.2722\n","\n","Epoch 00022: val_categorical_accuracy improved from 0.26664 to 0.27218, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 23/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9495 - categorical_accuracy: 0.4526 - val_loss: 2.1522 - val_categorical_accuracy: 0.2874\n","\n","Epoch 00023: val_categorical_accuracy improved from 0.27218 to 0.28743, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 24/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9439 - categorical_accuracy: 0.5111 - val_loss: 2.1424 - val_categorical_accuracy: 0.2934\n","\n","Epoch 00024: val_categorical_accuracy improved from 0.28743 to 0.29344, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 25/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9726 - categorical_accuracy: 0.5097 - val_loss: 2.1310 - val_categorical_accuracy: 0.2879\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.29344\n","Epoch 26/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9069 - categorical_accuracy: 0.4944 - val_loss: 2.1228 - val_categorical_accuracy: 0.2985\n","\n","Epoch 00026: val_categorical_accuracy improved from 0.29344 to 0.29852, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 27/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8892 - categorical_accuracy: 0.5509 - val_loss: 2.1094 - val_categorical_accuracy: 0.3050\n","\n","Epoch 00027: val_categorical_accuracy improved from 0.29852 to 0.30499, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 28/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.8849 - categorical_accuracy: 0.5390 - val_loss: 2.1016 - val_categorical_accuracy: 0.3299\n","\n","Epoch 00028: val_categorical_accuracy improved from 0.30499 to 0.32994, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 29/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8514 - categorical_accuracy: 0.6105 - val_loss: 2.0914 - val_categorical_accuracy: 0.3378\n","\n","Epoch 00029: val_categorical_accuracy improved from 0.32994 to 0.33780, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 30/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8461 - categorical_accuracy: 0.5541 - val_loss: 2.0826 - val_categorical_accuracy: 0.3313\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.33780\n","Epoch 31/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8595 - categorical_accuracy: 0.6104 - val_loss: 2.0748 - val_categorical_accuracy: 0.3540\n","\n","Epoch 00031: val_categorical_accuracy improved from 0.33780 to 0.35397, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 32/100\n","49/49 [==============================] - 0s 8ms/step - loss: 1.8504 - categorical_accuracy: 0.6286 - val_loss: 2.0676 - val_categorical_accuracy: 0.3489\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.35397\n","Epoch 33/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8349 - categorical_accuracy: 0.6056 - val_loss: 2.0574 - val_categorical_accuracy: 0.3669\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.35397 to 0.36691, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 34/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8376 - categorical_accuracy: 0.6161 - val_loss: 2.0465 - val_categorical_accuracy: 0.3669\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.36691\n","Epoch 35/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8297 - categorical_accuracy: 0.6313 - val_loss: 2.0435 - val_categorical_accuracy: 0.3577\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.36691\n","Epoch 36/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7993 - categorical_accuracy: 0.6257 - val_loss: 2.0327 - val_categorical_accuracy: 0.3835\n","\n","Epoch 00036: val_categorical_accuracy improved from 0.36691 to 0.38355, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 37/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7810 - categorical_accuracy: 0.6822 - val_loss: 2.0239 - val_categorical_accuracy: 0.3905\n","\n","Epoch 00037: val_categorical_accuracy improved from 0.38355 to 0.39048, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 38/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7714 - categorical_accuracy: 0.6201 - val_loss: 2.0178 - val_categorical_accuracy: 0.3812\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.39048\n","Epoch 39/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7818 - categorical_accuracy: 0.6582 - val_loss: 2.0115 - val_categorical_accuracy: 0.3956\n","\n","Epoch 00039: val_categorical_accuracy improved from 0.39048 to 0.39556, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 40/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.7644 - categorical_accuracy: 0.6677 - val_loss: 2.0049 - val_categorical_accuracy: 0.3919\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.39556\n","Epoch 41/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7805 - categorical_accuracy: 0.6833 - val_loss: 1.9983 - val_categorical_accuracy: 0.3933\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.39556\n","Epoch 42/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7360 - categorical_accuracy: 0.6942 - val_loss: 1.9902 - val_categorical_accuracy: 0.4071\n","\n","Epoch 00042: val_categorical_accuracy improved from 0.39556 to 0.40712, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 43/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7054 - categorical_accuracy: 0.6943 - val_loss: 1.9818 - val_categorical_accuracy: 0.4173\n","\n","Epoch 00043: val_categorical_accuracy improved from 0.40712 to 0.41728, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 44/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7245 - categorical_accuracy: 0.6898 - val_loss: 1.9752 - val_categorical_accuracy: 0.4219\n","\n","Epoch 00044: val_categorical_accuracy improved from 0.41728 to 0.42190, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 45/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7115 - categorical_accuracy: 0.7091 - val_loss: 1.9690 - val_categorical_accuracy: 0.4251\n","\n","Epoch 00045: val_categorical_accuracy improved from 0.42190 to 0.42514, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 46/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6878 - categorical_accuracy: 0.7133 - val_loss: 1.9628 - val_categorical_accuracy: 0.4233\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.42514\n","Epoch 47/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7032 - categorical_accuracy: 0.7118 - val_loss: 1.9551 - val_categorical_accuracy: 0.4348\n","\n","Epoch 00047: val_categorical_accuracy improved from 0.42514 to 0.43484, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 48/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6560 - categorical_accuracy: 0.7508 - val_loss: 1.9503 - val_categorical_accuracy: 0.4362\n","\n","Epoch 00048: val_categorical_accuracy improved from 0.43484 to 0.43623, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 49/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6586 - categorical_accuracy: 0.7513 - val_loss: 1.9429 - val_categorical_accuracy: 0.4552\n","\n","Epoch 00049: val_categorical_accuracy improved from 0.43623 to 0.45518, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 50/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6629 - categorical_accuracy: 0.7118 - val_loss: 1.9359 - val_categorical_accuracy: 0.4501\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.45518\n","Epoch 51/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6632 - categorical_accuracy: 0.7591 - val_loss: 1.9306 - val_categorical_accuracy: 0.4501\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.45518\n","Epoch 52/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6339 - categorical_accuracy: 0.7469 - val_loss: 1.9257 - val_categorical_accuracy: 0.4806\n","\n","Epoch 00052: val_categorical_accuracy improved from 0.45518 to 0.48059, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 53/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6247 - categorical_accuracy: 0.7720 - val_loss: 1.9192 - val_categorical_accuracy: 0.4774\n","\n","Epoch 00053: val_categorical_accuracy did not improve from 0.48059\n","Epoch 54/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6191 - categorical_accuracy: 0.7496 - val_loss: 1.9122 - val_categorical_accuracy: 0.4774\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.48059\n","Epoch 55/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6110 - categorical_accuracy: 0.7701 - val_loss: 1.9052 - val_categorical_accuracy: 0.4612\n","\n","Epoch 00055: val_categorical_accuracy did not improve from 0.48059\n","Epoch 56/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5800 - categorical_accuracy: 0.7981 - val_loss: 1.9007 - val_categorical_accuracy: 0.4695\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.48059\n","Epoch 57/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6337 - categorical_accuracy: 0.7839 - val_loss: 1.8927 - val_categorical_accuracy: 0.4667\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.48059\n","Epoch 58/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5985 - categorical_accuracy: 0.7876 - val_loss: 1.8900 - val_categorical_accuracy: 0.4769\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.48059\n","Epoch 59/100\n","49/49 [==============================] - 0s 6ms/step - loss: 1.5701 - categorical_accuracy: 0.7996 - val_loss: 1.8841 - val_categorical_accuracy: 0.4898\n","\n","Epoch 00059: val_categorical_accuracy improved from 0.48059 to 0.48983, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 60/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5436 - categorical_accuracy: 0.8123 - val_loss: 1.8769 - val_categorical_accuracy: 0.4898\n","\n","Epoch 00060: val_categorical_accuracy did not improve from 0.48983\n","Epoch 61/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5701 - categorical_accuracy: 0.7991 - val_loss: 1.8715 - val_categorical_accuracy: 0.4746\n","\n","Epoch 00061: val_categorical_accuracy did not improve from 0.48983\n","Epoch 62/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5392 - categorical_accuracy: 0.8067 - val_loss: 1.8678 - val_categorical_accuracy: 0.4986\n","\n","Epoch 00062: val_categorical_accuracy improved from 0.48983 to 0.49861, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 63/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.5324 - categorical_accuracy: 0.8087 - val_loss: 1.8620 - val_categorical_accuracy: 0.4861\n","\n","Epoch 00063: val_categorical_accuracy did not improve from 0.49861\n","Epoch 64/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5224 - categorical_accuracy: 0.8174 - val_loss: 1.8588 - val_categorical_accuracy: 0.4903\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.49861\n","Epoch 65/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5496 - categorical_accuracy: 0.7895 - val_loss: 1.8500 - val_categorical_accuracy: 0.4815\n","\n","Epoch 00065: val_categorical_accuracy did not improve from 0.49861\n","Epoch 66/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5121 - categorical_accuracy: 0.8258 - val_loss: 1.8482 - val_categorical_accuracy: 0.5079\n","\n","Epoch 00066: val_categorical_accuracy improved from 0.49861 to 0.50786, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 67/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5283 - categorical_accuracy: 0.8252 - val_loss: 1.8409 - val_categorical_accuracy: 0.5083\n","\n","Epoch 00067: val_categorical_accuracy improved from 0.50786 to 0.50832, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 68/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5340 - categorical_accuracy: 0.8212 - val_loss: 1.8365 - val_categorical_accuracy: 0.4848\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.50832\n","Epoch 69/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4724 - categorical_accuracy: 0.8470 - val_loss: 1.8322 - val_categorical_accuracy: 0.4958\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.50832\n","Epoch 70/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4893 - categorical_accuracy: 0.8243 - val_loss: 1.8248 - val_categorical_accuracy: 0.5088\n","\n","Epoch 00070: val_categorical_accuracy improved from 0.50832 to 0.50878, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 71/100\n","49/49 [==============================] - 0s 6ms/step - loss: 1.4739 - categorical_accuracy: 0.8274 - val_loss: 1.8231 - val_categorical_accuracy: 0.5116\n","\n","Epoch 00071: val_categorical_accuracy improved from 0.50878 to 0.51155, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 72/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.4725 - categorical_accuracy: 0.8456 - val_loss: 1.8189 - val_categorical_accuracy: 0.5300\n","\n","Epoch 00072: val_categorical_accuracy improved from 0.51155 to 0.53004, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 73/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4529 - categorical_accuracy: 0.8569 - val_loss: 1.8114 - val_categorical_accuracy: 0.4926\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.53004\n","Epoch 74/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4681 - categorical_accuracy: 0.8591 - val_loss: 1.8072 - val_categorical_accuracy: 0.5203\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.53004\n","Epoch 75/100\n","49/49 [==============================] - 0s 7ms/step - loss: 1.4494 - categorical_accuracy: 0.8591 - val_loss: 1.8042 - val_categorical_accuracy: 0.5236\n","\n","Epoch 00075: val_categorical_accuracy did not improve from 0.53004\n","Epoch 76/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4368 - categorical_accuracy: 0.8514 - val_loss: 1.7985 - val_categorical_accuracy: 0.5162\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.53004\n","Epoch 77/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4700 - categorical_accuracy: 0.8500 - val_loss: 1.7968 - val_categorical_accuracy: 0.5263\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.53004\n","Epoch 78/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4109 - categorical_accuracy: 0.8820 - val_loss: 1.7915 - val_categorical_accuracy: 0.5291\n","\n","Epoch 00078: val_categorical_accuracy did not improve from 0.53004\n","Epoch 79/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4098 - categorical_accuracy: 0.8758 - val_loss: 1.7845 - val_categorical_accuracy: 0.5263\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.53004\n","Epoch 80/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3980 - categorical_accuracy: 0.8559 - val_loss: 1.7808 - val_categorical_accuracy: 0.5379\n","\n","Epoch 00080: val_categorical_accuracy improved from 0.53004 to 0.53789, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 81/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.4073 - categorical_accuracy: 0.8741 - val_loss: 1.7753 - val_categorical_accuracy: 0.5513\n","\n","Epoch 00081: val_categorical_accuracy improved from 0.53789 to 0.55129, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 82/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3935 - categorical_accuracy: 0.8796 - val_loss: 1.7724 - val_categorical_accuracy: 0.5176\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.55129\n","Epoch 83/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4132 - categorical_accuracy: 0.8794 - val_loss: 1.7688 - val_categorical_accuracy: 0.5176\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.55129\n","Epoch 84/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4144 - categorical_accuracy: 0.8819 - val_loss: 1.7611 - val_categorical_accuracy: 0.5444\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.55129\n","Epoch 85/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3976 - categorical_accuracy: 0.9035 - val_loss: 1.7590 - val_categorical_accuracy: 0.5296\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.55129\n","Epoch 86/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4020 - categorical_accuracy: 0.8785 - val_loss: 1.7563 - val_categorical_accuracy: 0.5476\n","\n","Epoch 00086: val_categorical_accuracy did not improve from 0.55129\n","Epoch 87/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3711 - categorical_accuracy: 0.8846 - val_loss: 1.7503 - val_categorical_accuracy: 0.5434\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.55129\n","Epoch 88/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3729 - categorical_accuracy: 0.8988 - val_loss: 1.7462 - val_categorical_accuracy: 0.5448\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.55129\n","Epoch 89/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3557 - categorical_accuracy: 0.8915 - val_loss: 1.7429 - val_categorical_accuracy: 0.5411\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.55129\n","Epoch 90/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3851 - categorical_accuracy: 0.8867 - val_loss: 1.7369 - val_categorical_accuracy: 0.5384\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.55129\n","Epoch 91/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3654 - categorical_accuracy: 0.9009 - val_loss: 1.7320 - val_categorical_accuracy: 0.5337\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.55129\n","Epoch 92/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3653 - categorical_accuracy: 0.9008 - val_loss: 1.7299 - val_categorical_accuracy: 0.5457\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.55129\n","Epoch 93/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3330 - categorical_accuracy: 0.9098 - val_loss: 1.7273 - val_categorical_accuracy: 0.5323\n","\n","Epoch 00093: val_categorical_accuracy did not improve from 0.55129\n","Epoch 94/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3434 - categorical_accuracy: 0.8912 - val_loss: 1.7224 - val_categorical_accuracy: 0.5462\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.55129\n","Epoch 95/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3205 - categorical_accuracy: 0.9240 - val_loss: 1.7198 - val_categorical_accuracy: 0.5300\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.55129\n","Epoch 96/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3076 - categorical_accuracy: 0.9188 - val_loss: 1.7160 - val_categorical_accuracy: 0.5476\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.55129\n","Epoch 97/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3238 - categorical_accuracy: 0.9108 - val_loss: 1.7109 - val_categorical_accuracy: 0.5605\n","\n","Epoch 00097: val_categorical_accuracy improved from 0.55129 to 0.56054, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_200_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 98/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3005 - categorical_accuracy: 0.9189 - val_loss: 1.7089 - val_categorical_accuracy: 0.5564\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.56054\n","Epoch 99/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2744 - categorical_accuracy: 0.9303 - val_loss: 1.7020 - val_categorical_accuracy: 0.5591\n","\n","Epoch 00099: val_categorical_accuracy did not improve from 0.56054\n","Epoch 100/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2888 - categorical_accuracy: 0.9301 - val_loss: 1.6996 - val_categorical_accuracy: 0.5481\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.56054\n","68/68 [==============================] - 0s 2ms/step - loss: 1.7109 - categorical_accuracy: 0.5605\n","68/68 [==============================] - 0s 1ms/step\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 5, 5, 64)     256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 5, 5, 64)     0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 5, 5, 8)      584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 5, 5, 8)      584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 5, 5, 8)      584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 5, 5, 8)      584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 5, 5, 8)      584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 5, 5, 8)      584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 5, 5, 8)      584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 5, 5, 8)      0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 5, 5, 8)      584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 5, 5, 8)      584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 5, 5, 8)      584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 5, 5, 8)      584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 5, 5, 8)      584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 5, 5, 8)      584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 5, 5, 8)      584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 5, 5, 8)      584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 5, 5, 8)      584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 5, 5, 64)     0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 5, 5, 64)     256         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 5, 5, 64)     0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 5, 5, 128)    8320        activation_6[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 5, 5, 128)    512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 5, 5, 128)    0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 5, 5, 8, 16)  0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 5, 5, 16, 8)  0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 5, 5, 128)    512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 5, 5, 128)    0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 5, 5, 128)    0           tf.reshape_3[0][0]               \n","                                                                 activation_8[0][0]               \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 5, 5, 128)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_9[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 980.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [70 70 70 70 70 70 70 70 70 70 70 70 70 70]\n","\n","Total number of samples in test set 2164.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [200  31 181 145 199 199 189 133 244 178 206  63 171  25]\n","\n","X_train_transfer => (980, 128)\n","X_test_transfer  => (2164, 128)\n","y_train => (980, 14)\n","y_test  => (2164, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_14 (InputLayer)        [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","49/49 [==============================] - 1s 7ms/step - loss: 3.5361 - categorical_accuracy: 0.1070 - val_loss: 2.4736 - val_categorical_accuracy: 0.0601\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.06007, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.0956 - categorical_accuracy: 0.2801 - val_loss: 2.2344 - val_categorical_accuracy: 0.2528\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.06007 to 0.25277, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8606 - categorical_accuracy: 0.5144 - val_loss: 2.0610 - val_categorical_accuracy: 0.3595\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.25277 to 0.35952, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.6510 - categorical_accuracy: 0.6438 - val_loss: 1.9830 - val_categorical_accuracy: 0.3309\n","\n","Epoch 00004: val_categorical_accuracy did not improve from 0.35952\n","Epoch 5/100\n","49/49 [==============================] - 0s 6ms/step - loss: 1.5834 - categorical_accuracy: 0.6378 - val_loss: 1.8588 - val_categorical_accuracy: 0.4884\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.35952 to 0.48845, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 6/100\n","49/49 [==============================] - 0s 8ms/step - loss: 1.4377 - categorical_accuracy: 0.7095 - val_loss: 1.7533 - val_categorical_accuracy: 0.5647\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.48845 to 0.56470, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3811 - categorical_accuracy: 0.7684 - val_loss: 1.7076 - val_categorical_accuracy: 0.4677\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.56470\n","Epoch 8/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3157 - categorical_accuracy: 0.7683 - val_loss: 1.6311 - val_categorical_accuracy: 0.5342\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.56470\n","Epoch 9/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2593 - categorical_accuracy: 0.7879 - val_loss: 1.6168 - val_categorical_accuracy: 0.4940\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.56470\n","Epoch 10/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1929 - categorical_accuracy: 0.8572 - val_loss: 1.5668 - val_categorical_accuracy: 0.5984\n","\n","Epoch 00010: val_categorical_accuracy improved from 0.56470 to 0.59843, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 11/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1406 - categorical_accuracy: 0.9146 - val_loss: 1.5221 - val_categorical_accuracy: 0.6160\n","\n","Epoch 00011: val_categorical_accuracy improved from 0.59843 to 0.61599, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 12/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0844 - categorical_accuracy: 0.8995 - val_loss: 1.4846 - val_categorical_accuracy: 0.5855\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.61599\n","Epoch 13/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0413 - categorical_accuracy: 0.9158 - val_loss: 1.4750 - val_categorical_accuracy: 0.5818\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.61599\n","Epoch 14/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0240 - categorical_accuracy: 0.9414 - val_loss: 1.4694 - val_categorical_accuracy: 0.6252\n","\n","Epoch 00014: val_categorical_accuracy improved from 0.61599 to 0.62523, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 15/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9949 - categorical_accuracy: 0.9639 - val_loss: 1.4362 - val_categorical_accuracy: 0.5924\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.62523\n","Epoch 16/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9715 - categorical_accuracy: 0.9572 - val_loss: 1.3768 - val_categorical_accuracy: 0.6419\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.62523 to 0.64187, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 17/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9261 - categorical_accuracy: 0.9432 - val_loss: 1.3677 - val_categorical_accuracy: 0.6160\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.64187\n","Epoch 18/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8975 - categorical_accuracy: 0.9587 - val_loss: 1.3522 - val_categorical_accuracy: 0.6664\n","\n","Epoch 00018: val_categorical_accuracy improved from 0.64187 to 0.66636, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 19/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8891 - categorical_accuracy: 0.9604 - val_loss: 1.3274 - val_categorical_accuracy: 0.5966\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.66636\n","Epoch 20/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8661 - categorical_accuracy: 0.9585 - val_loss: 1.3452 - val_categorical_accuracy: 0.6368\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.66636\n","Epoch 21/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8144 - categorical_accuracy: 0.9593 - val_loss: 1.3057 - val_categorical_accuracy: 0.6719\n","\n","Epoch 00021: val_categorical_accuracy improved from 0.66636 to 0.67190, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 22/100\n","49/49 [==============================] - 0s 4ms/step - loss: 0.8025 - categorical_accuracy: 0.9847 - val_loss: 1.2834 - val_categorical_accuracy: 0.6530\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.67190\n","Epoch 23/100\n","49/49 [==============================] - 0s 6ms/step - loss: 0.7985 - categorical_accuracy: 0.9769 - val_loss: 1.2836 - val_categorical_accuracy: 0.6788\n","\n","Epoch 00023: val_categorical_accuracy improved from 0.67190 to 0.67884, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 24/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7454 - categorical_accuracy: 0.9851 - val_loss: 1.2575 - val_categorical_accuracy: 0.7163\n","\n","Epoch 00024: val_categorical_accuracy improved from 0.67884 to 0.71627, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 25/100\n","49/49 [==============================] - 0s 4ms/step - loss: 0.7399 - categorical_accuracy: 0.9851 - val_loss: 1.2548 - val_categorical_accuracy: 0.6738\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.71627\n","Epoch 26/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7338 - categorical_accuracy: 0.9724 - val_loss: 1.2255 - val_categorical_accuracy: 0.6821\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.71627\n","Epoch 27/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7036 - categorical_accuracy: 0.9677 - val_loss: 1.2208 - val_categorical_accuracy: 0.6996\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.71627\n","Epoch 28/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.6825 - categorical_accuracy: 0.9908 - val_loss: 1.1952 - val_categorical_accuracy: 0.7029\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.71627\n","Epoch 29/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.6614 - categorical_accuracy: 0.9941 - val_loss: 1.2133 - val_categorical_accuracy: 0.6821\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.71627\n","Epoch 30/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.6527 - categorical_accuracy: 0.9810 - val_loss: 1.1699 - val_categorical_accuracy: 0.7361\n","\n","Epoch 00030: val_categorical_accuracy improved from 0.71627 to 0.73614, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 31/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.6207 - categorical_accuracy: 0.9856 - val_loss: 1.1831 - val_categorical_accuracy: 0.7329\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.73614\n","Epoch 32/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.6447 - categorical_accuracy: 0.9907 - val_loss: 1.1690 - val_categorical_accuracy: 0.7015\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.73614\n","Epoch 33/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.6134 - categorical_accuracy: 0.9894 - val_loss: 1.1666 - val_categorical_accuracy: 0.7491\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.73614 to 0.74908, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 34/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.6030 - categorical_accuracy: 0.9906 - val_loss: 1.1789 - val_categorical_accuracy: 0.6733\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.74908\n","Epoch 35/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5812 - categorical_accuracy: 0.9899 - val_loss: 1.1352 - val_categorical_accuracy: 0.7278\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.74908\n","Epoch 36/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5684 - categorical_accuracy: 0.9925 - val_loss: 1.1338 - val_categorical_accuracy: 0.7098\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.74908\n","Epoch 37/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5698 - categorical_accuracy: 0.9942 - val_loss: 1.1377 - val_categorical_accuracy: 0.7043\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.74908\n","Epoch 38/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5394 - categorical_accuracy: 0.9955 - val_loss: 1.0959 - val_categorical_accuracy: 0.7190\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.74908\n","Epoch 39/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5468 - categorical_accuracy: 0.9912 - val_loss: 1.1249 - val_categorical_accuracy: 0.6941\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.74908\n","Epoch 40/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5053 - categorical_accuracy: 0.9951 - val_loss: 1.1183 - val_categorical_accuracy: 0.6904\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.74908\n","Epoch 41/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5123 - categorical_accuracy: 0.9925 - val_loss: 1.0922 - val_categorical_accuracy: 0.7121\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.74908\n","Epoch 42/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.5030 - categorical_accuracy: 0.9929 - val_loss: 1.0927 - val_categorical_accuracy: 0.7029\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.74908\n","Epoch 43/100\n","49/49 [==============================] - 0s 6ms/step - loss: 0.4892 - categorical_accuracy: 0.9959 - val_loss: 1.1124 - val_categorical_accuracy: 0.7255\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.74908\n","Epoch 44/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4891 - categorical_accuracy: 0.9901 - val_loss: 1.1012 - val_categorical_accuracy: 0.7107\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.74908\n","Epoch 45/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4673 - categorical_accuracy: 0.9963 - val_loss: 1.0804 - val_categorical_accuracy: 0.7112\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.74908\n","Epoch 46/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4684 - categorical_accuracy: 0.9945 - val_loss: 1.0489 - val_categorical_accuracy: 0.7495\n","\n","Epoch 00046: val_categorical_accuracy improved from 0.74908 to 0.74954, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_250_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 47/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4597 - categorical_accuracy: 0.9921 - val_loss: 1.0567 - val_categorical_accuracy: 0.7334\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.74954\n","Epoch 48/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4544 - categorical_accuracy: 0.9960 - val_loss: 1.0490 - val_categorical_accuracy: 0.7098\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.74954\n","Epoch 49/100\n","49/49 [==============================] - 0s 7ms/step - loss: 0.4322 - categorical_accuracy: 0.9948 - val_loss: 1.0396 - val_categorical_accuracy: 0.7116\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.74954\n","Epoch 50/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4307 - categorical_accuracy: 0.9961 - val_loss: 1.0540 - val_categorical_accuracy: 0.7112\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.74954\n","Epoch 51/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4105 - categorical_accuracy: 0.9916 - val_loss: 1.0391 - val_categorical_accuracy: 0.7195\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.74954\n","Epoch 52/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.4114 - categorical_accuracy: 0.9915 - val_loss: 1.0356 - val_categorical_accuracy: 0.7163\n","\n","Epoch 00052: val_categorical_accuracy did not improve from 0.74954\n","Epoch 53/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3964 - categorical_accuracy: 0.9954 - val_loss: 1.0278 - val_categorical_accuracy: 0.7079\n","\n","Epoch 00053: val_categorical_accuracy did not improve from 0.74954\n","Epoch 54/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3810 - categorical_accuracy: 0.9923 - val_loss: 1.0371 - val_categorical_accuracy: 0.7144\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.74954\n","Epoch 55/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3895 - categorical_accuracy: 0.9956 - val_loss: 1.0289 - val_categorical_accuracy: 0.7190\n","\n","Epoch 00055: val_categorical_accuracy did not improve from 0.74954\n","Epoch 56/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3869 - categorical_accuracy: 0.9940 - val_loss: 1.0143 - val_categorical_accuracy: 0.7408\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.74954\n","Epoch 57/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3870 - categorical_accuracy: 0.9958 - val_loss: 1.0406 - val_categorical_accuracy: 0.7264\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.74954\n","Epoch 58/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3581 - categorical_accuracy: 0.9944 - val_loss: 1.0323 - val_categorical_accuracy: 0.7241\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.74954\n","Epoch 59/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3668 - categorical_accuracy: 0.9958 - val_loss: 1.0121 - val_categorical_accuracy: 0.7223\n","\n","Epoch 00059: val_categorical_accuracy did not improve from 0.74954\n","Epoch 60/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3491 - categorical_accuracy: 0.9952 - val_loss: 1.0085 - val_categorical_accuracy: 0.7274\n","\n","Epoch 00060: val_categorical_accuracy did not improve from 0.74954\n","Epoch 61/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3506 - categorical_accuracy: 0.9937 - val_loss: 1.0028 - val_categorical_accuracy: 0.7186\n","\n","Epoch 00061: val_categorical_accuracy did not improve from 0.74954\n","Epoch 62/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3363 - categorical_accuracy: 0.9953 - val_loss: 1.0077 - val_categorical_accuracy: 0.7209\n","\n","Epoch 00062: val_categorical_accuracy did not improve from 0.74954\n","Epoch 63/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3383 - categorical_accuracy: 0.9968 - val_loss: 1.0099 - val_categorical_accuracy: 0.7213\n","\n","Epoch 00063: val_categorical_accuracy did not improve from 0.74954\n","Epoch 64/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3251 - categorical_accuracy: 0.9954 - val_loss: 1.0024 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.74954\n","Epoch 65/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3263 - categorical_accuracy: 0.9948 - val_loss: 1.0105 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00065: val_categorical_accuracy did not improve from 0.74954\n","Epoch 66/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3299 - categorical_accuracy: 0.9908 - val_loss: 1.0015 - val_categorical_accuracy: 0.7190\n","\n","Epoch 00066: val_categorical_accuracy did not improve from 0.74954\n","Epoch 67/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3188 - categorical_accuracy: 0.9965 - val_loss: 0.9740 - val_categorical_accuracy: 0.7357\n","\n","Epoch 00067: val_categorical_accuracy did not improve from 0.74954\n","Epoch 68/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3195 - categorical_accuracy: 0.9890 - val_loss: 1.0055 - val_categorical_accuracy: 0.7246\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.74954\n","Epoch 69/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3043 - categorical_accuracy: 0.9990 - val_loss: 0.9730 - val_categorical_accuracy: 0.7260\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.74954\n","Epoch 70/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.3067 - categorical_accuracy: 0.9950 - val_loss: 0.9956 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00070: val_categorical_accuracy did not improve from 0.74954\n","Epoch 71/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2991 - categorical_accuracy: 0.9996 - val_loss: 0.9744 - val_categorical_accuracy: 0.7371\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.74954\n","Epoch 72/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2899 - categorical_accuracy: 0.9963 - val_loss: 0.9982 - val_categorical_accuracy: 0.7209\n","\n","Epoch 00072: val_categorical_accuracy did not improve from 0.74954\n","Epoch 73/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2873 - categorical_accuracy: 0.9941 - val_loss: 0.9946 - val_categorical_accuracy: 0.7227\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.74954\n","Epoch 74/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2905 - categorical_accuracy: 0.9964 - val_loss: 0.9766 - val_categorical_accuracy: 0.7283\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.74954\n","Epoch 75/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2706 - categorical_accuracy: 0.9930 - val_loss: 0.9845 - val_categorical_accuracy: 0.7181\n","\n","Epoch 00075: val_categorical_accuracy did not improve from 0.74954\n","Epoch 76/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2751 - categorical_accuracy: 0.9984 - val_loss: 0.9688 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.74954\n","Epoch 77/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2579 - categorical_accuracy: 0.9965 - val_loss: 0.9927 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.74954\n","Epoch 78/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2757 - categorical_accuracy: 0.9932 - val_loss: 0.9833 - val_categorical_accuracy: 0.7223\n","\n","Epoch 00078: val_categorical_accuracy did not improve from 0.74954\n","Epoch 79/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2632 - categorical_accuracy: 0.9917 - val_loss: 0.9962 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.74954\n","Epoch 80/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2666 - categorical_accuracy: 0.9979 - val_loss: 0.9859 - val_categorical_accuracy: 0.7200\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.74954\n","Epoch 81/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2570 - categorical_accuracy: 0.9980 - val_loss: 0.9863 - val_categorical_accuracy: 0.7181\n","\n","Epoch 00081: val_categorical_accuracy did not improve from 0.74954\n","Epoch 82/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2596 - categorical_accuracy: 0.9953 - val_loss: 0.9620 - val_categorical_accuracy: 0.7398\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.74954\n","Epoch 83/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2517 - categorical_accuracy: 0.9958 - val_loss: 0.9659 - val_categorical_accuracy: 0.7227\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.74954\n","Epoch 84/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2495 - categorical_accuracy: 0.9936 - val_loss: 0.9740 - val_categorical_accuracy: 0.7195\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.74954\n","Epoch 85/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2499 - categorical_accuracy: 0.9970 - val_loss: 0.9697 - val_categorical_accuracy: 0.7227\n","\n","Epoch 00085: val_categorical_accuracy did not improve from 0.74954\n","Epoch 86/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2473 - categorical_accuracy: 0.9970 - val_loss: 0.9644 - val_categorical_accuracy: 0.7246\n","\n","Epoch 00086: val_categorical_accuracy did not improve from 0.74954\n","Epoch 87/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2409 - categorical_accuracy: 0.9951 - val_loss: 0.9687 - val_categorical_accuracy: 0.7227\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.74954\n","Epoch 88/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2355 - categorical_accuracy: 0.9980 - val_loss: 0.9659 - val_categorical_accuracy: 0.7181\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.74954\n","Epoch 89/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2350 - categorical_accuracy: 0.9928 - val_loss: 0.9713 - val_categorical_accuracy: 0.7250\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.74954\n","Epoch 90/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2350 - categorical_accuracy: 0.9983 - val_loss: 0.9555 - val_categorical_accuracy: 0.7246\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.74954\n","Epoch 91/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2227 - categorical_accuracy: 0.9984 - val_loss: 0.9551 - val_categorical_accuracy: 0.7232\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.74954\n","Epoch 92/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2186 - categorical_accuracy: 0.9956 - val_loss: 0.9643 - val_categorical_accuracy: 0.7260\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.74954\n","Epoch 93/100\n","49/49 [==============================] - 0s 7ms/step - loss: 0.2147 - categorical_accuracy: 0.9979 - val_loss: 0.9440 - val_categorical_accuracy: 0.7227\n","\n","Epoch 00093: val_categorical_accuracy did not improve from 0.74954\n","Epoch 94/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2222 - categorical_accuracy: 0.9923 - val_loss: 0.9587 - val_categorical_accuracy: 0.7204\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.74954\n","Epoch 95/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2289 - categorical_accuracy: 0.9951 - val_loss: 0.9318 - val_categorical_accuracy: 0.7264\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.74954\n","Epoch 96/100\n","49/49 [==============================] - 0s 6ms/step - loss: 0.2195 - categorical_accuracy: 0.9973 - val_loss: 0.9547 - val_categorical_accuracy: 0.7274\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.74954\n","Epoch 97/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2091 - categorical_accuracy: 0.9979 - val_loss: 0.9444 - val_categorical_accuracy: 0.7223\n","\n","Epoch 00097: val_categorical_accuracy did not improve from 0.74954\n","Epoch 98/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2079 - categorical_accuracy: 0.9995 - val_loss: 0.9466 - val_categorical_accuracy: 0.7334\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.74954\n","Epoch 99/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2043 - categorical_accuracy: 0.9981 - val_loss: 0.9467 - val_categorical_accuracy: 0.7274\n","\n","Epoch 00099: val_categorical_accuracy did not improve from 0.74954\n","Epoch 100/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.2131 - categorical_accuracy: 0.9984 - val_loss: 0.9572 - val_categorical_accuracy: 0.7237\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.74954\n","68/68 [==============================] - 0s 2ms/step - loss: 1.0489 - categorical_accuracy: 0.7495\n","68/68 [==============================] - 0s 1ms/step\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 5, 5, 64)     256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 5, 5, 64)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 5, 5, 8)      584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 5, 5, 8)      584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 5, 5, 8)      584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 5, 5, 8)      584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 5, 5, 8)      584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 5, 5, 8)      584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 5, 5, 8)      584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 5, 5, 8)      0           activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 5, 5, 8)      584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 5, 5, 8)      584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 5, 5, 8)      584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 5, 5, 8)      584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 5, 5, 8)      584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 5, 5, 8)      584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 5, 5, 8)      584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 5, 5, 8)      584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 5, 5, 8)      584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 5, 5, 64)     0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 5, 5, 64)     256         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 5, 5, 64)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 5, 5, 128)    8320        activation_11[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 5, 5, 128)    0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 5, 5, 8, 16)  0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 5, 5, 16, 8)  0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 5, 5, 16, 8)  0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 5, 5, 128)    512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 5, 5, 128)    0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 5, 5, 128)    0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 5, 5, 128)    0           tf.reshape_5[0][0]               \n","                                                                 activation_13[0][0]              \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 5, 5, 128)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_14[0][0]              \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [270, 101, 251, 215, 269, 269, 259, 203, 314, 248, 276, 133, 241, 95]\n","Total number of samples 3144.\n","\n","Total number of samples in training set 980.\n","unique classes in training set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in training set: [70 70 70 70 70 70 70 70 70 70 70 70 70 70]\n","\n","Total number of samples in test set 2164.\n","unique classes in test set: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n","Samples per class in test set: [200  31 181 145 199 199 189 133 244 178 206  63 171  25]\n","\n","X_train_transfer => (980, 128)\n","X_test_transfer  => (2164, 128)\n","y_train => (980, 14)\n","y_test  => (2164, 14)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_15 (InputLayer)        [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc14 (Dense)                 (None, 14)                3598      \n","=================================================================\n","Total params: 36,622\n","Trainable params: 36,622\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","49/49 [==============================] - 1s 8ms/step - loss: 3.1202 - categorical_accuracy: 0.0669 - val_loss: 2.5206 - val_categorical_accuracy: 0.1955\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.19547, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 2/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.3642 - categorical_accuracy: 0.2174 - val_loss: 2.3831 - val_categorical_accuracy: 0.2897\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.19547 to 0.28974, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 3/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.2317 - categorical_accuracy: 0.4514 - val_loss: 2.2902 - val_categorical_accuracy: 0.4062\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.28974 to 0.40619, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 4/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.1413 - categorical_accuracy: 0.5197 - val_loss: 2.2406 - val_categorical_accuracy: 0.4076\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.40619 to 0.40758, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 5/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.0784 - categorical_accuracy: 0.5081 - val_loss: 2.1795 - val_categorical_accuracy: 0.4723\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.40758 to 0.47227, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 6/100\n","49/49 [==============================] - 0s 5ms/step - loss: 2.0392 - categorical_accuracy: 0.5910 - val_loss: 2.1475 - val_categorical_accuracy: 0.4774\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.47227 to 0.47736, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 7/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9812 - categorical_accuracy: 0.5712 - val_loss: 2.1145 - val_categorical_accuracy: 0.4293\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.47736\n","Epoch 8/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.9088 - categorical_accuracy: 0.5736 - val_loss: 2.0696 - val_categorical_accuracy: 0.5032\n","\n","Epoch 00008: val_categorical_accuracy improved from 0.47736 to 0.50323, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 9/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.8919 - categorical_accuracy: 0.6356 - val_loss: 2.0457 - val_categorical_accuracy: 0.4686\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.50323\n","Epoch 10/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7829 - categorical_accuracy: 0.6394 - val_loss: 2.0180 - val_categorical_accuracy: 0.4653\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.50323\n","Epoch 11/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7975 - categorical_accuracy: 0.6504 - val_loss: 1.9882 - val_categorical_accuracy: 0.4695\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.50323\n","Epoch 12/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7668 - categorical_accuracy: 0.6652 - val_loss: 1.9628 - val_categorical_accuracy: 0.4242\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.50323\n","Epoch 13/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.7575 - categorical_accuracy: 0.6383 - val_loss: 1.9420 - val_categorical_accuracy: 0.4644\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.50323\n","Epoch 14/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6793 - categorical_accuracy: 0.7014 - val_loss: 1.9248 - val_categorical_accuracy: 0.5000\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.50323\n","Epoch 15/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6132 - categorical_accuracy: 0.6986 - val_loss: 1.8942 - val_categorical_accuracy: 0.4954\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.50323\n","Epoch 16/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.6090 - categorical_accuracy: 0.7451 - val_loss: 1.8768 - val_categorical_accuracy: 0.4718\n","\n","Epoch 00016: val_categorical_accuracy did not improve from 0.50323\n","Epoch 17/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5930 - categorical_accuracy: 0.6758 - val_loss: 1.8654 - val_categorical_accuracy: 0.4566\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.50323\n","Epoch 18/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5681 - categorical_accuracy: 0.7482 - val_loss: 1.8342 - val_categorical_accuracy: 0.5037\n","\n","Epoch 00018: val_categorical_accuracy improved from 0.50323 to 0.50370, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 19/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5619 - categorical_accuracy: 0.6982 - val_loss: 1.8160 - val_categorical_accuracy: 0.5116\n","\n","Epoch 00019: val_categorical_accuracy improved from 0.50370 to 0.51155, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 20/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.5311 - categorical_accuracy: 0.7497 - val_loss: 1.7979 - val_categorical_accuracy: 0.4968\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.51155\n","Epoch 21/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4848 - categorical_accuracy: 0.7077 - val_loss: 1.7882 - val_categorical_accuracy: 0.4958\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.51155\n","Epoch 22/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4954 - categorical_accuracy: 0.7664 - val_loss: 1.7765 - val_categorical_accuracy: 0.4727\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.51155\n","Epoch 23/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4449 - categorical_accuracy: 0.7470 - val_loss: 1.7549 - val_categorical_accuracy: 0.5111\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.51155\n","Epoch 24/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.4438 - categorical_accuracy: 0.7281 - val_loss: 1.7411 - val_categorical_accuracy: 0.5055\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.51155\n","Epoch 25/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3863 - categorical_accuracy: 0.7336 - val_loss: 1.7339 - val_categorical_accuracy: 0.4778\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.51155\n","Epoch 26/100\n","49/49 [==============================] - 0s 8ms/step - loss: 1.3700 - categorical_accuracy: 0.7646 - val_loss: 1.7133 - val_categorical_accuracy: 0.5189\n","\n","Epoch 00026: val_categorical_accuracy improved from 0.51155 to 0.51895, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 27/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3443 - categorical_accuracy: 0.7762 - val_loss: 1.7028 - val_categorical_accuracy: 0.5106\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.51895\n","Epoch 28/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.3321 - categorical_accuracy: 0.8092 - val_loss: 1.6909 - val_categorical_accuracy: 0.5250\n","\n","Epoch 00028: val_categorical_accuracy improved from 0.51895 to 0.52495, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 29/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.3312 - categorical_accuracy: 0.8109 - val_loss: 1.6780 - val_categorical_accuracy: 0.5037\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.52495\n","Epoch 30/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2840 - categorical_accuracy: 0.8666 - val_loss: 1.6631 - val_categorical_accuracy: 0.5240\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.52495\n","Epoch 31/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2997 - categorical_accuracy: 0.8464 - val_loss: 1.6517 - val_categorical_accuracy: 0.5254\n","\n","Epoch 00031: val_categorical_accuracy improved from 0.52495 to 0.52542, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 32/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2633 - categorical_accuracy: 0.8389 - val_loss: 1.6428 - val_categorical_accuracy: 0.5263\n","\n","Epoch 00032: val_categorical_accuracy improved from 0.52542 to 0.52634, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 33/100\n","49/49 [==============================] - 0s 4ms/step - loss: 1.3101 - categorical_accuracy: 0.8596 - val_loss: 1.6342 - val_categorical_accuracy: 0.5319\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.52634 to 0.53189, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 34/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2681 - categorical_accuracy: 0.8181 - val_loss: 1.6244 - val_categorical_accuracy: 0.5268\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.53189\n","Epoch 35/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2441 - categorical_accuracy: 0.8619 - val_loss: 1.6134 - val_categorical_accuracy: 0.5305\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.53189\n","Epoch 36/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1923 - categorical_accuracy: 0.8432 - val_loss: 1.6032 - val_categorical_accuracy: 0.5296\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.53189\n","Epoch 37/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2151 - categorical_accuracy: 0.8894 - val_loss: 1.6034 - val_categorical_accuracy: 0.5102\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.53189\n","Epoch 38/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1817 - categorical_accuracy: 0.8992 - val_loss: 1.5846 - val_categorical_accuracy: 0.5259\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.53189\n","Epoch 39/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.2030 - categorical_accuracy: 0.8844 - val_loss: 1.5761 - val_categorical_accuracy: 0.5291\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.53189\n","Epoch 40/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1825 - categorical_accuracy: 0.8562 - val_loss: 1.5788 - val_categorical_accuracy: 0.5176\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.53189\n","Epoch 41/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1489 - categorical_accuracy: 0.9024 - val_loss: 1.5629 - val_categorical_accuracy: 0.5268\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.53189\n","Epoch 42/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1324 - categorical_accuracy: 0.9122 - val_loss: 1.5475 - val_categorical_accuracy: 0.5323\n","\n","Epoch 00042: val_categorical_accuracy improved from 0.53189 to 0.53235, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 43/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1416 - categorical_accuracy: 0.8997 - val_loss: 1.5497 - val_categorical_accuracy: 0.5425\n","\n","Epoch 00043: val_categorical_accuracy improved from 0.53235 to 0.54251, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 44/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.1018 - categorical_accuracy: 0.9299 - val_loss: 1.5425 - val_categorical_accuracy: 0.5421\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.54251\n","Epoch 45/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0650 - categorical_accuracy: 0.9204 - val_loss: 1.5427 - val_categorical_accuracy: 0.5333\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.54251\n","Epoch 46/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0691 - categorical_accuracy: 0.9212 - val_loss: 1.5258 - val_categorical_accuracy: 0.5384\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.54251\n","Epoch 47/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0616 - categorical_accuracy: 0.9055 - val_loss: 1.5191 - val_categorical_accuracy: 0.5661\n","\n","Epoch 00047: val_categorical_accuracy improved from 0.54251 to 0.56608, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 48/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0949 - categorical_accuracy: 0.8979 - val_loss: 1.5130 - val_categorical_accuracy: 0.5370\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.56608\n","Epoch 49/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0310 - categorical_accuracy: 0.9317 - val_loss: 1.5053 - val_categorical_accuracy: 0.5467\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.56608\n","Epoch 50/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0397 - categorical_accuracy: 0.9153 - val_loss: 1.5032 - val_categorical_accuracy: 0.5485\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.56608\n","Epoch 51/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0297 - categorical_accuracy: 0.9295 - val_loss: 1.4930 - val_categorical_accuracy: 0.5573\n","\n","Epoch 00051: val_categorical_accuracy did not improve from 0.56608\n","Epoch 52/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0162 - categorical_accuracy: 0.9196 - val_loss: 1.4903 - val_categorical_accuracy: 0.5379\n","\n","Epoch 00052: val_categorical_accuracy did not improve from 0.56608\n","Epoch 53/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0144 - categorical_accuracy: 0.9177 - val_loss: 1.4816 - val_categorical_accuracy: 0.5421\n","\n","Epoch 00053: val_categorical_accuracy did not improve from 0.56608\n","Epoch 54/100\n","49/49 [==============================] - 0s 5ms/step - loss: 1.0127 - categorical_accuracy: 0.9231 - val_loss: 1.4773 - val_categorical_accuracy: 0.5434\n","\n","Epoch 00054: val_categorical_accuracy did not improve from 0.56608\n","Epoch 55/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9949 - categorical_accuracy: 0.9387 - val_loss: 1.4709 - val_categorical_accuracy: 0.5578\n","\n","Epoch 00055: val_categorical_accuracy did not improve from 0.56608\n","Epoch 56/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9804 - categorical_accuracy: 0.9377 - val_loss: 1.4594 - val_categorical_accuracy: 0.5518\n","\n","Epoch 00056: val_categorical_accuracy did not improve from 0.56608\n","Epoch 57/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9759 - categorical_accuracy: 0.9358 - val_loss: 1.4623 - val_categorical_accuracy: 0.5490\n","\n","Epoch 00057: val_categorical_accuracy did not improve from 0.56608\n","Epoch 58/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9694 - categorical_accuracy: 0.9278 - val_loss: 1.4539 - val_categorical_accuracy: 0.5508\n","\n","Epoch 00058: val_categorical_accuracy did not improve from 0.56608\n","Epoch 59/100\n","49/49 [==============================] - 0s 4ms/step - loss: 0.9330 - categorical_accuracy: 0.9463 - val_loss: 1.4549 - val_categorical_accuracy: 0.5490\n","\n","Epoch 00059: val_categorical_accuracy did not improve from 0.56608\n","Epoch 60/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9329 - categorical_accuracy: 0.9174 - val_loss: 1.4448 - val_categorical_accuracy: 0.5693\n","\n","Epoch 00060: val_categorical_accuracy improved from 0.56608 to 0.56932, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 61/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9343 - categorical_accuracy: 0.9283 - val_loss: 1.4437 - val_categorical_accuracy: 0.5762\n","\n","Epoch 00061: val_categorical_accuracy improved from 0.56932 to 0.57625, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 62/100\n","49/49 [==============================] - 0s 4ms/step - loss: 0.9477 - categorical_accuracy: 0.9321 - val_loss: 1.4365 - val_categorical_accuracy: 0.5776\n","\n","Epoch 00062: val_categorical_accuracy improved from 0.57625 to 0.57763, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 63/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.9047 - categorical_accuracy: 0.9286 - val_loss: 1.4226 - val_categorical_accuracy: 0.5887\n","\n","Epoch 00063: val_categorical_accuracy improved from 0.57763 to 0.58872, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 64/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8870 - categorical_accuracy: 0.9446 - val_loss: 1.4291 - val_categorical_accuracy: 0.5707\n","\n","Epoch 00064: val_categorical_accuracy did not improve from 0.58872\n","Epoch 65/100\n","49/49 [==============================] - 0s 6ms/step - loss: 0.9082 - categorical_accuracy: 0.9329 - val_loss: 1.4219 - val_categorical_accuracy: 0.5906\n","\n","Epoch 00065: val_categorical_accuracy improved from 0.58872 to 0.59057, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 66/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8820 - categorical_accuracy: 0.9459 - val_loss: 1.4212 - val_categorical_accuracy: 0.5762\n","\n","Epoch 00066: val_categorical_accuracy did not improve from 0.59057\n","Epoch 67/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8983 - categorical_accuracy: 0.9422 - val_loss: 1.4135 - val_categorical_accuracy: 0.5776\n","\n","Epoch 00067: val_categorical_accuracy did not improve from 0.59057\n","Epoch 68/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8634 - categorical_accuracy: 0.9393 - val_loss: 1.4189 - val_categorical_accuracy: 0.5568\n","\n","Epoch 00068: val_categorical_accuracy did not improve from 0.59057\n","Epoch 69/100\n","49/49 [==============================] - 0s 7ms/step - loss: 0.8846 - categorical_accuracy: 0.9378 - val_loss: 1.4100 - val_categorical_accuracy: 0.5850\n","\n","Epoch 00069: val_categorical_accuracy did not improve from 0.59057\n","Epoch 70/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8696 - categorical_accuracy: 0.9312 - val_loss: 1.4067 - val_categorical_accuracy: 0.5698\n","\n","Epoch 00070: val_categorical_accuracy did not improve from 0.59057\n","Epoch 71/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8762 - categorical_accuracy: 0.9506 - val_loss: 1.3989 - val_categorical_accuracy: 0.5855\n","\n","Epoch 00071: val_categorical_accuracy did not improve from 0.59057\n","Epoch 72/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8364 - categorical_accuracy: 0.9519 - val_loss: 1.4007 - val_categorical_accuracy: 0.5661\n","\n","Epoch 00072: val_categorical_accuracy did not improve from 0.59057\n","Epoch 73/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8272 - categorical_accuracy: 0.9474 - val_loss: 1.3951 - val_categorical_accuracy: 0.5795\n","\n","Epoch 00073: val_categorical_accuracy did not improve from 0.59057\n","Epoch 74/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8272 - categorical_accuracy: 0.9428 - val_loss: 1.3857 - val_categorical_accuracy: 0.5702\n","\n","Epoch 00074: val_categorical_accuracy did not improve from 0.59057\n","Epoch 75/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8470 - categorical_accuracy: 0.9442 - val_loss: 1.3885 - val_categorical_accuracy: 0.5860\n","\n","Epoch 00075: val_categorical_accuracy did not improve from 0.59057\n","Epoch 76/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8390 - categorical_accuracy: 0.9489 - val_loss: 1.3815 - val_categorical_accuracy: 0.5642\n","\n","Epoch 00076: val_categorical_accuracy did not improve from 0.59057\n","Epoch 77/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8097 - categorical_accuracy: 0.9584 - val_loss: 1.3722 - val_categorical_accuracy: 0.5795\n","\n","Epoch 00077: val_categorical_accuracy did not improve from 0.59057\n","Epoch 78/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8358 - categorical_accuracy: 0.9369 - val_loss: 1.3684 - val_categorical_accuracy: 0.5712\n","\n","Epoch 00078: val_categorical_accuracy did not improve from 0.59057\n","Epoch 79/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7981 - categorical_accuracy: 0.9447 - val_loss: 1.3719 - val_categorical_accuracy: 0.5823\n","\n","Epoch 00079: val_categorical_accuracy did not improve from 0.59057\n","Epoch 80/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7999 - categorical_accuracy: 0.9458 - val_loss: 1.3719 - val_categorical_accuracy: 0.5767\n","\n","Epoch 00080: val_categorical_accuracy did not improve from 0.59057\n","Epoch 81/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.8109 - categorical_accuracy: 0.9378 - val_loss: 1.3659 - val_categorical_accuracy: 0.5813\n","\n","Epoch 00081: val_categorical_accuracy did not improve from 0.59057\n","Epoch 82/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7654 - categorical_accuracy: 0.9531 - val_loss: 1.3590 - val_categorical_accuracy: 0.5883\n","\n","Epoch 00082: val_categorical_accuracy did not improve from 0.59057\n","Epoch 83/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7717 - categorical_accuracy: 0.9341 - val_loss: 1.3569 - val_categorical_accuracy: 0.5804\n","\n","Epoch 00083: val_categorical_accuracy did not improve from 0.59057\n","Epoch 84/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7887 - categorical_accuracy: 0.9370 - val_loss: 1.3522 - val_categorical_accuracy: 0.5818\n","\n","Epoch 00084: val_categorical_accuracy did not improve from 0.59057\n","Epoch 85/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7597 - categorical_accuracy: 0.9612 - val_loss: 1.3511 - val_categorical_accuracy: 0.5938\n","\n","Epoch 00085: val_categorical_accuracy improved from 0.59057 to 0.59381, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 86/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7524 - categorical_accuracy: 0.9626 - val_loss: 1.3504 - val_categorical_accuracy: 0.5998\n","\n","Epoch 00086: val_categorical_accuracy improved from 0.59381 to 0.59982, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 87/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7613 - categorical_accuracy: 0.9594 - val_loss: 1.3485 - val_categorical_accuracy: 0.5952\n","\n","Epoch 00087: val_categorical_accuracy did not improve from 0.59982\n","Epoch 88/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7756 - categorical_accuracy: 0.9473 - val_loss: 1.3417 - val_categorical_accuracy: 0.5864\n","\n","Epoch 00088: val_categorical_accuracy did not improve from 0.59982\n","Epoch 89/100\n","49/49 [==============================] - 0s 6ms/step - loss: 0.7817 - categorical_accuracy: 0.9500 - val_loss: 1.3495 - val_categorical_accuracy: 0.5933\n","\n","Epoch 00089: val_categorical_accuracy did not improve from 0.59982\n","Epoch 90/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7560 - categorical_accuracy: 0.9558 - val_loss: 1.3389 - val_categorical_accuracy: 0.5984\n","\n","Epoch 00090: val_categorical_accuracy did not improve from 0.59982\n","Epoch 91/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7388 - categorical_accuracy: 0.9488 - val_loss: 1.3355 - val_categorical_accuracy: 0.5883\n","\n","Epoch 00091: val_categorical_accuracy did not improve from 0.59982\n","Epoch 92/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7436 - categorical_accuracy: 0.9421 - val_loss: 1.3350 - val_categorical_accuracy: 0.5915\n","\n","Epoch 00092: val_categorical_accuracy did not improve from 0.59982\n","Epoch 93/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7170 - categorical_accuracy: 0.9479 - val_loss: 1.3291 - val_categorical_accuracy: 0.5929\n","\n","Epoch 00093: val_categorical_accuracy did not improve from 0.59982\n","Epoch 94/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7161 - categorical_accuracy: 0.9658 - val_loss: 1.3282 - val_categorical_accuracy: 0.5938\n","\n","Epoch 00094: val_categorical_accuracy did not improve from 0.59982\n","Epoch 95/100\n","49/49 [==============================] - 0s 6ms/step - loss: 0.7059 - categorical_accuracy: 0.9563 - val_loss: 1.3227 - val_categorical_accuracy: 0.5947\n","\n","Epoch 00095: val_categorical_accuracy did not improve from 0.59982\n","Epoch 96/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7048 - categorical_accuracy: 0.9570 - val_loss: 1.3218 - val_categorical_accuracy: 0.5933\n","\n","Epoch 00096: val_categorical_accuracy did not improve from 0.59982\n","Epoch 97/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7109 - categorical_accuracy: 0.9661 - val_loss: 1.3250 - val_categorical_accuracy: 0.5855\n","\n","Epoch 00097: val_categorical_accuracy did not improve from 0.59982\n","Epoch 98/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7175 - categorical_accuracy: 0.9493 - val_loss: 1.3178 - val_categorical_accuracy: 0.5966\n","\n","Epoch 00098: val_categorical_accuracy did not improve from 0.59982\n","Epoch 99/100\n","49/49 [==============================] - 0s 5ms/step - loss: 0.7031 - categorical_accuracy: 0.9686 - val_loss: 1.3157 - val_categorical_accuracy: 0.6058\n","\n","Epoch 00099: val_categorical_accuracy improved from 0.59982 to 0.60582, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/V1/Trained_models/transferred_models/fine_tune_on_botswana_with_70_samples_from_each_class_in_training_set_and_300_source_training_samples_per_class_used_in_pretraining.h5\n","Epoch 100/100\n","49/49 [==============================] - 0s 4ms/step - loss: 0.6749 - categorical_accuracy: 0.9604 - val_loss: 1.3141 - val_categorical_accuracy: 0.5910\n","\n","Epoch 00100: val_categorical_accuracy did not improve from 0.60582\n","68/68 [==============================] - 0s 2ms/step - loss: 1.3157 - categorical_accuracy: 0.6058\n","68/68 [==============================] - 0s 1ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"id":"kiUu9sP3QZt-","executionInfo":{"status":"ok","timestamp":1608532864698,"user_tz":480,"elapsed":1020,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"96b6b9a0-f123-47b0-d9f3-fc5f56473e91"},"source":["transfer_results"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training samples per class finetuning</th>\n","      <th>Training Samples per class pretraining</th>\n","      <th>Training Samples</th>\n","      <th>Test Samples</th>\n","      <th>Test_Accuracies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>30</td>\n","      <td>200</td>\n","      <td>420</td>\n","      <td>2724</td>\n","      <td>46.48</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30</td>\n","      <td>250</td>\n","      <td>420</td>\n","      <td>2724</td>\n","      <td>46.48</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30</td>\n","      <td>300</td>\n","      <td>420</td>\n","      <td>2724</td>\n","      <td>69.75</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>40</td>\n","      <td>200</td>\n","      <td>560</td>\n","      <td>2584</td>\n","      <td>69.75</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>40</td>\n","      <td>250</td>\n","      <td>560</td>\n","      <td>2584</td>\n","      <td>58.66</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>40</td>\n","      <td>300</td>\n","      <td>560</td>\n","      <td>2584</td>\n","      <td>58.66</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>50</td>\n","      <td>200</td>\n","      <td>700</td>\n","      <td>2444</td>\n","      <td>49.77</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>50</td>\n","      <td>250</td>\n","      <td>700</td>\n","      <td>2444</td>\n","      <td>49.77</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>50</td>\n","      <td>300</td>\n","      <td>700</td>\n","      <td>2444</td>\n","      <td>69.20</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>60</td>\n","      <td>200</td>\n","      <td>840</td>\n","      <td>2304</td>\n","      <td>69.20</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>60</td>\n","      <td>250</td>\n","      <td>840</td>\n","      <td>2304</td>\n","      <td>61.18</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>60</td>\n","      <td>300</td>\n","      <td>840</td>\n","      <td>2304</td>\n","      <td>61.18</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>70</td>\n","      <td>200</td>\n","      <td>980</td>\n","      <td>2164</td>\n","      <td>53.97</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>70</td>\n","      <td>250</td>\n","      <td>980</td>\n","      <td>2164</td>\n","      <td>53.97</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>70</td>\n","      <td>300</td>\n","      <td>980</td>\n","      <td>2164</td>\n","      <td>71.44</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Training samples per class finetuning  ...  Test_Accuracies\n","0                                      30  ...            46.48\n","1                                      30  ...            46.48\n","2                                      30  ...            69.75\n","3                                      40  ...            69.75\n","4                                      40  ...            58.66\n","5                                      40  ...            58.66\n","6                                      50  ...            49.77\n","7                                      50  ...            49.77\n","8                                      50  ...            69.20\n","9                                      60  ...            69.20\n","10                                     60  ...            61.18\n","11                                     60  ...            61.18\n","12                                     70  ...            53.97\n","13                                     70  ...            53.97\n","14                                     70  ...            71.44\n","\n","[15 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"5Ad6PvGGH6J2"},"source":[""],"execution_count":null,"outputs":[]}]}