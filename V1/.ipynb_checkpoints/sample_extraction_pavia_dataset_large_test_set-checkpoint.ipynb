{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import Input, Add, Dense, ReLU, Activation, ZeroPadding3D, Lambda, BatchNormalization \n",
    "from tensorflow.keras.layers import Flatten, Conv3D, Conv2D, concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hyperspectral Dataset - Pavia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uPavia = sio.loadmat('PaviaU.mat')\n",
    "gt_uPavia = sio.loadmat('PaviaU_gt.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_pavia = uPavia['paviaU']\n",
    "data = uPavia['paviaU']\n",
    "ground_truth = gt_uPavia['paviaU_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 340, 103)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 340)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distrubution of samples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>164624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  samples\n",
       "0      0   164624\n",
       "1      1     6631\n",
       "2      2    18649\n",
       "3      3     2099\n",
       "4      4     3064\n",
       "5      5     1345\n",
       "6      6     5029\n",
       "7      7     1330\n",
       "8      8     3682\n",
       "9      9      947"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution = pd.DataFrame(np.unique(ground_truth, return_counts = True))\n",
    "class_distribution = class_distribution.transpose()\n",
    "class_distribution.columns = ['class','samples']\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes , counts = np.unique(ground_truth, return_counts = True)\n",
    "classes = classes[1:] ## Not considering background\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick samples belonging to all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pick_samples_from_class(Class, cube_size, data, ground_truth, cubes, output_class, overlap_ratio, channels):\n",
    "    \n",
    "    ## Get row and column position from ground truth image for class\n",
    "    class_indices = np.where(ground_truth == Class)\n",
    "    \n",
    "    ## Remove border position class samples\n",
    "    class_cube_positions = [[class_indices[0][i], class_indices[1][i]] for i in range(len(class_indices[0])) \n",
    "                        if len(ground_truth) - np.ceil(cube_size / 2) > class_indices[0][i] > np.ceil(cube_size / 2) \n",
    "                        and len(ground_truth[0]) - np.ceil(cube_size / 2) > class_indices[1][i] > np.ceil(cube_size / 2)]\n",
    "    \n",
    "    #print('Length of class positions', len(class_cube_positions))\n",
    "    \n",
    "    extracted_cubes = [[class_cube_positions[0][0], class_cube_positions[0][1]]]\n",
    "    \n",
    "    ## Form the first cube for this class\n",
    "    cubes.append(np.array(data[class_cube_positions[0][0] - int(cube_size / 2):class_cube_positions[0][0] + int(cube_size / 2),\n",
    "                       (class_cube_positions[0][1] - int(cube_size / 2)):class_cube_positions[0][1] + int(cube_size / 2),\n",
    "                         :channels]))\n",
    "    \n",
    "    ## Output class value\n",
    "    output_class.append(Class)\n",
    "        \n",
    "    ## Pick cube/sample if it satisfies the criteria for the overlap ratio\n",
    "    for i in range(1, len(class_cube_positions)):\n",
    "        \n",
    "        distance_vector = [] ## Calculate distance from existing sample to the next candiddate cube sample\n",
    "        \n",
    "        for k in range(len(extracted_cubes)):\n",
    "            \n",
    "            distance = math.sqrt((class_cube_positions[i][0] - extracted_cubes[k][0]) ** 2 + \n",
    "                                 (class_cube_positions[i][1] - extracted_cubes[k][1]) ** 2)\n",
    "            \n",
    "            distance_vector.append(distance)\n",
    "            \n",
    "        if np.min(distance_vector) > int(cube_size * (1 - overlap_ratio)):\n",
    "            \n",
    "            cubes.append(np.array(data[class_cube_positions[i][0] - int(cube_size / 2):class_cube_positions[i][0] + int(cube_size / 2),\n",
    "                                      (class_cube_positions[i][1] - int(cube_size / 2)):class_cube_positions[i][1] + int(cube_size / 2),\n",
    "                                      :channels]))\n",
    "            \n",
    "            output_class.append(Class)\n",
    "            extracted_cubes.append([class_cube_positions[i][0], class_cube_positions[i][1]])\n",
    "            \n",
    "    return cubes, output_class, extracted_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and combine samples from all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_samples_from_all_classes(classes, cube_size, data, ground_truth, cubes, output_class, overlap_ratio, channels):\n",
    "    \n",
    "    class_samples = []\n",
    "    \n",
    "    for Class in classes:\n",
    "        cubes, output_class, extracted_cubes = pick_samples_from_class(Class, cube_size, data, ground_truth, cubes, \n",
    "                                                                       output_class,overlap_ratio, channels)\n",
    "        class_samples.append(len(extracted_cubes))\n",
    "    \n",
    "    cubes = np.array(cubes)\n",
    "    output_class = np.array(output_class)\n",
    "    \n",
    "    print('Class Samples : ', class_samples)\n",
    "    \n",
    "    return cubes, output_class, class_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes, output_class, class_samples = collect_samples_from_all_classes(classes = classes, cube_size = 20, data = data, ground_truth = ground_truth, cubes = [], output_class = [], overlap_ratio = 1, channels = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_test_set(training_samples_from_each_class, validation_samples_from_each_class, \n",
    "                          class_samples, cubes, output_class):\n",
    "    \n",
    "    class_1_samples = cubes[np.where(output_class == 1)[0]]\n",
    "    class_1_labels = output_class[np.where(output_class == 1)[0]]\n",
    "\n",
    "    class_2_samples = cubes[np.where(output_class == 2)[0]]\n",
    "    class_2_labels = output_class[np.where(output_class == 2)[0]]\n",
    "\n",
    "    class_3_samples = cubes[np.where(output_class == 3)[0]]\n",
    "    class_3_labels = output_class[np.where(output_class == 3)[0]]\n",
    "\n",
    "    class_4_samples = cubes[np.where(output_class == 4)[0]]\n",
    "    class_4_labels = output_class[np.where(output_class == 4)[0]]\n",
    "\n",
    "    class_5_samples = cubes[np.where(output_class == 5)[0]]\n",
    "    class_5_labels = output_class[np.where(output_class == 5)[0]]\n",
    "\n",
    "    class_6_samples = cubes[np.where(output_class == 6)[0]]\n",
    "    class_6_labels = output_class[np.where(output_class == 6)[0]]\n",
    "\n",
    "    class_7_samples = cubes[np.where(output_class == 7)[0]]\n",
    "    class_7_labels = output_class[np.where(output_class == 7)[0]]\n",
    "\n",
    "    class_8_samples = cubes[np.where(output_class == 8)[0]]\n",
    "    class_8_labels = output_class[np.where(output_class == 8)[0]]\n",
    "\n",
    "    class_9_samples = cubes[np.where(output_class == 9)[0]]\n",
    "    class_9_labels = output_class[np.where(output_class == 9)[0]]\n",
    "\n",
    "    #print(len(class_1_samples), len(class_1_labels))\n",
    "    #print(len(class_2_samples), len(class_2_labels))\n",
    "    #print(len(class_3_samples), len(class_3_labels))\n",
    "    #print(len(class_4_samples), len(class_4_labels))\n",
    "    #print(len(class_5_samples), len(class_5_labels))\n",
    "    #print(len(class_6_samples), len(class_6_labels))\n",
    "    #print(len(class_7_samples), len(class_7_labels))\n",
    "    #print(len(class_8_samples), len(class_8_labels))\n",
    "    #print(len(class_9_samples), len(class_9_labels))\n",
    "\n",
    "    class_samples_collection = [class_1_samples, class_2_samples, class_3_samples, class_4_samples, class_5_samples,\n",
    "                               class_6_samples, class_7_samples, class_8_samples, class_9_samples]\n",
    "\n",
    "    class_labels_collection = [class_1_labels, class_2_labels, class_3_labels, class_4_labels, class_5_labels,\n",
    "                              class_6_labels, class_7_labels, class_8_labels, class_9_labels]\n",
    "\n",
    "    # Training & Test Set Arrays\n",
    "    X_train = []\n",
    "    X_val = []\n",
    "    X_test = []\n",
    "\n",
    "    y_train = []\n",
    "    y_val = []\n",
    "    y_test = []\n",
    "\n",
    "    # Get Training set size samples from each class\n",
    "    for samples in class_samples_collection:\n",
    "        \n",
    "        X_train.append(samples[0:training_samples_from_each_class])\n",
    "        \n",
    "        X_val.append(samples[training_samples_from_each_class : training_samples_from_each_class +\n",
    "                                                                validation_samples_from_each_class])\n",
    "        \n",
    "        X_test.append(samples[training_samples_from_each_class + validation_samples_from_each_class:])\n",
    "        \n",
    "    # Get output labels\n",
    "    for labels in class_labels_collection:\n",
    "        y_train.append(labels[0:training_samples_from_each_class])\n",
    "        \n",
    "        y_val.append(labels[training_samples_from_each_class : training_samples_from_each_class +\n",
    "                                                               validation_samples_from_each_class])\n",
    "        \n",
    "        y_test.append(labels[training_samples_from_each_class + validation_samples_from_each_class:])\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis = 0)\n",
    "    X_val = np.concatenate(X_val, axis = 0)\n",
    "    X_test = np.concatenate(X_test, axis = 0)\n",
    "\n",
    "    y_train = np.concatenate(y_train, axis = 0)\n",
    "    y_val = np.concatenate(y_val, axis = 0)\n",
    "    y_test = np.concatenate(y_test, axis = 0)\n",
    "\n",
    "#     print('Training set shape before shuffling',X_train.shape)\n",
    "#     print('Training labels before shuffling', y_train.shape)\n",
    "\n",
    "#     print('Test set shape before shuffling', X_test.shape)\n",
    "#     print('Test set labels before shuffling', y_test.shape)\n",
    "    \n",
    "#     print('\\n')\n",
    "    \n",
    "    ## Shuffle Training Set\n",
    "    samples_train = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(samples_train)\n",
    "\n",
    "    X_train = X_train[samples_train]\n",
    "    y_train = y_train[samples_train]\n",
    "\n",
    "    ## Shuffle Validation Set\n",
    "    samples_val = np.arange(X_val.shape[0])\n",
    "    np.random.shuffle(samples_val)\n",
    "\n",
    "    X_val = X_val[samples_val]\n",
    "    y_val = y_val[samples_val]\n",
    "\n",
    "    ## Shuffle Test Set\n",
    "    samples_test = np.arange(X_test.shape[0])\n",
    "    np.random.shuffle(samples_test)\n",
    "\n",
    "    X_test = X_test[samples_test]\n",
    "    y_test = y_test[samples_test]\n",
    "\n",
    "    # Get counts(samples) of each class in test set\n",
    "    values_test_set, counts_test_set = np.unique(y_test, return_counts = True)\n",
    "    values_validation_set, counts_validation_set = np.unique(y_val, return_counts = True)\n",
    "    values_training_set, counts_training_set = np.unique(y_train, return_counts = True)\n",
    "\n",
    "\n",
    "    print(\"Samples per class: \" + str(class_samples) + '\\n'\n",
    "          \"Total number of samples is \" + str(np.sum(class_samples)) + '.\\n')\n",
    "    \n",
    "    print(\"unique classes in training set: \" + str(values_training_set) + '\\n'\n",
    "          \"Total number of samples in training set is \" + str(np.sum(counts_training_set)) + '.\\n'\n",
    "          \"Samples per class in training set: \" + str(counts_training_set) + '\\n')\n",
    "\n",
    "    print(\"unique classes in validation set: \" + str(values_validation_set) + '\\n'\n",
    "          \"Total number of samples in validation set is \" + str(np.sum(counts_validation_set)) + '.\\n'\n",
    "          \"Samples per class in validation set: \" + str(counts_validation_set) + '\\n')\n",
    "\n",
    "    print(\"unique classes in test set: \" + str(values_test_set) + '\\n'\n",
    "          \"Total number of samples in test set is \" + str(np.sum(counts_test_set)) + '.\\n'\n",
    "          \"Samples per class in test set: \" + str(counts_test_set) + '\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    ## one hot encode labels\n",
    "    onehot_encoder = OneHotEncoder(sparse = False)\n",
    "\n",
    "    y_train = y_train.reshape(len(y_train), 1)\n",
    "    y_val = y_val.reshape(len(y_val), 1)\n",
    "    y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "    y_train = onehot_encoder.fit_transform(y_train)\n",
    "    y_val = onehot_encoder.fit_transform(y_val)\n",
    "    y_test = onehot_encoder.fit_transform(y_test)\n",
    "\n",
    "#     print('Training set shape',X_train.shape)\n",
    "#     print('Training labels', y_train.shape)\n",
    "\n",
    "#     print('Test set shape', X_test.shape)\n",
    "#     print('Test set labels', y_test.shape)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, counts, class_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_extraction(classes, cube_size, data, ground_truth, cubes, output_class, training_samples_from_each_class,\n",
    "                      validation_samples_from_each_class, overlap_ratio, channels):\n",
    "    \n",
    "    cubes, output_class, class_samples = collect_samples_from_all_classes(classes, \n",
    "                                                                      cube_size, \n",
    "                                                                      data,  \n",
    "                                                                      ground_truth, \n",
    "                                                                      cubes, \n",
    "                                                                      output_class , \n",
    "                                                                      overlap_ratio, \n",
    "                                                                      channels)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, counts, class_samples = training_and_test_set(\n",
    "                                                                            training_samples_from_each_class,\n",
    "                                                                            validation_samples_from_each_class,\n",
    "                                                                            class_samples, \n",
    "                                                                            cubes,\n",
    "                                                                            output_class)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, counts, class_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training and test data by extracting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, class_samples, counts = sample_extraction(classes = classes, \n",
    "                                                                            cube_size = 20, \n",
    "                                                                            data = data, \n",
    "                                                                            ground_truth = ground_truth, \n",
    "                                                                            cubes = [], \n",
    "                                                                            output_class = [], \n",
    "                                                                            for_training_set = 150,\n",
    "                                                                            overlap_ratio = 1, \n",
    "                                                                            channels = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initial_convolution_block(samples):\n",
    "    \n",
    "    X = Conv2D(64, (3, 3), strides = (2, 2), padding = 'same', name = 'conv_initial',\n",
    "               input_shape = (20, 20, 64))(samples)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cardinality = 8 #Paths\n",
    "def group_convolution(y, channels):\n",
    "    \n",
    "    assert not channels % cardinality\n",
    "    \n",
    "    d = channels // cardinality\n",
    "\n",
    "    # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "    # and convolutions are separately performed within each group\n",
    "    \n",
    "    groups = []\n",
    "    \n",
    "    for j in range(cardinality):\n",
    "        \n",
    "        if j % 2 == 0:\n",
    "            \n",
    "            no_dilation = Lambda(lambda z: z[:, :, :, j * d:j * d + d])(y)\n",
    "            groups.append(Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same')(no_dilation))\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            dilation_group = Lambda(lambda z: z[:, :, :, j * d:j * d + d])(y)\n",
    "            x = Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same', dilation_rate = 1)(dilation_group)\n",
    "            x = Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same', dilation_rate = 3)(x)\n",
    "            x = Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same', dilation_rate = 5)(x)\n",
    "            groups.append(x)\n",
    "\n",
    "            \n",
    "    # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "    y = concatenate(groups)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SG_Unit(X):\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "    l2_ = 0.01\n",
    "    X = Conv2D(64, (1, 1), kernel_regularizer = regularizers.l2(l2_), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = group_convolution(X, 64)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(128, (1, 1), kernel_regularizer = regularizers.l2(l2_), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X_shortcut = Conv2D(128, (1, 1), kernel_regularizer = regularizers.l2(l2_), padding=\"same\")(X_shortcut)\n",
    "    X_shortcut = BatchNormalization()(X_shortcut)\n",
    "    X_shortcut = Activation('relu')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model(input_shape, classes):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = initial_convolution_block(X_input)\n",
    "    X = SG_Unit(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dense(256, input_dim = X.shape, activation='relu', name = 'fc_256', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Dense(classes, input_dim = X.shape, activation = 'softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(training_set_size,\n",
    "             validation_samples_from_each_class,\n",
    "             classes,\n",
    "             cube_size,\n",
    "             overlap_ratio,\n",
    "             data,\n",
    "             ground_truth,\n",
    "             batch_size,\n",
    "             channels,\n",
    "             epochs,\n",
    "             Verbosity,\n",
    "             accuracies,\n",
    "             learning_rate):\n",
    "    \n",
    "    for i in range(len(training_set_size)):\n",
    "        \n",
    "        print(\"\\n===========================================================================================================\\n\"\n",
    "              \"Model training starts for data with \" + str(int(training_set_size[i])) + \" samples from each class in training set\\n\"\n",
    "              \"==============================================================================================================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, counts, class_samples = sample_extraction(classes = classes, \n",
    "                                                                                cube_size = cube_size, \n",
    "                                                                                data = data, \n",
    "                                                                                ground_truth = ground_truth, \n",
    "                                                                                cubes = [], \n",
    "                                                                                output_class = [], \n",
    "                                                                                training_samples_from_each_class = training_set_size[i],\n",
    "                                                                                validation_samples_from_each_class = validation_samples_from_each_class,\n",
    "                                                                                overlap_ratio = overlap_ratio, \n",
    "                                                                                channels = channels)\n",
    "        print('X_train => ' + str(X_train.shape) + '\\n' +\n",
    "              'X_val =>' + str(X_val.shape) + '\\n' +\n",
    "              'X_test  => ' + str(X_test.shape) + '\\n' +\n",
    "              'y_train => ' + str(y_train.shape) + '\\n' +\n",
    "              'y_val =>' + str(y_val.shape) + '\\n' +\n",
    "              'y_test  => ' + str(y_test.shape) + '\\n')\n",
    "\n",
    "        X_train = np.array(X_train).astype(np.float32)\n",
    "        X_val = np.array(X_val).astype(np.float32)\n",
    "        X_test = np.array(X_test).astype(np.float32)\n",
    "\n",
    "        model_to_train = model(input_shape = X_train[0].shape, classes = len(classes))\n",
    "        model_to_train.summary()\n",
    "\n",
    "        # save best model\n",
    "        model_checkpoint = ModelCheckpoint('pavia_as_source_with ' \n",
    "                                           + str(int(training_set_size[i])) \n",
    "                                           + ' samples_from_each_class_in_training_set.h5',\n",
    "                                            monitor = 'val_categorical_accuracy', \n",
    "                                            verbose = 1, \n",
    "                                            save_best_only = True)\n",
    "\n",
    "        model_to_train.compile(optimizer = keras.optimizers.SGD(learning_rate = learning_rate), \n",
    "                                                     loss = 'categorical_crossentropy', \n",
    "                                                     metrics = ['categorical_accuracy'])\n",
    "\n",
    "        model_to_train.fit(X_train, y_train, \n",
    "                          epochs = epochs, \n",
    "                          batch_size = batch_size,\n",
    "                          #validation_split = 0.2,\n",
    "                          validation_data = (X_val, y_val),\n",
    "                          verbose = Verbosity, \n",
    "                          callbacks = [model_checkpoint])\n",
    "\n",
    "        evaluation = model_to_train.evaluate(X_test, y_test)\n",
    "        print(\"Test Accuracy = \", evaluation[1])\n",
    "\n",
    "        y_pred = model_to_train.predict(X_test, verbose = 1)\n",
    "        confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(y_test, axis = 1), np.argmax(y_pred, axis = 1))\n",
    "        \n",
    "        print(\"Confusion Matrix for Training Set Size \" + str(training_set_size[i]), confusion_matrix)\n",
    "\n",
    "        accuracies.append(evaluation[1] * 100)\n",
    "\n",
    "    print(model_to_train.layers)\n",
    "                        \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================================================\n",
      "Model training starts for data with 600 samples from each class in training set\n",
      "==============================================================================================================\n",
      "\n",
      "Class Samples :  [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n",
      "Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n",
      "Total number of samples is 37959.\n",
      "\n",
      "unique classes in training set: [1 2 3 4 5 6 7 8 9]\n",
      "Total number of samples in training set is 5400.\n",
      "Samples per class in training set: [600 600 600 600 600 600 600 600 600]\n",
      "\n",
      "unique classes in validation set: [1 2 3 4 5 6 7 8 9]\n",
      "Total number of samples in validation set is 1800.\n",
      "Samples per class in validation set: [200 200 200 200 200 200 200 200 200]\n",
      "\n",
      "unique classes in test set: [1 2 3 4 5 6 7 8 9]\n",
      "Total number of samples in test set is 30759.\n",
      "Samples per class in test set: [ 5175 14262   942  2054   545  4229   530  2882   140]\n",
      "\n",
      "\n",
      "\n",
      "X_train => (5400, 20, 20, 64)\n",
      "X_val =>(1800, 20, 20, 64)\n",
      "X_test  => (30759, 20, 20, 64)\n",
      "y_train => (5400, 9)\n",
      "y_val =>(1800, 9)\n",
      "y_test  => (30759, 9)\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 10, 10, 64)   4160        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 10, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 10, 10, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 10, 10, 8)    584         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 10, 10, 8)    584         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 10, 10, 8)    584         lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 10, 10, 8)    584         lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 10, 8)    584         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 10, 10, 8)    584         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 10, 10, 8)    584         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 10, 10, 8)    584         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 10, 10, 8)    584         lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 10, 10, 8)    584         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 10, 10, 8)    584         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 10, 10, 8)    584         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 10, 10, 8)    584         lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 10, 10, 8)    584         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 10, 10, 8)    584         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 10, 10, 8)    584         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10, 10, 64)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10, 10, 64)   256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 10, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 10, 10, 128)  8320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 10, 10, 128)  8320        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10, 10, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10, 10, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10, 10, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10, 10, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 10, 128)  0           activation_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10, 10, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_256 (Dense)                  (None, 256)          33024       global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 9)            2313        fc_256[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 103,305\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - ETA: 0s - loss: 4.6206 - categorical_accuracy: 0.1144\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.17833, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 23s 109ms/step - loss: 4.6206 - categorical_accuracy: 0.1144 - val_loss: 4.4478 - val_categorical_accuracy: 0.1783\n",
      "Epoch 2/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.4470 - categorical_accuracy: 0.1665\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.17833 to 0.30500, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 24s 110ms/step - loss: 4.4470 - categorical_accuracy: 0.1665 - val_loss: 4.3632 - val_categorical_accuracy: 0.3050\n",
      "Epoch 3/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.3515 - categorical_accuracy: 0.2419\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.30500\n",
      "216/216 [==============================] - 20s 91ms/step - loss: 4.3515 - categorical_accuracy: 0.2419 - val_loss: 4.3016 - val_categorical_accuracy: 0.2867\n",
      "Epoch 4/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.2905 - categorical_accuracy: 0.2691\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.30500\n",
      "216/216 [==============================] - 23s 107ms/step - loss: 4.2905 - categorical_accuracy: 0.2691 - val_loss: 4.2629 - val_categorical_accuracy: 0.2678\n",
      "Epoch 5/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.2445 - categorical_accuracy: 0.3059\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.30500\n",
      "216/216 [==============================] - 20s 93ms/step - loss: 4.2445 - categorical_accuracy: 0.3059 - val_loss: 4.2286 - val_categorical_accuracy: 0.2556\n",
      "Epoch 6/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.2017 - categorical_accuracy: 0.3465\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.30500\n",
      "216/216 [==============================] - 17s 79ms/step - loss: 4.2017 - categorical_accuracy: 0.3465 - val_loss: 4.1912 - val_categorical_accuracy: 0.2550\n",
      "Epoch 7/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.1702 - categorical_accuracy: 0.3961\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.30500 to 0.32222, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 86ms/step - loss: 4.1702 - categorical_accuracy: 0.3961 - val_loss: 4.1656 - val_categorical_accuracy: 0.3222\n",
      "Epoch 8/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.1340 - categorical_accuracy: 0.4567\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.32222 to 0.35333, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 86ms/step - loss: 4.1340 - categorical_accuracy: 0.4567 - val_loss: 4.1415 - val_categorical_accuracy: 0.3533\n",
      "Epoch 9/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.1063 - categorical_accuracy: 0.4863\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.35333 to 0.38333, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 77ms/step - loss: 4.1063 - categorical_accuracy: 0.4863 - val_loss: 4.1186 - val_categorical_accuracy: 0.3833\n",
      "Epoch 10/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.0763 - categorical_accuracy: 0.5150\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.38333 to 0.39778, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 77ms/step - loss: 4.0763 - categorical_accuracy: 0.5150 - val_loss: 4.0968 - val_categorical_accuracy: 0.3978\n",
      "Epoch 11/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.0474 - categorical_accuracy: 0.5354\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.39778 to 0.40278, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 77ms/step - loss: 4.0474 - categorical_accuracy: 0.5354 - val_loss: 4.0773 - val_categorical_accuracy: 0.4028\n",
      "Epoch 12/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.0234 - categorical_accuracy: 0.5607\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.40278 to 0.40833, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 77ms/step - loss: 4.0234 - categorical_accuracy: 0.5607 - val_loss: 4.0535 - val_categorical_accuracy: 0.4083\n",
      "Epoch 13/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.9992 - categorical_accuracy: 0.5700\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.40833 to 0.41444, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 16s 76ms/step - loss: 3.9992 - categorical_accuracy: 0.5700 - val_loss: 4.0346 - val_categorical_accuracy: 0.4144\n",
      "Epoch 14/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.9758 - categorical_accuracy: 0.5835\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.41444 to 0.42500, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 88ms/step - loss: 3.9758 - categorical_accuracy: 0.5835 - val_loss: 4.0154 - val_categorical_accuracy: 0.4250\n",
      "Epoch 15/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.9569 - categorical_accuracy: 0.5915\n",
      "Epoch 00015: val_categorical_accuracy improved from 0.42500 to 0.42722, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 89ms/step - loss: 3.9569 - categorical_accuracy: 0.5915 - val_loss: 3.9990 - val_categorical_accuracy: 0.4272\n",
      "Epoch 16/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.9340 - categorical_accuracy: 0.5998\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.42722\n",
      "216/216 [==============================] - 23s 105ms/step - loss: 3.9340 - categorical_accuracy: 0.5998 - val_loss: 3.9793 - val_categorical_accuracy: 0.4222\n",
      "Epoch 17/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.9114 - categorical_accuracy: 0.6146\n",
      "Epoch 00017: val_categorical_accuracy improved from 0.42722 to 0.43222, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 20s 90ms/step - loss: 3.9114 - categorical_accuracy: 0.6146 - val_loss: 3.9600 - val_categorical_accuracy: 0.4322\n",
      "Epoch 18/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.8914 - categorical_accuracy: 0.6230\n",
      "Epoch 00018: val_categorical_accuracy improved from 0.43222 to 0.44833, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 20s 90ms/step - loss: 3.8914 - categorical_accuracy: 0.6230 - val_loss: 3.9435 - val_categorical_accuracy: 0.4483\n",
      "Epoch 19/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.8705 - categorical_accuracy: 0.6359\n",
      "Epoch 00019: val_categorical_accuracy improved from 0.44833 to 0.45389, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 86ms/step - loss: 3.8705 - categorical_accuracy: 0.6359 - val_loss: 3.9286 - val_categorical_accuracy: 0.4539\n",
      "Epoch 20/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.8493 - categorical_accuracy: 0.6422\n",
      "Epoch 00020: val_categorical_accuracy improved from 0.45389 to 0.46500, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 20s 93ms/step - loss: 3.8493 - categorical_accuracy: 0.6422 - val_loss: 3.9097 - val_categorical_accuracy: 0.4650\n",
      "Epoch 21/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.8317 - categorical_accuracy: 0.6559\n",
      "Epoch 00021: val_categorical_accuracy improved from 0.46500 to 0.47000, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 20s 93ms/step - loss: 3.8317 - categorical_accuracy: 0.6559 - val_loss: 3.8936 - val_categorical_accuracy: 0.4700\n",
      "Epoch 22/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.8130 - categorical_accuracy: 0.6513\n",
      "Epoch 00022: val_categorical_accuracy improved from 0.47000 to 0.47889, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 25s 114ms/step - loss: 3.8130 - categorical_accuracy: 0.6513 - val_loss: 3.8756 - val_categorical_accuracy: 0.4789\n",
      "Epoch 23/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.7907 - categorical_accuracy: 0.6715\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.47889\n",
      "216/216 [==============================] - 19s 87ms/step - loss: 3.7907 - categorical_accuracy: 0.6715 - val_loss: 3.8621 - val_categorical_accuracy: 0.4783\n",
      "Epoch 24/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.7713 - categorical_accuracy: 0.6691\n",
      "Epoch 00024: val_categorical_accuracy improved from 0.47889 to 0.49111, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 80ms/step - loss: 3.7713 - categorical_accuracy: 0.6691 - val_loss: 3.8435 - val_categorical_accuracy: 0.4911\n",
      "Epoch 25/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.7506 - categorical_accuracy: 0.6870\n",
      "Epoch 00025: val_categorical_accuracy improved from 0.49111 to 0.49611, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 78ms/step - loss: 3.7506 - categorical_accuracy: 0.6870 - val_loss: 3.8275 - val_categorical_accuracy: 0.4961\n",
      "Epoch 26/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.7333 - categorical_accuracy: 0.6898\n",
      "Epoch 00026: val_categorical_accuracy improved from 0.49611 to 0.50556, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 79ms/step - loss: 3.7333 - categorical_accuracy: 0.6898 - val_loss: 3.8091 - val_categorical_accuracy: 0.5056\n",
      "Epoch 27/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.7165 - categorical_accuracy: 0.6915\n",
      "Epoch 00027: val_categorical_accuracy improved from 0.50556 to 0.51111, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 79ms/step - loss: 3.7165 - categorical_accuracy: 0.6915 - val_loss: 3.7926 - val_categorical_accuracy: 0.5111\n",
      "Epoch 28/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.7039 - categorical_accuracy: 0.6957\n",
      "Epoch 00028: val_categorical_accuracy improved from 0.51111 to 0.51222, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 80ms/step - loss: 3.7039 - categorical_accuracy: 0.6957 - val_loss: 3.7747 - val_categorical_accuracy: 0.5122\n",
      "Epoch 29/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.6792 - categorical_accuracy: 0.7172\n",
      "Epoch 00029: val_categorical_accuracy improved from 0.51222 to 0.52167, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 18s 82ms/step - loss: 3.6792 - categorical_accuracy: 0.7172 - val_loss: 3.7605 - val_categorical_accuracy: 0.5217\n",
      "Epoch 30/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.6640 - categorical_accuracy: 0.7198\n",
      "Epoch 00030: val_categorical_accuracy improved from 0.52167 to 0.53611, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 18s 84ms/step - loss: 3.6640 - categorical_accuracy: 0.7198 - val_loss: 3.7402 - val_categorical_accuracy: 0.5361\n",
      "Epoch 31/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.6458 - categorical_accuracy: 0.7187\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.53611\n",
      "216/216 [==============================] - 19s 87ms/step - loss: 3.6458 - categorical_accuracy: 0.7187 - val_loss: 3.7285 - val_categorical_accuracy: 0.5300\n",
      "Epoch 32/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.6309 - categorical_accuracy: 0.7287\n",
      "Epoch 00032: val_categorical_accuracy improved from 0.53611 to 0.54278, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 18s 85ms/step - loss: 3.6309 - categorical_accuracy: 0.7287 - val_loss: 3.7178 - val_categorical_accuracy: 0.5428\n",
      "Epoch 33/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.6154 - categorical_accuracy: 0.7293\n",
      "Epoch 00033: val_categorical_accuracy improved from 0.54278 to 0.54778, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 18s 83ms/step - loss: 3.6154 - categorical_accuracy: 0.7293 - val_loss: 3.6996 - val_categorical_accuracy: 0.5478\n",
      "Epoch 34/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5971 - categorical_accuracy: 0.7394\n",
      "Epoch 00034: val_categorical_accuracy improved from 0.54778 to 0.56167, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 87ms/step - loss: 3.5971 - categorical_accuracy: 0.7394 - val_loss: 3.6847 - val_categorical_accuracy: 0.5617\n",
      "Epoch 35/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5860 - categorical_accuracy: 0.7406\n",
      "Epoch 00035: val_categorical_accuracy improved from 0.56167 to 0.58056, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 89ms/step - loss: 3.5860 - categorical_accuracy: 0.7406 - val_loss: 3.6694 - val_categorical_accuracy: 0.5806\n",
      "Epoch 36/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5655 - categorical_accuracy: 0.7511\n",
      "Epoch 00036: val_categorical_accuracy improved from 0.58056 to 0.61444, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 19s 88ms/step - loss: 3.5655 - categorical_accuracy: 0.7511 - val_loss: 3.6529 - val_categorical_accuracy: 0.6144\n",
      "Epoch 37/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5504 - categorical_accuracy: 0.7550\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.61444\n",
      "216/216 [==============================] - 19s 87ms/step - loss: 3.5504 - categorical_accuracy: 0.7550 - val_loss: 3.6408 - val_categorical_accuracy: 0.6039\n",
      "Epoch 38/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5362 - categorical_accuracy: 0.7593\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.61444\n",
      "216/216 [==============================] - 19s 89ms/step - loss: 3.5362 - categorical_accuracy: 0.7593 - val_loss: 3.6289 - val_categorical_accuracy: 0.5933\n",
      "Epoch 39/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5160 - categorical_accuracy: 0.7652\n",
      "Epoch 00039: val_categorical_accuracy improved from 0.61444 to 0.62000, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 22s 101ms/step - loss: 3.5160 - categorical_accuracy: 0.7652 - val_loss: 3.6145 - val_categorical_accuracy: 0.6200\n",
      "Epoch 40/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5063 - categorical_accuracy: 0.7646\n",
      "Epoch 00040: val_categorical_accuracy improved from 0.62000 to 0.62722, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 23s 107ms/step - loss: 3.5063 - categorical_accuracy: 0.7646 - val_loss: 3.6011 - val_categorical_accuracy: 0.6272\n",
      "Epoch 41/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4898 - categorical_accuracy: 0.7820\n",
      "Epoch 00041: val_categorical_accuracy improved from 0.62722 to 0.64444, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 21s 98ms/step - loss: 3.4898 - categorical_accuracy: 0.7820 - val_loss: 3.5850 - val_categorical_accuracy: 0.6444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4794 - categorical_accuracy: 0.7715\n",
      "Epoch 00042: val_categorical_accuracy improved from 0.64444 to 0.64556, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 22s 101ms/step - loss: 3.4794 - categorical_accuracy: 0.7715 - val_loss: 3.5740 - val_categorical_accuracy: 0.6456\n",
      "Epoch 43/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4583 - categorical_accuracy: 0.7802\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.64556\n",
      "216/216 [==============================] - 21s 96ms/step - loss: 3.4583 - categorical_accuracy: 0.7802 - val_loss: 3.5602 - val_categorical_accuracy: 0.6456\n",
      "Epoch 44/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4490 - categorical_accuracy: 0.7826\n",
      "Epoch 00044: val_categorical_accuracy improved from 0.64556 to 0.65111, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 77ms/step - loss: 3.4490 - categorical_accuracy: 0.7826 - val_loss: 3.5491 - val_categorical_accuracy: 0.6511\n",
      "Epoch 45/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4330 - categorical_accuracy: 0.7876\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.65111\n",
      "216/216 [==============================] - 17s 77ms/step - loss: 3.4330 - categorical_accuracy: 0.7876 - val_loss: 3.5397 - val_categorical_accuracy: 0.6456\n",
      "Epoch 46/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4216 - categorical_accuracy: 0.7906\n",
      "Epoch 00046: val_categorical_accuracy improved from 0.65111 to 0.65556, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 78ms/step - loss: 3.4216 - categorical_accuracy: 0.7906 - val_loss: 3.5247 - val_categorical_accuracy: 0.6556\n",
      "Epoch 47/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4081 - categorical_accuracy: 0.7956\n",
      "Epoch 00047: val_categorical_accuracy improved from 0.65556 to 0.66833, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 78ms/step - loss: 3.4081 - categorical_accuracy: 0.7956 - val_loss: 3.5076 - val_categorical_accuracy: 0.6683\n",
      "Epoch 48/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.3886 - categorical_accuracy: 0.7959\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.66833\n",
      "216/216 [==============================] - 17s 77ms/step - loss: 3.3886 - categorical_accuracy: 0.7959 - val_loss: 3.4976 - val_categorical_accuracy: 0.6622\n",
      "Epoch 49/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.3743 - categorical_accuracy: 0.8063\n",
      "Epoch 00049: val_categorical_accuracy improved from 0.66833 to 0.67222, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 79ms/step - loss: 3.3743 - categorical_accuracy: 0.8063 - val_loss: 3.4887 - val_categorical_accuracy: 0.6722\n",
      "Epoch 50/50\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.3691 - categorical_accuracy: 0.8037\n",
      "Epoch 00050: val_categorical_accuracy improved from 0.67222 to 0.67444, saving model to pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 17s 78ms/step - loss: 3.3691 - categorical_accuracy: 0.8037 - val_loss: 3.4732 - val_categorical_accuracy: 0.6744\n",
      "962/962 [==============================] - 31s 32ms/step - loss: 3.6208 - categorical_accuracy: 0.5760\n",
      "Test Accuracy =  0.5759615302085876\n",
      "962/962 [==============================] - 25s 26ms/step\n",
      "Confusion Matrix for Training Set Size 600 [[ 2206   523   157  1333     0     0   643    37   276]\n",
      " [ 1436 10023     0  2238     0    48   247   247    23]\n",
      " [   24     0   692     0     0     6   113    67    40]\n",
      " [    4    63    13  1862     0     4     2    18    88]\n",
      " [    0     0     0     0   526     0     0     0    19]\n",
      " [  140  2509    10     0     0  1413   139    10     8]\n",
      " [  263     0     0    17     0     0   249     0     1]\n",
      " [    9     0  1465    71     0     0   169   615   553]\n",
      " [    0     0     6     0     0     0     4     0   130]]\n",
      "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x1a447aa2d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a438127d0>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x1a25693f50>, <tensorflow.python.keras.layers.advanced_activations.ReLU object at 0x1a25698ad0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a2568d750>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x1a627520d0>, <tensorflow.python.keras.layers.core.Activation object at 0x1a6271d0d0>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a766fb750>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a47343990>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a5e8b7550>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a48769d10>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a767029d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a47343850>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a5e8b7450>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a48770610>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a4723efd0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a47347750>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a473417d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a473296d0>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a472132d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a5e8c8710>, <tensorflow.python.keras.layers.core.Lambda object at 0x1a4875b690>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a4877da50>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a627514d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a47328450>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a47341590>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a47342750>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a5e89f7d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a5e8b06d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a4875b490>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1a4878c3d0>, <tensorflow.python.keras.layers.merge.Concatenate object at 0x1a4878b390>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x1a4878b190>, <tensorflow.python.keras.layers.core.Activation object at 0x1ab94a8e10>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1ab94a83d0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x1ab94e1fd0>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x1ab94d7fd0>, <tensorflow.python.keras.layers.normalization_v2.BatchNormalization object at 0x1ab94d0a10>, <tensorflow.python.keras.layers.core.Activation object at 0x1ab94de390>, <tensorflow.python.keras.layers.core.Activation object at 0x11184e110>, <tensorflow.python.keras.layers.merge.Add object at 0x111852810>, <tensorflow.python.keras.layers.core.Activation object at 0x111852710>, <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D object at 0x111880ad0>, <tensorflow.python.keras.layers.core.Dense object at 0x111884c10>, <tensorflow.python.keras.layers.core.Dense object at 0x111884bd0>]\n"
     ]
    }
   ],
   "source": [
    "result = Training(training_set_size = [600],\n",
    "                  validation_samples_from_each_class = 200,\n",
    "                  classes = classes,\n",
    "                  cube_size = 20,\n",
    "                  overlap_ratio = 1,\n",
    "                  data = data,\n",
    "                  ground_truth = ground_truth,\n",
    "                  batch_size = 25,\n",
    "                  channels = 64,\n",
    "                  epochs = 50,\n",
    "                  Verbosity = 1,\n",
    "                  accuracies = [],\n",
    "                  learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
