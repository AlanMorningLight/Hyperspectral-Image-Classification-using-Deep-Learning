{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"SGCNN_8_pavia_to_indian_pines.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f_nG8NlV-lz-"},"source":["## Set up google colab environment"]},{"cell_type":"code","metadata":{"id":"9bq_kqWQtg0f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615769017642,"user_tz":420,"elapsed":306,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"36c719cd-f743-4517-ef32-5c18751b2030"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jK95udG-qHAy","executionInfo":{"status":"ok","timestamp":1615769018783,"user_tz":420,"elapsed":341,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["import os\n","os.chdir('/content/drive/My Drive/Hyperspectral_Image_Classification')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vn-wRAH4t3WY","executionInfo":{"status":"ok","timestamp":1615769103874,"user_tz":420,"elapsed":2367,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["from SGCNN_8_Utils import *\n","import scipy.io as sio"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ky2Qzuw_qDdS"},"source":["## Load Pavia Dataset - Source"]},{"cell_type":"code","metadata":{"id":"I7pW15RM8nSf","executionInfo":{"status":"ok","timestamp":1615769107871,"user_tz":420,"elapsed":666,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["uPavia = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/PaviaU.mat')\n","gt_Pavia = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/PaviaU_gt.mat')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"svwF-yzh-l0N","executionInfo":{"status":"ok","timestamp":1615769109347,"user_tz":420,"elapsed":584,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["data_source = uPavia['paviaU']\n","ground_truth_source = gt_Pavia['paviaU_gt']"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHQe_Xhwza79","executionInfo":{"status":"ok","timestamp":1615769109612,"user_tz":420,"elapsed":324,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"d6a4cfbf-81a5-461f-9843-54e2144a45b4"},"source":["data_source.shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(610, 340, 103)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAjjOxj3qDdb","executionInfo":{"status":"ok","timestamp":1615769109870,"user_tz":420,"elapsed":348,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"48cf3c09-90ac-4360-a0ff-41f2ca8b2e3c"},"source":["ground_truth_source.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(610, 340)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"wmLGB_VlWx6m"},"source":["# Load Indian Pines dataset - Target"]},{"cell_type":"code","metadata":{"id":"qu6T10joWpmQ","executionInfo":{"status":"ok","timestamp":1615769111360,"user_tz":420,"elapsed":334,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["uIndianPines = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/Indian_pines_corrected.mat')\n","gt_IndianPines = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/Indian_pines_gt.mat')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"09gPGGX8W2Us","executionInfo":{"status":"ok","timestamp":1615769112365,"user_tz":420,"elapsed":365,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["data_target = uIndianPines['indian_pines_corrected']\n","ground_truth_target = gt_IndianPines['indian_pines_gt']"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qG_LxbHeXBVQ","executionInfo":{"status":"ok","timestamp":1615769113461,"user_tz":420,"elapsed":364,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"2c79737d-7fc3-4cf1-9c19-d7f470904871"},"source":["data_target.shape"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(145, 145, 200)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYillUc_XDKC","executionInfo":{"status":"ok","timestamp":1615769114390,"user_tz":420,"elapsed":501,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"49edd650-960a-4549-da98-d825c0b9726c"},"source":["ground_truth_target.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(145, 145)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"5WxjgWNGqDdc"},"source":["## Distrubution of samples for each class in Source"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"yFA7eqA7qDdd","executionInfo":{"status":"ok","timestamp":1615769117011,"user_tz":420,"elapsed":276,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"1be5659a-fd6d-4344-aebd-e92c9a0cd534"},"source":["class_distribution_source = pd.DataFrame(np.unique(ground_truth_source, return_counts = True))\n","class_distribution_source = class_distribution_source.transpose()\n","class_distribution_source.columns = ['class','samples']\n","class_distribution_source"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>164624</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>6631</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>18649</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2099</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3064</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>1345</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>5029</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>1330</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>3682</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>947</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   class  samples\n","0      0   164624\n","1      1     6631\n","2      2    18649\n","3      3     2099\n","4      4     3064\n","5      5     1345\n","6      6     5029\n","7      7     1330\n","8      8     3682\n","9      9      947"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"xK0U9fRXDwwJ"},"source":["## Drop background class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ri9e6FYf-w4n","executionInfo":{"status":"ok","timestamp":1615769123763,"user_tz":420,"elapsed":271,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"b69bb8f7-0917-4e6d-efe8-81cedf166802"},"source":["classes_source , counts_source = np.unique(ground_truth_source, return_counts = True)\n","classes_source = classes_source[1:] ## Dropping classes with background\n","classes_source"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"lwQmdin9Xmeg"},"source":["# Class distribution of samples in Target"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"id":"8EjIMOQHXU3h","executionInfo":{"status":"ok","timestamp":1615769126451,"user_tz":420,"elapsed":327,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"91d72df4-6500-433d-a8fa-352be02c45f4"},"source":["class_distribution_target = pd.DataFrame(np.unique(ground_truth_target, return_counts = True))\n","class_distribution_target = class_distribution_target.transpose()\n","class_distribution_target.columns = ['class','samples']\n","class_distribution_target"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>10776</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1428</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>830</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>237</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>483</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>730</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>478</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>972</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>2455</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>593</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>205</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>1265</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>386</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>93</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    class  samples\n","0       0    10776\n","1       1       46\n","2       2     1428\n","3       3      830\n","4       4      237\n","5       5      483\n","6       6      730\n","7       7       28\n","8       8      478\n","9       9       20\n","10     10      972\n","11     11     2455\n","12     12      593\n","13     13      205\n","14     14     1265\n","15     15      386\n","16     16       93"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"uSNioWW0EE9z"},"source":["## Dropping classes with small number of samples"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgE2FPXbXtt9","executionInfo":{"status":"ok","timestamp":1615769128746,"user_tz":420,"elapsed":298,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"aa41c427-9018-49b3-b7bd-57e720fe756e"},"source":["classes_target , counts_target = np.unique(ground_truth_target, return_counts = True)\n","classes_target = classes_target[[2,3,5,6,8,10,11,12,14]] ## Dropping classes with small number of samples\n","classes_target"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2,  3,  5,  6,  8, 10, 11, 12, 14], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"l_PESscnqDde"},"source":["## Source : Pavia\n","\n","## Train model for samples extracted with different overlap ratios and a percent of  samples picked from each class to be present in the training set. \n","\n","## Model except the final fully connected layer is saved for transfer learning."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vc1sJwRqDdf","executionInfo":{"status":"ok","timestamp":1615770427398,"user_tz":420,"elapsed":1288277,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"7fdf7d90-8fae-4694-f783-f75c0b03b8a4"},"source":["pretrain_results = pretrain_source_models(percentages = [60,75,90],\n","                                          classes = classes_source,\n","                                          cube_size = 20,\n","                                          overlap_ratios = [1],\n","                                          data = data_source,\n","                                          ground_truth = ground_truth_source,\n","                                          batch_size = 20,\n","                                          channels = 64,\n","                                          epochs = 50,\n","                                          Verbosity = 1,\n","                                          accuracies = [],\n","                                          learning_rate = 0.0001,\n","                                          source_dataset = 'pavia')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 60 percent samples from each class in training set \n","==============================================================================================================\n","\n","Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n","Total number of samples is 37959.\n","\n","unique classes in training set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in training set is 22774.\n","Samples per class in training set: [3585 9037 1045 1712  807 3017  798 2209  564]\n","\n","unique classes in test set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in test set is 15185.\n","Samples per class in test set: [2390 6025  697 1142  538 2012  532 1473  376]\n","\n","X_train => (22774, 20, 20, 64)\n","X_test  => (15185, 20, 20, 64)\n","y_train => (22774, 9)\n","y_test  => (15185, 9)\n","\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 5, 5, 8, 8)   0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 5, 5, 8, 8)   0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         tf.reshape_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n","                                                                 activation_2[0][0]               \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_3[0][0]               \n","__________________________________________________________________________________________________\n","final_fully_connected (Dense)   (None, 256)          33024       global_average_pooling2d[0][0]   \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 9)            2313        final_fully_connected[0][0]      \n","==================================================================================================\n","Total params: 104,201\n","Trainable params: 103,305\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Epoch 1/50\n","1139/1139 [==============================] - 30s 25ms/step - loss: 3.8817 - categorical_accuracy: 0.5032 - val_loss: 3.2674 - val_categorical_accuracy: 0.6693\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.66934, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","1139/1139 [==============================] - 28s 24ms/step - loss: 2.9870 - categorical_accuracy: 0.7740 - val_loss: 3.0583 - val_categorical_accuracy: 0.6223\n","\n","Epoch 00002: val_categorical_accuracy did not improve from 0.66934\n","Epoch 3/50\n","1139/1139 [==============================] - 28s 24ms/step - loss: 2.6562 - categorical_accuracy: 0.8222 - val_loss: 2.8004 - val_categorical_accuracy: 0.7102\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.66934 to 0.71024, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","1139/1139 [==============================] - 28s 24ms/step - loss: 2.4370 - categorical_accuracy: 0.8738 - val_loss: 2.7167 - val_categorical_accuracy: 0.7127\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.71024 to 0.71274, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 5/50\n","1139/1139 [==============================] - 28s 24ms/step - loss: 2.2684 - categorical_accuracy: 0.9005 - val_loss: 2.4290 - val_categorical_accuracy: 0.7828\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.71274 to 0.78281, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 6/50\n","1139/1139 [==============================] - 28s 25ms/step - loss: 2.1201 - categorical_accuracy: 0.9262 - val_loss: 2.4829 - val_categorical_accuracy: 0.7368\n","\n","Epoch 00006: val_categorical_accuracy did not improve from 0.78281\n","Epoch 7/50\n","1139/1139 [==============================] - 28s 24ms/step - loss: 1.9966 - categorical_accuracy: 0.9414 - val_loss: 2.4039 - val_categorical_accuracy: 0.7431\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.78281\n","475/475 [==============================] - 4s 7ms/step - loss: 2.4290 - categorical_accuracy: 0.7828\n","Test Accuracy =  78.0\n","475/475 [==============================] - 4s 7ms/step\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 5, 5, 8, 8)   0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 5, 5, 8, 8)   0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         tf.reshape_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n","                                                                 activation_2[0][0]               \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_3[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 75 percent samples from each class in training set \n","==============================================================================================================\n","\n","Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n","Total number of samples is 37959.\n","\n","unique classes in training set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in training set is 28465.\n","Samples per class in training set: [ 4481 11296  1306  2140  1008  3771   997  2761   705]\n","\n","unique classes in test set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in test set is 9494.\n","Samples per class in test set: [1494 3766  436  714  337 1258  333  921  235]\n","\n","X_train => (28465, 20, 20, 64)\n","X_test  => (9494, 20, 20, 64)\n","y_train => (28465, 9)\n","y_test  => (9494, 9)\n","\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 5, 5, 64)     256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 5, 5, 64)     0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 5, 5, 8)      584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 5, 5, 8)      584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 5, 5, 8)      584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 5, 5, 8)      584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 5, 5, 8)      584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 5, 5, 8)      584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 5, 5, 8)      584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 5, 5, 8)      584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 5, 5, 8)      584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 5, 5, 8)      584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 5, 5, 8)      584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 5, 5, 8)      584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 5, 5, 8)      584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 5, 5, 8)      584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 5, 5, 8)      584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 5, 5, 8)      584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 5, 5, 64)     0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 5, 5, 8, 8)   0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 5, 5, 8, 8)   0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 5, 5, 64)     256         tf.reshape_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 5, 5, 64)     0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 5, 5, 128)    8320        activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 5, 5, 128)    512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 5, 5, 128)    512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 5, 5, 128)    0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 5, 5, 128)    0           batch_normalization_8[0][0]      \n","                                                                 activation_6[0][0]               \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 5, 5, 128)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","final_fully_connected (Dense)   (None, 256)          33024       global_average_pooling2d_1[0][0] \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 9)            2313        final_fully_connected[0][0]      \n","==================================================================================================\n","Total params: 104,201\n","Trainable params: 103,305\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Epoch 1/50\n","1424/1424 [==============================] - 35s 24ms/step - loss: 3.8664 - categorical_accuracy: 0.4947 - val_loss: 3.1215 - val_categorical_accuracy: 0.6476\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.64757, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","1424/1424 [==============================] - 33s 23ms/step - loss: 2.9296 - categorical_accuracy: 0.7668 - val_loss: 2.7201 - val_categorical_accuracy: 0.7926\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.64757 to 0.79261, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","1424/1424 [==============================] - 33s 23ms/step - loss: 2.5602 - categorical_accuracy: 0.8429 - val_loss: 2.5336 - val_categorical_accuracy: 0.7728\n","\n","Epoch 00003: val_categorical_accuracy did not improve from 0.79261\n","Epoch 4/50\n","1424/1424 [==============================] - 33s 23ms/step - loss: 2.3082 - categorical_accuracy: 0.8895 - val_loss: 2.4000 - val_categorical_accuracy: 0.8062\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.79261 to 0.80619, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 5/50\n","1424/1424 [==============================] - 33s 24ms/step - loss: 2.1275 - categorical_accuracy: 0.9164 - val_loss: 2.2382 - val_categorical_accuracy: 0.8631\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.80619 to 0.86307, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 6/50\n","1424/1424 [==============================] - 34s 24ms/step - loss: 1.9827 - categorical_accuracy: 0.9364 - val_loss: 2.1555 - val_categorical_accuracy: 0.8452\n","\n","Epoch 00006: val_categorical_accuracy did not improve from 0.86307\n","Epoch 7/50\n","1424/1424 [==============================] - 34s 24ms/step - loss: 1.8544 - categorical_accuracy: 0.9458 - val_loss: 1.9731 - val_categorical_accuracy: 0.8542\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.86307\n","297/297 [==============================] - 3s 8ms/step - loss: 2.2382 - categorical_accuracy: 0.8631\n","Test Accuracy =  86.0\n","297/297 [==============================] - 2s 7ms/step\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 5, 5, 64)     256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 5, 5, 64)     0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 5, 5, 8)      584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 5, 5, 8)      584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 5, 5, 8)      584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 5, 5, 8)      584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 5, 5, 8)      584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 5, 5, 8)      584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 5, 5, 8)      584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 5, 5, 8)      584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 5, 5, 8)      584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 5, 5, 8)      584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 5, 5, 8)      584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 5, 5, 8)      584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 5, 5, 8)      584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 5, 5, 8)      584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 5, 5, 8)      584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 5, 5, 8)      584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 5, 5, 64)     0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 5, 5, 8, 8)   0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 5, 5, 8, 8)   0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 5, 5, 64)     256         tf.reshape_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 5, 5, 64)     0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 5, 5, 128)    8320        activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 5, 5, 128)    512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 5, 5, 128)    512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 5, 5, 128)    0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 5, 5, 128)    0           batch_normalization_8[0][0]      \n","                                                                 activation_6[0][0]               \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 5, 5, 128)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_7[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 90 percent samples from each class in training set \n","==============================================================================================================\n","\n","Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n","Total number of samples is 37959.\n","\n","unique classes in training set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in training set is 34159.\n","Samples per class in training set: [ 5377 13555  1567  2568  1210  4526  1197  3313   846]\n","\n","unique classes in test set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in test set is 3800.\n","Samples per class in test set: [ 598 1507  175  286  135  503  133  369   94]\n","\n","X_train => (34159, 20, 20, 64)\n","X_test  => (3800, 20, 20, 64)\n","y_train => (34159, 9)\n","y_test  => (3800, 9)\n","\n","Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 5, 5, 64)     256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 5, 5, 64)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 5, 5, 8)      584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 5, 5, 8)      584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 5, 5, 8)      584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 5, 5, 8)      584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 5, 5, 8)      584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 5, 5, 8)      584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 5, 5, 8)      584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 5, 5, 8)      584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 5, 5, 8)      584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 5, 5, 8)      584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 5, 5, 8)      584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 5, 5, 8)      584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 5, 5, 8)      584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 5, 5, 8)      584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 5, 5, 8)      584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 5, 5, 8)      584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 5, 5, 64)     0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 5, 5, 8, 8)   0           concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 5, 5, 8, 8)   0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 5, 5, 64)     256         tf.reshape_5[0][0]               \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 5, 5, 64)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 5, 5, 128)    8320        activation_9[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 5, 5, 128)    512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 5, 5, 128)    0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 5, 5, 128)    0           batch_normalization_13[0][0]     \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 5, 5, 128)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_11[0][0]              \n","__________________________________________________________________________________________________\n","final_fully_connected (Dense)   (None, 256)          33024       global_average_pooling2d_2[0][0] \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 9)            2313        final_fully_connected[0][0]      \n","==================================================================================================\n","Total params: 104,201\n","Trainable params: 103,305\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Epoch 1/50\n","1708/1708 [==============================] - 41s 23ms/step - loss: 3.7647 - categorical_accuracy: 0.5350 - val_loss: 3.0247 - val_categorical_accuracy: 0.7337\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.73368, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","1708/1708 [==============================] - 39s 23ms/step - loss: 2.8453 - categorical_accuracy: 0.7872 - val_loss: 2.7021 - val_categorical_accuracy: 0.7724\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.73368 to 0.77237, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","1708/1708 [==============================] - 39s 23ms/step - loss: 2.4804 - categorical_accuracy: 0.8594 - val_loss: 2.4249 - val_categorical_accuracy: 0.8192\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.77237 to 0.81921, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","1708/1708 [==============================] - 39s 23ms/step - loss: 2.2453 - categorical_accuracy: 0.8942 - val_loss: 2.3916 - val_categorical_accuracy: 0.7966\n","\n","Epoch 00004: val_categorical_accuracy did not improve from 0.81921\n","Epoch 5/50\n","1708/1708 [==============================] - 39s 23ms/step - loss: 2.0707 - categorical_accuracy: 0.9134 - val_loss: 2.3053 - val_categorical_accuracy: 0.7832\n","\n","Epoch 00005: val_categorical_accuracy did not improve from 0.81921\n","119/119 [==============================] - 1s 8ms/step - loss: 2.4249 - categorical_accuracy: 0.8192\n","Test Accuracy =  82.0\n","119/119 [==============================] - 1s 7ms/step\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 5, 5, 64)     256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 5, 5, 64)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 5, 5, 8)      584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 5, 5, 8)      584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 5, 5, 8)      584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 5, 5, 8)      584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 5, 5, 8)      584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 5, 5, 8)      584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 5, 5, 8)      584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 5, 5, 8)      584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 5, 5, 8)      584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 5, 5, 8)      584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 5, 5, 8)      584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 5, 5, 8)      584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 5, 5, 8)      584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 5, 5, 8)      584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 5, 5, 8)      584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 5, 5, 8)      584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 5, 5, 64)     0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 5, 5, 8, 8)   0           concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 5, 5, 8, 8)   0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 5, 5, 64)     256         tf.reshape_5[0][0]               \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 5, 5, 64)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 5, 5, 128)    8320        activation_9[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 5, 5, 128)    512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 5, 5, 128)    0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 5, 5, 128)    0           batch_normalization_13[0][0]     \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 5, 5, 128)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_11[0][0]              \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dCxOwh-vpTVi","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1615770670365,"user_tz":420,"elapsed":382,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"680e626f-4ca7-46ab-a59a-b8bccf5f9c94"},"source":["pretrain_results"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Overlap_ratio</th>\n","      <th>Training Samples</th>\n","      <th>Test Samples</th>\n","      <th>Training_Test_Split</th>\n","      <th>Test_Accuracies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>22774</td>\n","      <td>15185</td>\n","      <td>60</td>\n","      <td>78.28</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>28465</td>\n","      <td>9494</td>\n","      <td>75</td>\n","      <td>86.31</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>34159</td>\n","      <td>3800</td>\n","      <td>90</td>\n","      <td>81.92</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Overlap_ratio  Training Samples  ...  Training_Test_Split  Test_Accuracies\n","0              1             22774  ...                   60            78.28\n","1              1             28465  ...                   75            86.31\n","2              1             34159  ...                   90            81.92\n","\n","[3 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"RkxgCjIejh0l"},"source":["# Fine tune on Indian Pines"]},{"cell_type":"code","metadata":{"id":"SJ1kSkHtCuHd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615770779142,"user_tz":420,"elapsed":99718,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"5ed50203-644f-49a5-f205-154a1ce4646b"},"source":["transfer_results, confusion_matrixes = transfer_learning(percentages = [60,75,90],\n","                                                        source_dataset = 'pavia',\n","                                                        target_dataset = 'indian_pines',\n","                                                        data = data_target,\n","                                                        ground_truth = ground_truth_target,\n","                                                        classes = classes_target,\n","                                                        overlap_ratios = [1],\n","                                                        channels = 64,\n","                                                        cube_size = 20,\n","                                                        learning_rate = 0.0001,\n","                                                        epochs = 50,\n","                                                        batch_size = 20)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 60 percent samples from each class in training set \n","==============================================================================================================\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 5, 5, 8, 8)   0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 5, 5, 8, 8)   0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         tf.reshape_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n","                                                                 activation_2[0][0]               \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_3[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n","Total number of samples is 7906.\n","\n","unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in training set is 4740.\n","Samples per class in training set: [ 820  313  193  438  213  502 1334  276  651]\n","\n","unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in test set is 3166.\n","Samples per class in test set: [548 210 130 292 143 335 890 184 434]\n","\n","X_train_transfer => (4740, 128)\n","X_test_transfer  => (3166, 128)\n","y_train => (4740, 9)\n","y_test  => (3166, 9)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc9 (Dense)                  (None, 9)                 2313      \n","=================================================================\n","Total params: 35,337\n","Trainable params: 35,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","237/237 [==============================] - 1s 2ms/step - loss: 2.0265 - categorical_accuracy: 0.3386 - val_loss: 1.5157 - val_categorical_accuracy: 0.4526\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.45262, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.4692 - categorical_accuracy: 0.4446 - val_loss: 1.4484 - val_categorical_accuracy: 0.4542\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.45262 to 0.45420, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.3746 - categorical_accuracy: 0.4975 - val_loss: 1.4374 - val_categorical_accuracy: 0.4713\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.45420 to 0.47126, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.3020 - categorical_accuracy: 0.5360 - val_loss: 1.4504 - val_categorical_accuracy: 0.4668\n","\n","Epoch 00004: val_categorical_accuracy did not improve from 0.47126\n","Epoch 5/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.2757 - categorical_accuracy: 0.5452 - val_loss: 1.4130 - val_categorical_accuracy: 0.4678\n","\n","Epoch 00005: val_categorical_accuracy did not improve from 0.47126\n","Epoch 6/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.2230 - categorical_accuracy: 0.5761 - val_loss: 1.4369 - val_categorical_accuracy: 0.4719\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.47126 to 0.47189, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 7/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1852 - categorical_accuracy: 0.5927 - val_loss: 1.3979 - val_categorical_accuracy: 0.4820\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.47189 to 0.48200, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 8/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1573 - categorical_accuracy: 0.6177 - val_loss: 1.4124 - val_categorical_accuracy: 0.4940\n","\n","Epoch 00008: val_categorical_accuracy improved from 0.48200 to 0.49400, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 9/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1355 - categorical_accuracy: 0.6248 - val_loss: 1.3947 - val_categorical_accuracy: 0.5158\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.49400 to 0.51579, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 10/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1236 - categorical_accuracy: 0.6137 - val_loss: 1.4990 - val_categorical_accuracy: 0.4700\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.51579\n","Epoch 11/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0954 - categorical_accuracy: 0.6312 - val_loss: 1.4712 - val_categorical_accuracy: 0.4817\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.51579\n","Epoch 12/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0805 - categorical_accuracy: 0.6360 - val_loss: 1.4309 - val_categorical_accuracy: 0.4905\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.51579\n","Epoch 13/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0332 - categorical_accuracy: 0.6563 - val_loss: 1.4101 - val_categorical_accuracy: 0.4984\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.51579\n","Epoch 14/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0182 - categorical_accuracy: 0.6642 - val_loss: 1.4546 - val_categorical_accuracy: 0.4896\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.51579\n","Epoch 15/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0055 - categorical_accuracy: 0.6587 - val_loss: 1.4643 - val_categorical_accuracy: 0.5051\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.51579\n","Epoch 16/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0059 - categorical_accuracy: 0.6716 - val_loss: 1.4626 - val_categorical_accuracy: 0.5250\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.51579 to 0.52495, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 17/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0005 - categorical_accuracy: 0.6639 - val_loss: 1.4217 - val_categorical_accuracy: 0.5171\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.52495\n","Epoch 18/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9652 - categorical_accuracy: 0.6816 - val_loss: 1.4115 - val_categorical_accuracy: 0.5190\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.52495\n","Epoch 19/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9588 - categorical_accuracy: 0.6815 - val_loss: 1.4207 - val_categorical_accuracy: 0.5111\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.52495\n","Epoch 20/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9396 - categorical_accuracy: 0.6889 - val_loss: 1.4030 - val_categorical_accuracy: 0.5246\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.52495\n","Epoch 21/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9287 - categorical_accuracy: 0.7040 - val_loss: 1.4318 - val_categorical_accuracy: 0.5196\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.52495\n","Epoch 22/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9405 - categorical_accuracy: 0.6945 - val_loss: 1.5060 - val_categorical_accuracy: 0.5120\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.52495\n","Epoch 23/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9285 - categorical_accuracy: 0.6862 - val_loss: 1.4440 - val_categorical_accuracy: 0.5164\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.52495\n","Epoch 24/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8943 - categorical_accuracy: 0.7058 - val_loss: 1.4108 - val_categorical_accuracy: 0.5366\n","\n","Epoch 00024: val_categorical_accuracy improved from 0.52495 to 0.53664, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 25/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9008 - categorical_accuracy: 0.7123 - val_loss: 1.4137 - val_categorical_accuracy: 0.5278\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.53664\n","Epoch 26/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8993 - categorical_accuracy: 0.7048 - val_loss: 1.4336 - val_categorical_accuracy: 0.5212\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.53664\n","Epoch 27/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8735 - categorical_accuracy: 0.7149 - val_loss: 1.4227 - val_categorical_accuracy: 0.5316\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.53664\n","Epoch 28/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8498 - categorical_accuracy: 0.7287 - val_loss: 1.3970 - val_categorical_accuracy: 0.5464\n","\n","Epoch 00028: val_categorical_accuracy improved from 0.53664 to 0.54643, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 29/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8612 - categorical_accuracy: 0.7137 - val_loss: 1.4246 - val_categorical_accuracy: 0.5234\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.54643\n","Epoch 30/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8539 - categorical_accuracy: 0.7181 - val_loss: 1.4606 - val_categorical_accuracy: 0.5250\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.54643\n","Epoch 31/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8534 - categorical_accuracy: 0.7237 - val_loss: 1.4303 - val_categorical_accuracy: 0.5268\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.54643\n","Epoch 32/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8439 - categorical_accuracy: 0.7311 - val_loss: 1.4738 - val_categorical_accuracy: 0.5227\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.54643\n","Epoch 33/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8255 - categorical_accuracy: 0.7379 - val_loss: 1.4402 - val_categorical_accuracy: 0.5351\n","\n","Epoch 00033: val_categorical_accuracy did not improve from 0.54643\n","Epoch 34/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8191 - categorical_accuracy: 0.7412 - val_loss: 1.4389 - val_categorical_accuracy: 0.5366\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.54643\n","Epoch 35/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8033 - categorical_accuracy: 0.7457 - val_loss: 1.4595 - val_categorical_accuracy: 0.5253\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.54643\n","Epoch 36/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7868 - categorical_accuracy: 0.7532 - val_loss: 1.4703 - val_categorical_accuracy: 0.5433\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.54643\n","Epoch 37/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7946 - categorical_accuracy: 0.7414 - val_loss: 1.4715 - val_categorical_accuracy: 0.5310\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.54643\n","Epoch 38/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7603 - categorical_accuracy: 0.7666 - val_loss: 1.4456 - val_categorical_accuracy: 0.5291\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.54643\n","Epoch 39/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7651 - categorical_accuracy: 0.7610 - val_loss: 1.4322 - val_categorical_accuracy: 0.5297\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.54643\n","Epoch 40/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7665 - categorical_accuracy: 0.7559 - val_loss: 1.4538 - val_categorical_accuracy: 0.5376\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.54643\n","Epoch 41/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7381 - categorical_accuracy: 0.7715 - val_loss: 1.4868 - val_categorical_accuracy: 0.5174\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.54643\n","Epoch 42/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7423 - categorical_accuracy: 0.7746 - val_loss: 1.4422 - val_categorical_accuracy: 0.5347\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.54643\n","Epoch 43/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7610 - categorical_accuracy: 0.7624 - val_loss: 1.4668 - val_categorical_accuracy: 0.5335\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.54643\n","Epoch 44/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7428 - categorical_accuracy: 0.7605 - val_loss: 1.4152 - val_categorical_accuracy: 0.5417\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.54643\n","Epoch 45/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7352 - categorical_accuracy: 0.7727 - val_loss: 1.4543 - val_categorical_accuracy: 0.5275\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.54643\n","Epoch 46/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7085 - categorical_accuracy: 0.7783 - val_loss: 1.4197 - val_categorical_accuracy: 0.5512\n","\n","Epoch 00046: val_categorical_accuracy improved from 0.54643 to 0.55117, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 47/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7047 - categorical_accuracy: 0.7804 - val_loss: 1.4255 - val_categorical_accuracy: 0.5382\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.55117\n","Epoch 48/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6934 - categorical_accuracy: 0.7942 - val_loss: 1.4439 - val_categorical_accuracy: 0.5404\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.55117\n","Epoch 49/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7109 - categorical_accuracy: 0.7814 - val_loss: 1.4467 - val_categorical_accuracy: 0.5445\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.55117\n","Epoch 50/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6912 - categorical_accuracy: 0.7864 - val_loss: 1.4479 - val_categorical_accuracy: 0.5420\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.55117\n","99/99 [==============================] - 0s 855us/step - loss: 1.4197 - categorical_accuracy: 0.5512\n","Test accuracy on target dataset = 0.55116868019104\n","99/99 [==============================] - 0s 720us/step\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 75 percent samples from each class in training set \n","==============================================================================================================\n","\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_1 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 5, 5, 64)     256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 5, 5, 64)     0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 5, 5, 8)      584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 5, 5, 8)      584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 5, 5, 8)      584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 5, 5, 8)      584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 5, 5, 8)      584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 5, 5, 8)      584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 5, 5, 8)      584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 5, 5, 8)      0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 5, 5, 8)      584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 5, 5, 8)      584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 5, 5, 8)      584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 5, 5, 8)      584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 5, 5, 8)      584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 5, 5, 8)      584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 5, 5, 8)      584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 5, 5, 8)      584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 5, 5, 8)      584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 5, 5, 64)     0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 5, 5, 8, 8)   0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 5, 5, 8, 8)   0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 5, 5, 64)     256         tf.reshape_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 5, 5, 64)     0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 5, 5, 128)    8320        activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 5, 5, 128)    512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 5, 5, 128)    512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 5, 5, 128)    0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 5, 5, 128)    0           batch_normalization_8[0][0]      \n","                                                                 activation_6[0][0]               \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 5, 5, 128)    0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_7[0][0]               \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n","Total number of samples is 7906.\n","\n","unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in training set is 5927.\n","Samples per class in training set: [1026  392  242  547  267  627 1668  345  813]\n","\n","unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in test set is 1979.\n","Samples per class in test set: [342 131  81 183  89 210 556 115 272]\n","\n","X_train_transfer => (5927, 128)\n","X_test_transfer  => (1979, 128)\n","y_train => (5927, 9)\n","y_test  => (1979, 9)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_5 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc9 (Dense)                  (None, 9)                 2313      \n","=================================================================\n","Total params: 35,337\n","Trainable params: 35,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","297/297 [==============================] - 1s 2ms/step - loss: 2.5611 - categorical_accuracy: 0.3517 - val_loss: 1.4443 - val_categorical_accuracy: 0.4260\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.42597, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.3920 - categorical_accuracy: 0.4860 - val_loss: 1.4258 - val_categorical_accuracy: 0.3876\n","\n","Epoch 00002: val_categorical_accuracy did not improve from 0.42597\n","Epoch 3/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.3069 - categorical_accuracy: 0.5153 - val_loss: 1.4438 - val_categorical_accuracy: 0.4553\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.42597 to 0.45528, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.2705 - categorical_accuracy: 0.5455 - val_loss: 1.4582 - val_categorical_accuracy: 0.4896\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.45528 to 0.48964, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 5/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.2353 - categorical_accuracy: 0.5409 - val_loss: 1.3927 - val_categorical_accuracy: 0.4346\n","\n","Epoch 00005: val_categorical_accuracy did not improve from 0.48964\n","Epoch 6/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.1801 - categorical_accuracy: 0.5703 - val_loss: 1.4180 - val_categorical_accuracy: 0.4780\n","\n","Epoch 00006: val_categorical_accuracy did not improve from 0.48964\n","Epoch 7/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.1598 - categorical_accuracy: 0.5814 - val_loss: 1.3855 - val_categorical_accuracy: 0.4098\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.48964\n","Epoch 8/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.1483 - categorical_accuracy: 0.5833 - val_loss: 1.3782 - val_categorical_accuracy: 0.5124\n","\n","Epoch 00008: val_categorical_accuracy improved from 0.48964 to 0.51238, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 9/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.1278 - categorical_accuracy: 0.5860 - val_loss: 1.4801 - val_categorical_accuracy: 0.4871\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.51238\n","Epoch 10/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.1054 - categorical_accuracy: 0.5979 - val_loss: 1.4263 - val_categorical_accuracy: 0.5174\n","\n","Epoch 00010: val_categorical_accuracy improved from 0.51238 to 0.51743, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 11/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.0843 - categorical_accuracy: 0.6047 - val_loss: 1.4112 - val_categorical_accuracy: 0.5088\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.51743\n","Epoch 12/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.0634 - categorical_accuracy: 0.6279 - val_loss: 1.3608 - val_categorical_accuracy: 0.4927\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.51743\n","Epoch 13/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.0590 - categorical_accuracy: 0.6185 - val_loss: 1.4507 - val_categorical_accuracy: 0.4922\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.51743\n","Epoch 14/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.0558 - categorical_accuracy: 0.6147 - val_loss: 1.3858 - val_categorical_accuracy: 0.4391\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.51743\n","Epoch 15/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.0383 - categorical_accuracy: 0.6363 - val_loss: 1.3944 - val_categorical_accuracy: 0.5038\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.51743\n","Epoch 16/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.0429 - categorical_accuracy: 0.6238 - val_loss: 1.4348 - val_categorical_accuracy: 0.5427\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.51743 to 0.54270, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 17/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.0087 - categorical_accuracy: 0.6331 - val_loss: 1.3833 - val_categorical_accuracy: 0.5255\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.54270\n","Epoch 18/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9893 - categorical_accuracy: 0.6402 - val_loss: 1.3866 - val_categorical_accuracy: 0.5417\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.54270\n","Epoch 19/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9966 - categorical_accuracy: 0.6457 - val_loss: 1.3661 - val_categorical_accuracy: 0.5104\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.54270\n","Epoch 20/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9965 - categorical_accuracy: 0.6397 - val_loss: 1.4200 - val_categorical_accuracy: 0.5316\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.54270\n","Epoch 21/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9825 - categorical_accuracy: 0.6524 - val_loss: 1.5503 - val_categorical_accuracy: 0.5139\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.54270\n","Epoch 22/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9628 - categorical_accuracy: 0.6495 - val_loss: 1.4506 - val_categorical_accuracy: 0.5235\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.54270\n","Epoch 23/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9716 - categorical_accuracy: 0.6515 - val_loss: 1.3943 - val_categorical_accuracy: 0.5503\n","\n","Epoch 00023: val_categorical_accuracy improved from 0.54270 to 0.55028, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 24/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9308 - categorical_accuracy: 0.6668 - val_loss: 1.3258 - val_categorical_accuracy: 0.5073\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.55028\n","Epoch 25/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9420 - categorical_accuracy: 0.6674 - val_loss: 1.4827 - val_categorical_accuracy: 0.5366\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.55028\n","Epoch 26/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9682 - categorical_accuracy: 0.6468 - val_loss: 1.5899 - val_categorical_accuracy: 0.5169\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.55028\n","Epoch 27/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9466 - categorical_accuracy: 0.6639 - val_loss: 1.4089 - val_categorical_accuracy: 0.5321\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.55028\n","Epoch 28/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9059 - categorical_accuracy: 0.6731 - val_loss: 1.3739 - val_categorical_accuracy: 0.5503\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.55028\n","Epoch 29/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8990 - categorical_accuracy: 0.6903 - val_loss: 1.3434 - val_categorical_accuracy: 0.5240\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.55028\n","Epoch 30/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9077 - categorical_accuracy: 0.6789 - val_loss: 1.4094 - val_categorical_accuracy: 0.5341\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.55028\n","Epoch 31/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8935 - categorical_accuracy: 0.6785 - val_loss: 1.3717 - val_categorical_accuracy: 0.5316\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.55028\n","Epoch 32/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8858 - categorical_accuracy: 0.6949 - val_loss: 1.3538 - val_categorical_accuracy: 0.5543\n","\n","Epoch 00032: val_categorical_accuracy improved from 0.55028 to 0.55432, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 33/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9027 - categorical_accuracy: 0.6784 - val_loss: 1.4420 - val_categorical_accuracy: 0.5235\n","\n","Epoch 00033: val_categorical_accuracy did not improve from 0.55432\n","Epoch 34/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8916 - categorical_accuracy: 0.6951 - val_loss: 1.4228 - val_categorical_accuracy: 0.5538\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.55432\n","Epoch 35/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8650 - categorical_accuracy: 0.6848 - val_loss: 1.3707 - val_categorical_accuracy: 0.5407\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.55432\n","Epoch 36/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8669 - categorical_accuracy: 0.6929 - val_loss: 1.4023 - val_categorical_accuracy: 0.5725\n","\n","Epoch 00036: val_categorical_accuracy improved from 0.55432 to 0.57251, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 37/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8616 - categorical_accuracy: 0.6920 - val_loss: 1.4569 - val_categorical_accuracy: 0.5467\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.57251\n","Epoch 38/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8609 - categorical_accuracy: 0.6917 - val_loss: 1.3610 - val_categorical_accuracy: 0.5412\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.57251\n","Epoch 39/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8604 - categorical_accuracy: 0.6892 - val_loss: 1.2646 - val_categorical_accuracy: 0.5518\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.57251\n","Epoch 40/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8499 - categorical_accuracy: 0.6964 - val_loss: 1.4208 - val_categorical_accuracy: 0.5351\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.57251\n","Epoch 41/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8400 - categorical_accuracy: 0.7065 - val_loss: 1.4083 - val_categorical_accuracy: 0.5422\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.57251\n","Epoch 42/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8281 - categorical_accuracy: 0.7110 - val_loss: 1.3571 - val_categorical_accuracy: 0.5629\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.57251\n","Epoch 43/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8331 - categorical_accuracy: 0.7129 - val_loss: 1.4390 - val_categorical_accuracy: 0.5270\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.57251\n","Epoch 44/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8336 - categorical_accuracy: 0.7028 - val_loss: 1.3682 - val_categorical_accuracy: 0.5397\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.57251\n","Epoch 45/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8249 - categorical_accuracy: 0.7112 - val_loss: 1.3515 - val_categorical_accuracy: 0.5503\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.57251\n","Epoch 46/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8347 - categorical_accuracy: 0.7033 - val_loss: 1.4378 - val_categorical_accuracy: 0.5457\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.57251\n","Epoch 47/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8067 - categorical_accuracy: 0.7131 - val_loss: 1.3756 - val_categorical_accuracy: 0.5553\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.57251\n","Epoch 48/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8225 - categorical_accuracy: 0.7083 - val_loss: 1.3943 - val_categorical_accuracy: 0.5376\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.57251\n","Epoch 49/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8050 - categorical_accuracy: 0.7179 - val_loss: 1.4377 - val_categorical_accuracy: 0.5417\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.57251\n","Epoch 50/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.7946 - categorical_accuracy: 0.7211 - val_loss: 1.3804 - val_categorical_accuracy: 0.5467\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.57251\n","62/62 [==============================] - 0s 858us/step - loss: 1.4023 - categorical_accuracy: 0.5725\n","Test accuracy on target dataset = 0.5725113749504089\n","62/62 [==============================] - 0s 672us/step\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 90 percent samples from each class in training set \n","==============================================================================================================\n","\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 10, 10, 64)   256         conv_initial[0][0]               \n","__________________________________________________________________________________________________\n","re_lu_2 (ReLU)                  (None, 10, 10, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 5, 5, 64)     256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 5, 5, 64)     0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 5, 5, 8)      584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 5, 5, 8)      584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 5, 5, 8)      584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 5, 5, 8)      584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 5, 5, 8)      584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 5, 5, 8)      584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 5, 5, 8)      584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 5, 5, 8)      0           activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 5, 5, 8)      584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 5, 5, 8)      584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 5, 5, 8)      584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 5, 5, 8)      584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 5, 5, 8)      584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 5, 5, 8)      584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 5, 5, 8)      584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 5, 5, 8)      584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 5, 5, 8)      584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 5, 5, 64)     0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 5, 5, 8, 8)   0           concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 5, 5, 8, 8)   0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 5, 5, 8, 8)   0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 5, 5, 64)     0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 5, 5, 64)     256         tf.reshape_5[0][0]               \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 5, 5, 64)     0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 5, 5, 128)    8320        activation_9[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 5, 5, 128)    512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 5, 5, 128)    512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 5, 5, 128)    0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 5, 5, 128)    0           batch_normalization_13[0][0]     \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 5, 5, 128)    0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_11[0][0]              \n","==================================================================================================\n","Total params: 68,864\n","Trainable params: 67,968\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n","Total number of samples is 7906.\n","\n","unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in training set is 7112.\n","Samples per class in training set: [1231  470  290  657  320  753 2001  414  976]\n","\n","unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in test set is 794.\n","Samples per class in test set: [137  53  33  73  36  84 223  46 109]\n","\n","X_train_transfer => (7112, 128)\n","X_test_transfer  => (794, 128)\n","y_train => (7112, 9)\n","y_test  => (794, 9)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc9 (Dense)                  (None, 9)                 2313      \n","=================================================================\n","Total params: 35,337\n","Trainable params: 35,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","356/356 [==============================] - 1s 2ms/step - loss: 2.2226 - categorical_accuracy: 0.3434 - val_loss: 1.5358 - val_categorical_accuracy: 0.4723\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.47229, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.4466 - categorical_accuracy: 0.4632 - val_loss: 1.4176 - val_categorical_accuracy: 0.4635\n","\n","Epoch 00002: val_categorical_accuracy did not improve from 0.47229\n","Epoch 3/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.3372 - categorical_accuracy: 0.5167 - val_loss: 1.3310 - val_categorical_accuracy: 0.5076\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.47229 to 0.50756, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.2567 - categorical_accuracy: 0.5426 - val_loss: 1.3387 - val_categorical_accuracy: 0.5038\n","\n","Epoch 00004: val_categorical_accuracy did not improve from 0.50756\n","Epoch 5/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.2211 - categorical_accuracy: 0.5681 - val_loss: 1.3139 - val_categorical_accuracy: 0.5327\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.50756 to 0.53275, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 6/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.1773 - categorical_accuracy: 0.5803 - val_loss: 1.3032 - val_categorical_accuracy: 0.5126\n","\n","Epoch 00006: val_categorical_accuracy did not improve from 0.53275\n","Epoch 7/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.1620 - categorical_accuracy: 0.5991 - val_loss: 1.3409 - val_categorical_accuracy: 0.5290\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.53275\n","Epoch 8/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.1207 - categorical_accuracy: 0.6119 - val_loss: 1.2532 - val_categorical_accuracy: 0.5290\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.53275\n","Epoch 9/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0899 - categorical_accuracy: 0.6194 - val_loss: 1.2478 - val_categorical_accuracy: 0.5202\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.53275\n","Epoch 10/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0914 - categorical_accuracy: 0.6239 - val_loss: 1.3070 - val_categorical_accuracy: 0.5101\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.53275\n","Epoch 11/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0689 - categorical_accuracy: 0.6242 - val_loss: 1.3328 - val_categorical_accuracy: 0.4282\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.53275\n","Epoch 12/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0495 - categorical_accuracy: 0.6297 - val_loss: 1.3191 - val_categorical_accuracy: 0.5126\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.53275\n","Epoch 13/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0255 - categorical_accuracy: 0.6433 - val_loss: 1.2727 - val_categorical_accuracy: 0.5176\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.53275\n","Epoch 14/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0123 - categorical_accuracy: 0.6595 - val_loss: 1.2014 - val_categorical_accuracy: 0.5277\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.53275\n","Epoch 15/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9932 - categorical_accuracy: 0.6537 - val_loss: 1.2430 - val_categorical_accuracy: 0.5416\n","\n","Epoch 00015: val_categorical_accuracy improved from 0.53275 to 0.54156, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 16/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9908 - categorical_accuracy: 0.6601 - val_loss: 1.1813 - val_categorical_accuracy: 0.5504\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.54156 to 0.55038, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 17/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9928 - categorical_accuracy: 0.6563 - val_loss: 1.2322 - val_categorical_accuracy: 0.5139\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.55038\n","Epoch 18/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9878 - categorical_accuracy: 0.6561 - val_loss: 1.2370 - val_categorical_accuracy: 0.5202\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.55038\n","Epoch 19/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9484 - categorical_accuracy: 0.6764 - val_loss: 1.1915 - val_categorical_accuracy: 0.5491\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.55038\n","Epoch 20/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9461 - categorical_accuracy: 0.6679 - val_loss: 1.2035 - val_categorical_accuracy: 0.5378\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.55038\n","Epoch 21/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9322 - categorical_accuracy: 0.6690 - val_loss: 1.1758 - val_categorical_accuracy: 0.5390\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.55038\n","Epoch 22/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9190 - categorical_accuracy: 0.6785 - val_loss: 1.2263 - val_categorical_accuracy: 0.5340\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.55038\n","Epoch 23/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9191 - categorical_accuracy: 0.6844 - val_loss: 1.1574 - val_categorical_accuracy: 0.5378\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.55038\n","Epoch 24/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8938 - categorical_accuracy: 0.6886 - val_loss: 1.1585 - val_categorical_accuracy: 0.5428\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.55038\n","Epoch 25/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8958 - categorical_accuracy: 0.6840 - val_loss: 1.1791 - val_categorical_accuracy: 0.5655\n","\n","Epoch 00025: val_categorical_accuracy improved from 0.55038 to 0.56549, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 26/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9092 - categorical_accuracy: 0.6814 - val_loss: 1.2218 - val_categorical_accuracy: 0.5151\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.56549\n","Epoch 27/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8949 - categorical_accuracy: 0.6863 - val_loss: 1.1766 - val_categorical_accuracy: 0.5365\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.56549\n","Epoch 28/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8826 - categorical_accuracy: 0.6888 - val_loss: 1.1664 - val_categorical_accuracy: 0.5466\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.56549\n","Epoch 29/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8596 - categorical_accuracy: 0.6958 - val_loss: 1.1149 - val_categorical_accuracy: 0.5605\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.56549\n","Epoch 30/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8621 - categorical_accuracy: 0.6930 - val_loss: 1.1671 - val_categorical_accuracy: 0.5693\n","\n","Epoch 00030: val_categorical_accuracy improved from 0.56549 to 0.56927, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 31/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8543 - categorical_accuracy: 0.7002 - val_loss: 1.1777 - val_categorical_accuracy: 0.5390\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.56927\n","Epoch 32/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8434 - categorical_accuracy: 0.7090 - val_loss: 1.2109 - val_categorical_accuracy: 0.5088\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.56927\n","Epoch 33/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8276 - categorical_accuracy: 0.7211 - val_loss: 1.1655 - val_categorical_accuracy: 0.5592\n","\n","Epoch 00033: val_categorical_accuracy did not improve from 0.56927\n","Epoch 34/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8403 - categorical_accuracy: 0.7115 - val_loss: 1.1164 - val_categorical_accuracy: 0.5529\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.56927\n","Epoch 35/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8088 - categorical_accuracy: 0.7146 - val_loss: 1.1246 - val_categorical_accuracy: 0.5705\n","\n","Epoch 00035: val_categorical_accuracy improved from 0.56927 to 0.57053, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 36/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8454 - categorical_accuracy: 0.7044 - val_loss: 1.1491 - val_categorical_accuracy: 0.5491\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.57053\n","Epoch 37/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8219 - categorical_accuracy: 0.7184 - val_loss: 1.0666 - val_categorical_accuracy: 0.6045\n","\n","Epoch 00037: val_categorical_accuracy improved from 0.57053 to 0.60453, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_8//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 38/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8127 - categorical_accuracy: 0.7214 - val_loss: 1.1701 - val_categorical_accuracy: 0.5365\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.60453\n","Epoch 39/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8056 - categorical_accuracy: 0.7261 - val_loss: 1.0905 - val_categorical_accuracy: 0.5730\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.60453\n","Epoch 40/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8055 - categorical_accuracy: 0.7157 - val_loss: 1.1568 - val_categorical_accuracy: 0.5579\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.60453\n","Epoch 41/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7832 - categorical_accuracy: 0.7251 - val_loss: 1.1141 - val_categorical_accuracy: 0.5592\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.60453\n","Epoch 42/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7943 - categorical_accuracy: 0.7219 - val_loss: 1.1400 - val_categorical_accuracy: 0.5416\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.60453\n","Epoch 43/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7809 - categorical_accuracy: 0.7276 - val_loss: 1.1358 - val_categorical_accuracy: 0.5441\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.60453\n","Epoch 44/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7902 - categorical_accuracy: 0.7282 - val_loss: 1.0894 - val_categorical_accuracy: 0.5781\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.60453\n","Epoch 45/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7825 - categorical_accuracy: 0.7241 - val_loss: 1.1216 - val_categorical_accuracy: 0.5592\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.60453\n","Epoch 46/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7736 - categorical_accuracy: 0.7301 - val_loss: 1.1845 - val_categorical_accuracy: 0.5264\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.60453\n","Epoch 47/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7617 - categorical_accuracy: 0.7369 - val_loss: 1.1526 - val_categorical_accuracy: 0.5479\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.60453\n","Epoch 48/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7510 - categorical_accuracy: 0.7435 - val_loss: 1.0506 - val_categorical_accuracy: 0.5894\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.60453\n","Epoch 49/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7448 - categorical_accuracy: 0.7431 - val_loss: 1.1299 - val_categorical_accuracy: 0.5466\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.60453\n","Epoch 50/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7361 - categorical_accuracy: 0.7507 - val_loss: 1.0691 - val_categorical_accuracy: 0.5945\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.60453\n","25/25 [==============================] - 0s 953us/step - loss: 1.0666 - categorical_accuracy: 0.6045\n","Test accuracy on target dataset = 0.6045340299606323\n","25/25 [==============================] - 0s 711us/step\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VdgxRQRP0_mp"},"source":["# Transfer Learning results"]},{"cell_type":"code","metadata":{"id":"R_WJcBsGjtRA","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1615770811777,"user_tz":420,"elapsed":342,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"322b193c-0711-476f-af7a-732975767789"},"source":["transfer_results"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Overlap_ratio</th>\n","      <th>Training Samples</th>\n","      <th>Test Samples</th>\n","      <th>Training_Test_Split</th>\n","      <th>Test_Accuracies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>4740</td>\n","      <td>3166</td>\n","      <td>60</td>\n","      <td>55.12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>5927</td>\n","      <td>1979</td>\n","      <td>75</td>\n","      <td>57.25</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>7112</td>\n","      <td>794</td>\n","      <td>90</td>\n","      <td>60.45</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Overlap_ratio  Training Samples  ...  Training_Test_Split  Test_Accuracies\n","0              1              4740  ...                   60            55.12\n","1              1              5927  ...                   75            57.25\n","2              1              7112  ...                   90            60.45\n","\n","[3 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"1o6mCjHJ01PG"},"source":["# Classification accuracies per class for each model"]},{"cell_type":"code","metadata":{"id":"rhikZcccng2K"},"source":["for cm in confusion_matrixes:\n","  print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BFKA0crn0Gp"},"source":[""],"execution_count":null,"outputs":[]}]}