{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"SGCNN_7_pavia_to_indian_pines.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f_nG8NlV-lz-"},"source":["## Set up google colab environment"]},{"cell_type":"code","metadata":{"id":"9bq_kqWQtg0f"},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jK95udG-qHAy","executionInfo":{"status":"ok","timestamp":1615762373783,"user_tz":420,"elapsed":450,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["import os\n","os.chdir('/content/drive/My Drive/Hyperspectral_Image_Classification')"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vn-wRAH4t3WY","executionInfo":{"status":"ok","timestamp":1615762378659,"user_tz":420,"elapsed":2463,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["from SGCNN_7_Utils import *\n","import scipy.io as sio"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ky2Qzuw_qDdS"},"source":["## Load Pavia Dataset - Source"]},{"cell_type":"code","metadata":{"id":"I7pW15RM8nSf","executionInfo":{"status":"ok","timestamp":1615762381621,"user_tz":420,"elapsed":752,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["uPavia = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/PaviaU.mat')\n","gt_Pavia = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/PaviaU_gt.mat')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"svwF-yzh-l0N","executionInfo":{"status":"ok","timestamp":1615762382658,"user_tz":420,"elapsed":289,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["data_source = uPavia['paviaU']\n","ground_truth_source = gt_Pavia['paviaU_gt']"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHQe_Xhwza79","executionInfo":{"status":"ok","timestamp":1615762383274,"user_tz":420,"elapsed":291,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"7fbf6038-eacf-401b-ab7c-12dd9937eb7c"},"source":["data_source.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(610, 340, 103)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAjjOxj3qDdb","executionInfo":{"status":"ok","timestamp":1615762383826,"user_tz":420,"elapsed":369,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"cab5a3ac-ed22-4cc5-a530-2c3838a0aed3"},"source":["ground_truth_source.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(610, 340)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"wmLGB_VlWx6m"},"source":["# Load Indian Pines dataset - Target"]},{"cell_type":"code","metadata":{"id":"qu6T10joWpmQ","executionInfo":{"status":"ok","timestamp":1615762385803,"user_tz":420,"elapsed":259,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["uIndianPines = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/Indian_pines_corrected.mat')\n","gt_IndianPines = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/Datasets/Indian_pines_gt.mat')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"09gPGGX8W2Us","executionInfo":{"status":"ok","timestamp":1615762386324,"user_tz":420,"elapsed":232,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}}},"source":["data_target = uIndianPines['indian_pines_corrected']\n","ground_truth_target = gt_IndianPines['indian_pines_gt']"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qG_LxbHeXBVQ","executionInfo":{"status":"ok","timestamp":1615762386852,"user_tz":420,"elapsed":232,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"b5b82948-9483-46c0-d146-33ff97c1e9e5"},"source":["data_target.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(145, 145, 200)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYillUc_XDKC","executionInfo":{"status":"ok","timestamp":1615762387331,"user_tz":420,"elapsed":279,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"021b44c0-7bf5-46a8-dd1d-298526e5df26"},"source":["ground_truth_target.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(145, 145)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"5WxjgWNGqDdc"},"source":["## Distrubution of samples for each class in Source"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"yFA7eqA7qDdd","executionInfo":{"status":"ok","timestamp":1615762388299,"user_tz":420,"elapsed":290,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"77731365-2453-4346-c013-c25ca299a444"},"source":["class_distribution_source = pd.DataFrame(np.unique(ground_truth_source, return_counts = True))\n","class_distribution_source = class_distribution_source.transpose()\n","class_distribution_source.columns = ['class','samples']\n","class_distribution_source"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>164624</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>6631</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>18649</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2099</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3064</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>1345</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>5029</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>1330</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>3682</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>947</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   class  samples\n","0      0   164624\n","1      1     6631\n","2      2    18649\n","3      3     2099\n","4      4     3064\n","5      5     1345\n","6      6     5029\n","7      7     1330\n","8      8     3682\n","9      9      947"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"xK0U9fRXDwwJ"},"source":["## Drop background class"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ri9e6FYf-w4n","executionInfo":{"status":"ok","timestamp":1615762389419,"user_tz":420,"elapsed":270,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"0577d624-08fd-44ae-d2db-7e2ef1938100"},"source":["classes_source , counts_source = np.unique(ground_truth_source, return_counts = True)\n","classes_source = classes_source[1:] ## Dropping classes with background\n","classes_source"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"lwQmdin9Xmeg"},"source":["# Class distribution of samples in Target"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"id":"8EjIMOQHXU3h","executionInfo":{"status":"ok","timestamp":1615762390502,"user_tz":420,"elapsed":281,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"fa29164f-00b3-4989-9a35-3c21065542cc"},"source":["class_distribution_target = pd.DataFrame(np.unique(ground_truth_target, return_counts = True))\n","class_distribution_target = class_distribution_target.transpose()\n","class_distribution_target.columns = ['class','samples']\n","class_distribution_target"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>10776</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1428</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>830</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>237</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>483</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>730</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>28</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>478</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>972</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>2455</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>593</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>205</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>1265</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>386</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>93</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    class  samples\n","0       0    10776\n","1       1       46\n","2       2     1428\n","3       3      830\n","4       4      237\n","5       5      483\n","6       6      730\n","7       7       28\n","8       8      478\n","9       9       20\n","10     10      972\n","11     11     2455\n","12     12      593\n","13     13      205\n","14     14     1265\n","15     15      386\n","16     16       93"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"uSNioWW0EE9z"},"source":["## Dropping classes with small number of samples"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CgE2FPXbXtt9","executionInfo":{"status":"ok","timestamp":1615762392139,"user_tz":420,"elapsed":286,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"81f17548-2fcd-4c56-bc42-f751f9a29089"},"source":["classes_target , counts_target = np.unique(ground_truth_target, return_counts = True)\n","classes_target = classes_target[[2,3,5,6,8,10,11,12,14]] ## Dropping classes with small number of samples\n","classes_target"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2,  3,  5,  6,  8, 10, 11, 12, 14], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"l_PESscnqDde"},"source":["## Source : Pavia\n","\n","## Train model for samples extracted with different overlap ratios and a percent of  samples picked from each class to be present in the training set. \n","\n","## Model except the final fully connected layer is saved for transfer learning."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vc1sJwRqDdf","executionInfo":{"status":"ok","timestamp":1615765263279,"user_tz":420,"elapsed":2860965,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"f9a338c5-7805-490e-d59e-cf117ef39118"},"source":["pretrain_results = pretrain_source_models(percentages = [60,75,90],\n","                                          classes = classes_source,\n","                                          cube_size = 20,\n","                                          overlap_ratios = [1],\n","                                          data = data_source,\n","                                          ground_truth = ground_truth_source,\n","                                          batch_size = 20,\n","                                          channels = 64,\n","                                          epochs = 50,\n","                                          Verbosity = 1,\n","                                          accuracies = [],\n","                                          learning_rate = 0.0001,\n","                                          source_dataset = 'pavia')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 60 percent samples from each class in training set \n","==============================================================================================================\n","\n","Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n","Total number of samples is 37959.\n","\n","unique classes in training set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in training set is 22774.\n","Samples per class in training set: [3585 9037 1045 1712  807 3017  798 2209  564]\n","\n","unique classes in test set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in test set is 15185.\n","Samples per class in test set: [2390 6025  697 1142  538 2012  532 1473  376]\n","\n","X_train => (22774, 20, 20, 64)\n","X_test  => (15185, 20, 20, 64)\n","y_train => (22774, 9)\n","y_test  => (15185, 9)\n","\n","Group convolution output:  (None, 20, 20, 64)\n","Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 20, 20, 64)   4160        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 20, 20, 64)   256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 20, 20, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 20, 20, 8)    584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 20, 20, 8)    584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 20, 20, 8)    584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 20, 20, 8)    584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 20, 20, 8)    584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 20, 20, 8)    584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 20, 20, 8)    584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 20, 20, 8)    584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 20, 20, 8)    584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 20, 20, 8)    584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 20, 20, 8)    584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 20, 20, 8)    584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 20, 20, 8)    584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 20, 20, 8)    584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 20, 20, 8)    584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 20, 20, 8)    584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 20, 20, 64)   0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 20, 20, 64)   256         concatenate[0][0]                \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 20, 20, 64)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 20, 20, 8, 8) 0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 20, 20, 8, 8) 0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 20, 20, 64)   256         tf.reshape_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 20, 20, 64)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 20, 20, 128)  8320        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 20, 20, 128)  8320        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 20, 20, 128)  512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 20, 20, 128)  512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 20, 20, 128)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 20, 20, 128)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 20, 20, 128)  0           activation_3[0][0]               \n","                                                                 activation_4[0][0]               \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 20, 20, 128)  0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","final_fully_connected (Dense)   (None, 256)          33024       global_average_pooling2d[0][0]   \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 9)            2313        final_fully_connected[0][0]      \n","==================================================================================================\n","Total params: 67,273\n","Trainable params: 66,377\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Epoch 1/50\n","1139/1139 [==============================] - 64s 55ms/step - loss: 4.0818 - categorical_accuracy: 0.4063 - val_loss: 3.4540 - val_categorical_accuracy: 0.6227\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.62265, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","1139/1139 [==============================] - 63s 55ms/step - loss: 3.2849 - categorical_accuracy: 0.6855 - val_loss: 3.1207 - val_categorical_accuracy: 0.7016\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.62265 to 0.70161, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","1139/1139 [==============================] - 64s 56ms/step - loss: 2.9539 - categorical_accuracy: 0.7586 - val_loss: 2.9481 - val_categorical_accuracy: 0.6848\n","\n","Epoch 00003: val_categorical_accuracy did not improve from 0.70161\n","Epoch 4/50\n","1139/1139 [==============================] - 65s 57ms/step - loss: 2.7392 - categorical_accuracy: 0.7850 - val_loss: 2.8097 - val_categorical_accuracy: 0.6661\n","\n","Epoch 00004: val_categorical_accuracy did not improve from 0.70161\n","475/475 [==============================] - 7s 15ms/step - loss: 3.1207 - categorical_accuracy: 0.7016\n","Test Accuracy =  70.0\n","475/475 [==============================] - 7s 14ms/step\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 20, 20, 64)   4160        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 20, 20, 64)   256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 20, 20, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 20, 20, 8)    584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 20, 20, 8)    584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 20, 20, 8)    584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 20, 20, 8)    584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 20, 20, 8)    584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 20, 20, 8)    584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 20, 20, 8)    584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 20, 20, 8)    584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 20, 20, 8)    584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 20, 20, 8)    584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 20, 20, 8)    584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 20, 20, 8)    584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 20, 20, 8)    584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 20, 20, 8)    584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 20, 20, 8)    584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 20, 20, 8)    584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 20, 20, 64)   0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 20, 20, 64)   256         concatenate[0][0]                \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 20, 20, 64)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 20, 20, 8, 8) 0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 20, 20, 8, 8) 0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 20, 20, 64)   256         tf.reshape_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 20, 20, 64)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 20, 20, 128)  8320        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 20, 20, 128)  8320        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 20, 20, 128)  512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 20, 20, 128)  512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 20, 20, 128)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 20, 20, 128)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 20, 20, 128)  0           activation_3[0][0]               \n","                                                                 activation_4[0][0]               \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 20, 20, 128)  0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_5[0][0]               \n","==================================================================================================\n","Total params: 31,936\n","Trainable params: 31,040\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 75 percent samples from each class in training set \n","==============================================================================================================\n","\n","Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n","Total number of samples is 37959.\n","\n","unique classes in training set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in training set is 28465.\n","Samples per class in training set: [ 4481 11296  1306  2140  1008  3771   997  2761   705]\n","\n","unique classes in test set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in test set is 9494.\n","Samples per class in test set: [1494 3766  436  714  337 1258  333  921  235]\n","\n","X_train => (28465, 20, 20, 64)\n","X_test  => (9494, 20, 20, 64)\n","y_train => (28465, 9)\n","y_test  => (9494, 9)\n","\n","Group convolution output:  (None, 20, 20, 64)\n","Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 20, 20, 64)   4160        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 20, 20, 64)   256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 20, 20, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 20, 20, 8)    584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 20, 20, 8)    584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 20, 20, 8)    584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 20, 20, 8)    584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 20, 20, 8)    584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 20, 20, 8)    584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 20, 20, 8)    584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 20, 20, 8)    584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 20, 20, 8)    584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 20, 20, 8)    584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 20, 20, 8)    584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 20, 20, 8)    584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 20, 20, 8)    584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 20, 20, 8)    584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 20, 20, 8)    584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 20, 20, 8)    584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 20, 20, 64)   0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 20, 20, 64)   256         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 20, 20, 64)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 20, 20, 8, 8) 0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 20, 20, 8, 8) 0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 20, 20, 64)   256         tf.reshape_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 20, 20, 64)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 20, 20, 128)  8320        activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 20, 20, 128)  8320        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 20, 20, 128)  512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 20, 20, 128)  512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 20, 20, 128)  0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 20, 20, 128)  0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 20, 20, 128)  0           activation_9[0][0]               \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 20, 20, 128)  0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_11[0][0]              \n","__________________________________________________________________________________________________\n","final_fully_connected (Dense)   (None, 256)          33024       global_average_pooling2d_1[0][0] \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 9)            2313        final_fully_connected[0][0]      \n","==================================================================================================\n","Total params: 67,273\n","Trainable params: 66,377\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Epoch 1/50\n","1424/1424 [==============================] - 78s 54ms/step - loss: 3.9354 - categorical_accuracy: 0.4703 - val_loss: 3.2965 - val_categorical_accuracy: 0.6309\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.63092, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","1424/1424 [==============================] - 81s 57ms/step - loss: 3.1581 - categorical_accuracy: 0.6864 - val_loss: 2.9542 - val_categorical_accuracy: 0.7508\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.63092 to 0.75079, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","1424/1424 [==============================] - 76s 54ms/step - loss: 2.8659 - categorical_accuracy: 0.7409 - val_loss: 2.7412 - val_categorical_accuracy: 0.7843\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.75079 to 0.78428, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","1424/1424 [==============================] - 76s 53ms/step - loss: 2.6353 - categorical_accuracy: 0.7787 - val_loss: 2.5698 - val_categorical_accuracy: 0.7879\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.78428 to 0.78787, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 5/50\n","1424/1424 [==============================] - 76s 54ms/step - loss: 2.4470 - categorical_accuracy: 0.8093 - val_loss: 2.3978 - val_categorical_accuracy: 0.7993\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.78787 to 0.79935, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 6/50\n","1424/1424 [==============================] - 80s 56ms/step - loss: 2.2835 - categorical_accuracy: 0.8325 - val_loss: 2.2576 - val_categorical_accuracy: 0.8049\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.79935 to 0.80493, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 7/50\n","1424/1424 [==============================] - 77s 54ms/step - loss: 2.1384 - categorical_accuracy: 0.8541 - val_loss: 2.1535 - val_categorical_accuracy: 0.8083\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.80493 to 0.80830, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 8/50\n","1424/1424 [==============================] - 78s 55ms/step - loss: 2.0166 - categorical_accuracy: 0.8682 - val_loss: 2.0178 - val_categorical_accuracy: 0.8176\n","\n","Epoch 00008: val_categorical_accuracy improved from 0.80830 to 0.81757, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 9/50\n","1424/1424 [==============================] - 77s 54ms/step - loss: 1.8923 - categorical_accuracy: 0.8861 - val_loss: 1.9059 - val_categorical_accuracy: 0.8249\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.81757 to 0.82494, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 10/50\n","1424/1424 [==============================] - 81s 57ms/step - loss: 1.7770 - categorical_accuracy: 0.8999 - val_loss: 1.8077 - val_categorical_accuracy: 0.8385\n","\n","Epoch 00010: val_categorical_accuracy improved from 0.82494 to 0.83853, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 11/50\n","1424/1424 [==============================] - 77s 54ms/step - loss: 1.6859 - categorical_accuracy: 0.9082 - val_loss: 1.7163 - val_categorical_accuracy: 0.8369\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.83853\n","Epoch 12/50\n","1424/1424 [==============================] - 76s 54ms/step - loss: 1.5901 - categorical_accuracy: 0.9199 - val_loss: 1.6628 - val_categorical_accuracy: 0.8408\n","\n","Epoch 00012: val_categorical_accuracy improved from 0.83853 to 0.84085, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 13/50\n","1424/1424 [==============================] - 77s 54ms/step - loss: 1.4960 - categorical_accuracy: 0.9294 - val_loss: 1.8251 - val_categorical_accuracy: 0.7851\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.84085\n","Epoch 14/50\n","1424/1424 [==============================] - 80s 56ms/step - loss: 1.4183 - categorical_accuracy: 0.9375 - val_loss: 1.6028 - val_categorical_accuracy: 0.8242\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.84085\n","297/297 [==============================] - 5s 15ms/step - loss: 1.6628 - categorical_accuracy: 0.8408\n","Test Accuracy =  84.0\n","297/297 [==============================] - 5s 14ms/step\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 20, 20, 64)   4160        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 20, 20, 64)   256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 20, 20, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 20, 20, 8)    584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 20, 20, 8)    584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 20, 20, 8)    584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 20, 20, 8)    584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 20, 20, 8)    584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 20, 20, 8)    584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 20, 20, 8)    584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 20, 20, 8)    584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 20, 20, 8)    584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 20, 20, 8)    584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 20, 20, 8)    584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 20, 20, 8)    584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 20, 20, 8)    584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 20, 20, 8)    584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 20, 20, 8)    584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 20, 20, 8)    584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 20, 20, 64)   0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 20, 20, 64)   256         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 20, 20, 64)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 20, 20, 8, 8) 0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 20, 20, 8, 8) 0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 20, 20, 64)   256         tf.reshape_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 20, 20, 64)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 20, 20, 128)  8320        activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 20, 20, 128)  8320        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 20, 20, 128)  512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 20, 20, 128)  512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 20, 20, 128)  0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 20, 20, 128)  0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 20, 20, 128)  0           activation_9[0][0]               \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 20, 20, 128)  0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_11[0][0]              \n","==================================================================================================\n","Total params: 31,936\n","Trainable params: 31,040\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 90 percent samples from each class in training set \n","==============================================================================================================\n","\n","Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n","Total number of samples is 37959.\n","\n","unique classes in training set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in training set is 34159.\n","Samples per class in training set: [ 5377 13555  1567  2568  1210  4526  1197  3313   846]\n","\n","unique classes in test set: [1 2 3 4 5 6 7 8 9]\n","Total number of samples in test set is 3800.\n","Samples per class in test set: [ 598 1507  175  286  135  503  133  369   94]\n","\n","X_train => (34159, 20, 20, 64)\n","X_test  => (3800, 20, 20, 64)\n","y_train => (34159, 9)\n","y_test  => (3800, 9)\n","\n","Group convolution output:  (None, 20, 20, 64)\n","Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 20, 20, 64)   4160        input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 20, 20, 64)   256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 20, 20, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 20, 20, 8)    584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 20, 20, 8)    584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 20, 20, 8)    584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 20, 20, 8)    584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 20, 20, 8)    584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 20, 20, 8)    584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 20, 20, 8)    584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 20, 20, 8)    584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 20, 20, 8)    584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 20, 20, 8)    584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 20, 20, 8)    584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 20, 20, 8)    584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 20, 20, 8)    584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 20, 20, 8)    584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 20, 20, 8)    584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 20, 20, 8)    584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 20, 20, 64)   0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 20, 20, 64)   256         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 20, 20, 64)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 20, 20, 8, 8) 0           activation_13[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 20, 20, 8, 8) 0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 20, 20, 64)   256         tf.reshape_5[0][0]               \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 20, 20, 64)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 20, 20, 128)  8320        activation_14[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 20, 20, 128)  8320        input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 20, 20, 128)  512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 20, 20, 128)  512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 20, 20, 128)  0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 20, 20, 128)  0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 20, 20, 128)  0           activation_15[0][0]              \n","                                                                 activation_16[0][0]              \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 20, 20, 128)  0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_17[0][0]              \n","__________________________________________________________________________________________________\n","final_fully_connected (Dense)   (None, 256)          33024       global_average_pooling2d_2[0][0] \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 9)            2313        final_fully_connected[0][0]      \n","==================================================================================================\n","Total params: 67,273\n","Trainable params: 66,377\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Epoch 1/50\n","1708/1708 [==============================] - 99s 57ms/step - loss: 3.8585 - categorical_accuracy: 0.4860 - val_loss: 3.1367 - val_categorical_accuracy: 0.6961\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.69605, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","1708/1708 [==============================] - 96s 56ms/step - loss: 3.0958 - categorical_accuracy: 0.6965 - val_loss: 2.7411 - val_categorical_accuracy: 0.8226\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.69605 to 0.82263, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","1708/1708 [==============================] - 95s 56ms/step - loss: 2.7600 - categorical_accuracy: 0.7516 - val_loss: 2.5336 - val_categorical_accuracy: 0.8145\n","\n","Epoch 00003: val_categorical_accuracy did not improve from 0.82263\n","Epoch 4/50\n","1708/1708 [==============================] - 96s 56ms/step - loss: 2.5165 - categorical_accuracy: 0.7813 - val_loss: 2.3326 - val_categorical_accuracy: 0.8355\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.82263 to 0.83553, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 5/50\n","1708/1708 [==============================] - 89s 52ms/step - loss: 2.3170 - categorical_accuracy: 0.8064 - val_loss: 2.1163 - val_categorical_accuracy: 0.8834\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.83553 to 0.88342, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 6/50\n","1708/1708 [==============================] - 89s 52ms/step - loss: 2.1453 - categorical_accuracy: 0.8309 - val_loss: 2.0112 - val_categorical_accuracy: 0.8897\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.88342 to 0.88974, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 7/50\n","1708/1708 [==============================] - 90s 53ms/step - loss: 1.9955 - categorical_accuracy: 0.8541 - val_loss: 1.8340 - val_categorical_accuracy: 0.9342\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.88974 to 0.93421, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//full_models/pavia_as_source_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 8/50\n","1708/1708 [==============================] - 92s 54ms/step - loss: 1.8609 - categorical_accuracy: 0.8683 - val_loss: 1.7812 - val_categorical_accuracy: 0.8821\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.93421\n","Epoch 9/50\n","1708/1708 [==============================] - 89s 52ms/step - loss: 1.7424 - categorical_accuracy: 0.8863 - val_loss: 1.6927 - val_categorical_accuracy: 0.8750\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.93421\n","119/119 [==============================] - 2s 15ms/step - loss: 1.8340 - categorical_accuracy: 0.9342\n","Test Accuracy =  93.0\n","119/119 [==============================] - 2s 14ms/step\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 20, 20, 64)   4160        input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 20, 20, 64)   256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 20, 20, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 20, 20, 8)    584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 20, 20, 8)    584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 20, 20, 8)    584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 20, 20, 8)    584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 20, 20, 8)    584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 20, 20, 8)    584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 20, 20, 8)    584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 20, 20, 8)    584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 20, 20, 8)    584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 20, 20, 8)    584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 20, 20, 8)    584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 20, 20, 8)    584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 20, 20, 8)    584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 20, 20, 8)    584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 20, 20, 8)    584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 20, 20, 8)    584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 20, 20, 64)   0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 20, 20, 64)   256         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 20, 20, 64)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 20, 20, 8, 8) 0           activation_13[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 20, 20, 8, 8) 0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 20, 20, 64)   256         tf.reshape_5[0][0]               \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 20, 20, 64)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 20, 20, 128)  8320        activation_14[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 20, 20, 128)  8320        input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 20, 20, 128)  512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 20, 20, 128)  512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 20, 20, 128)  0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 20, 20, 128)  0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 20, 20, 128)  0           activation_15[0][0]              \n","                                                                 activation_16[0][0]              \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 20, 20, 128)  0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_17[0][0]              \n","==================================================================================================\n","Total params: 31,936\n","Trainable params: 31,040\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dCxOwh-vpTVi","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1615765674878,"user_tz":420,"elapsed":318,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"2b291b37-62eb-4912-acc1-9d56071244c5"},"source":["pretrain_results"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Overlap_ratio</th>\n","      <th>Training Samples</th>\n","      <th>Test Samples</th>\n","      <th>Training_Test_Split</th>\n","      <th>Test_Accuracies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>22774</td>\n","      <td>15185</td>\n","      <td>60</td>\n","      <td>70.16</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>28465</td>\n","      <td>9494</td>\n","      <td>75</td>\n","      <td>84.08</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>34159</td>\n","      <td>3800</td>\n","      <td>90</td>\n","      <td>93.42</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Overlap_ratio  Training Samples  ...  Training_Test_Split  Test_Accuracies\n","0              1             22774  ...                   60            70.16\n","1              1             28465  ...                   75            84.08\n","2              1             34159  ...                   90            93.42\n","\n","[3 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"RkxgCjIejh0l"},"source":["# Fine tune on Indian Pines"]},{"cell_type":"code","metadata":{"id":"SJ1kSkHtCuHd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615765843174,"user_tz":420,"elapsed":107301,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"9c2748d9-54e4-4c06-e976-a6aa569e02da"},"source":["transfer_results, confusion_matrixes = transfer_learning(percentages = [60,75,90],\n","                                                        source_dataset = 'pavia',\n","                                                        target_dataset = 'indian_pines',\n","                                                        data = data_target,\n","                                                        ground_truth = ground_truth_target,\n","                                                        classes = classes_target,\n","                                                        overlap_ratios = [1],\n","                                                        channels = 64,\n","                                                        cube_size = 20,\n","                                                        learning_rate = 0.0001,\n","                                                        epochs = 50,\n","                                                        batch_size = 20)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 60 percent samples from each class in training set \n","==============================================================================================================\n","\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 20, 20, 64)   4160        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 20, 20, 64)   256         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 20, 20, 64)   0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_5 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","lambda_7 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 20, 20, 8)    584         lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_6 (Conv2D)               (None, 20, 20, 8)    584         lambda_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_10 (Conv2D)              (None, 20, 20, 8)    584         lambda_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_14 (Conv2D)              (None, 20, 20, 8)    584         lambda_7[0][0]                   \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 20, 20, 8)    584         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_7 (Conv2D)               (None, 20, 20, 8)    584         conv2d_6[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_4 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_11 (Conv2D)              (None, 20, 20, 8)    584         conv2d_10[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_6 (Lambda)               (None, 20, 20, 8)    0           activation[0][0]                 \n","__________________________________________________________________________________________________\n","conv2d_15 (Conv2D)              (None, 20, 20, 8)    584         conv2d_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 20, 20, 8)    584         lambda[0][0]                     \n","__________________________________________________________________________________________________\n","conv2d_4 (Conv2D)               (None, 20, 20, 8)    584         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_5 (Conv2D)               (None, 20, 20, 8)    584         lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_8 (Conv2D)               (None, 20, 20, 8)    584         conv2d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_9 (Conv2D)               (None, 20, 20, 8)    584         lambda_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_12 (Conv2D)              (None, 20, 20, 8)    584         conv2d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_13 (Conv2D)              (None, 20, 20, 8)    584         lambda_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_16 (Conv2D)              (None, 20, 20, 8)    584         conv2d_15[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 20, 20, 64)   0           conv2d_1[0][0]                   \n","                                                                 conv2d_4[0][0]                   \n","                                                                 conv2d_5[0][0]                   \n","                                                                 conv2d_8[0][0]                   \n","                                                                 conv2d_9[0][0]                   \n","                                                                 conv2d_12[0][0]                  \n","                                                                 conv2d_13[0][0]                  \n","                                                                 conv2d_16[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 20, 20, 64)   256         concatenate[0][0]                \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 20, 20, 64)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape (TFOpLambda)         (None, 20, 20, 8, 8) 0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose (TFOpLam (None, 20, 20, 8, 8) 0           tf.reshape[0][0]                 \n","__________________________________________________________________________________________________\n","tf.reverse (TFOpLambda)         (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_1 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 20, 20, 64)   256         tf.reshape_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 20, 20, 64)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_17 (Conv2D)              (None, 20, 20, 128)  8320        activation_2[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_18 (Conv2D)              (None, 20, 20, 128)  8320        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 20, 20, 128)  512         conv2d_17[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 20, 20, 128)  512         conv2d_18[0][0]                  \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 20, 20, 128)  0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 20, 20, 128)  0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 20, 20, 128)  0           activation_3[0][0]               \n","                                                                 activation_4[0][0]               \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 20, 20, 128)  0           add[0][0]                        \n","__________________________________________________________________________________________________\n","global_average_pooling2d (Globa (None, 128)          0           activation_5[0][0]               \n","==================================================================================================\n","Total params: 31,936\n","Trainable params: 31,040\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n","Total number of samples is 7906.\n","\n","unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in training set is 4740.\n","Samples per class in training set: [ 820  313  193  438  213  502 1334  276  651]\n","\n","unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in test set is 3166.\n","Samples per class in test set: [548 210 130 292 143 335 890 184 434]\n","\n","X_train_transfer => (4740, 128)\n","X_test_transfer  => (3166, 128)\n","y_train => (4740, 9)\n","y_test  => (3166, 9)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc9 (Dense)                  (None, 9)                 2313      \n","=================================================================\n","Total params: 35,337\n","Trainable params: 35,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","237/237 [==============================] - 1s 2ms/step - loss: 2.0783 - categorical_accuracy: 0.3531 - val_loss: 1.4585 - val_categorical_accuracy: 0.4103\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.41030, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.4349 - categorical_accuracy: 0.4554 - val_loss: 1.3932 - val_categorical_accuracy: 0.4425\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.41030 to 0.44251, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.3587 - categorical_accuracy: 0.4822 - val_loss: 1.3926 - val_categorical_accuracy: 0.4662\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.44251 to 0.46620, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.2881 - categorical_accuracy: 0.5195 - val_loss: 1.3789 - val_categorical_accuracy: 0.4605\n","\n","Epoch 00004: val_categorical_accuracy did not improve from 0.46620\n","Epoch 5/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.2468 - categorical_accuracy: 0.5389 - val_loss: 1.3583 - val_categorical_accuracy: 0.4804\n","\n","Epoch 00005: val_categorical_accuracy improved from 0.46620 to 0.48042, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 6/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.2066 - categorical_accuracy: 0.5722 - val_loss: 1.2472 - val_categorical_accuracy: 0.5556\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.48042 to 0.55559, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 7/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1628 - categorical_accuracy: 0.5870 - val_loss: 1.3188 - val_categorical_accuracy: 0.4924\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.55559\n","Epoch 8/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1641 - categorical_accuracy: 0.5804 - val_loss: 1.2825 - val_categorical_accuracy: 0.5335\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.55559\n","Epoch 9/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1380 - categorical_accuracy: 0.6002 - val_loss: 1.2535 - val_categorical_accuracy: 0.4915\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.55559\n","Epoch 10/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.1000 - categorical_accuracy: 0.6210 - val_loss: 1.2876 - val_categorical_accuracy: 0.4889\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.55559\n","Epoch 11/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0881 - categorical_accuracy: 0.6202 - val_loss: 1.3147 - val_categorical_accuracy: 0.4836\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.55559\n","Epoch 12/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0820 - categorical_accuracy: 0.6272 - val_loss: 1.3520 - val_categorical_accuracy: 0.4968\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.55559\n","Epoch 13/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0481 - categorical_accuracy: 0.6408 - val_loss: 1.2628 - val_categorical_accuracy: 0.5199\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.55559\n","Epoch 14/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0192 - categorical_accuracy: 0.6676 - val_loss: 1.3060 - val_categorical_accuracy: 0.5038\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.55559\n","Epoch 15/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0178 - categorical_accuracy: 0.6611 - val_loss: 1.1751 - val_categorical_accuracy: 0.5521\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.55559\n","Epoch 16/50\n","237/237 [==============================] - 0s 2ms/step - loss: 1.0004 - categorical_accuracy: 0.6641 - val_loss: 1.2680 - val_categorical_accuracy: 0.5227\n","\n","Epoch 00016: val_categorical_accuracy did not improve from 0.55559\n","Epoch 17/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9755 - categorical_accuracy: 0.6750 - val_loss: 1.2475 - val_categorical_accuracy: 0.5313\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.55559\n","Epoch 18/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9617 - categorical_accuracy: 0.6875 - val_loss: 1.1891 - val_categorical_accuracy: 0.5467\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.55559\n","Epoch 19/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9599 - categorical_accuracy: 0.6871 - val_loss: 1.3067 - val_categorical_accuracy: 0.5294\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.55559\n","Epoch 20/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9314 - categorical_accuracy: 0.6956 - val_loss: 1.2574 - val_categorical_accuracy: 0.5253\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.55559\n","Epoch 21/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9238 - categorical_accuracy: 0.6951 - val_loss: 1.3016 - val_categorical_accuracy: 0.5578\n","\n","Epoch 00021: val_categorical_accuracy improved from 0.55559 to 0.55780, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 22/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8890 - categorical_accuracy: 0.7094 - val_loss: 1.2861 - val_categorical_accuracy: 0.4940\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.55780\n","Epoch 23/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8893 - categorical_accuracy: 0.7169 - val_loss: 1.2089 - val_categorical_accuracy: 0.5436\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.55780\n","Epoch 24/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.9010 - categorical_accuracy: 0.7037 - val_loss: 1.2790 - val_categorical_accuracy: 0.5357\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.55780\n","Epoch 25/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8747 - categorical_accuracy: 0.7108 - val_loss: 1.2127 - val_categorical_accuracy: 0.5360\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.55780\n","Epoch 26/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8693 - categorical_accuracy: 0.7101 - val_loss: 1.1614 - val_categorical_accuracy: 0.5594\n","\n","Epoch 00026: val_categorical_accuracy improved from 0.55780 to 0.55938, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 27/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8656 - categorical_accuracy: 0.7148 - val_loss: 1.2850 - val_categorical_accuracy: 0.5373\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.55938\n","Epoch 28/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8688 - categorical_accuracy: 0.7153 - val_loss: 1.1812 - val_categorical_accuracy: 0.5720\n","\n","Epoch 00028: val_categorical_accuracy improved from 0.55938 to 0.57202, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 29/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8327 - categorical_accuracy: 0.7305 - val_loss: 1.1780 - val_categorical_accuracy: 0.5527\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.57202\n","Epoch 30/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8264 - categorical_accuracy: 0.7241 - val_loss: 1.2049 - val_categorical_accuracy: 0.5701\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.57202\n","Epoch 31/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8181 - categorical_accuracy: 0.7349 - val_loss: 1.1733 - val_categorical_accuracy: 0.5635\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.57202\n","Epoch 32/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8015 - categorical_accuracy: 0.7399 - val_loss: 1.2457 - val_categorical_accuracy: 0.5780\n","\n","Epoch 00032: val_categorical_accuracy improved from 0.57202 to 0.57802, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 33/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.8065 - categorical_accuracy: 0.7367 - val_loss: 1.1488 - val_categorical_accuracy: 0.5881\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.57802 to 0.58812, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 34/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7849 - categorical_accuracy: 0.7399 - val_loss: 1.1698 - val_categorical_accuracy: 0.5859\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.58812\n","Epoch 35/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7798 - categorical_accuracy: 0.7516 - val_loss: 1.2552 - val_categorical_accuracy: 0.5524\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.58812\n","Epoch 36/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7750 - categorical_accuracy: 0.7566 - val_loss: 1.1680 - val_categorical_accuracy: 0.5875\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.58812\n","Epoch 37/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7671 - categorical_accuracy: 0.7523 - val_loss: 1.1942 - val_categorical_accuracy: 0.5862\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.58812\n","Epoch 38/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7531 - categorical_accuracy: 0.7560 - val_loss: 1.1601 - val_categorical_accuracy: 0.5843\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.58812\n","Epoch 39/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7722 - categorical_accuracy: 0.7529 - val_loss: 1.2193 - val_categorical_accuracy: 0.5752\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.58812\n","Epoch 40/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7322 - categorical_accuracy: 0.7623 - val_loss: 1.2289 - val_categorical_accuracy: 0.5761\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.58812\n","Epoch 41/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7315 - categorical_accuracy: 0.7645 - val_loss: 1.1816 - val_categorical_accuracy: 0.5944\n","\n","Epoch 00041: val_categorical_accuracy improved from 0.58812 to 0.59444, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 42/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7037 - categorical_accuracy: 0.7765 - val_loss: 1.2435 - val_categorical_accuracy: 0.5840\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.59444\n","Epoch 43/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7096 - categorical_accuracy: 0.7717 - val_loss: 1.1616 - val_categorical_accuracy: 0.5884\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.59444\n","Epoch 44/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.7144 - categorical_accuracy: 0.7786 - val_loss: 1.1954 - val_categorical_accuracy: 0.5900\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.59444\n","Epoch 45/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6976 - categorical_accuracy: 0.7832 - val_loss: 1.1503 - val_categorical_accuracy: 0.6011\n","\n","Epoch 00045: val_categorical_accuracy improved from 0.59444 to 0.60107, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 46/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6919 - categorical_accuracy: 0.7780 - val_loss: 1.1949 - val_categorical_accuracy: 0.6061\n","\n","Epoch 00046: val_categorical_accuracy improved from 0.60107 to 0.60613, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 47/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6789 - categorical_accuracy: 0.7914 - val_loss: 1.2668 - val_categorical_accuracy: 0.5831\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.60613\n","Epoch 48/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6602 - categorical_accuracy: 0.7907 - val_loss: 1.1660 - val_categorical_accuracy: 0.5900\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.60613\n","Epoch 49/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6778 - categorical_accuracy: 0.7825 - val_loss: 1.1877 - val_categorical_accuracy: 0.6099\n","\n","Epoch 00049: val_categorical_accuracy improved from 0.60613 to 0.60992, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_60_samples_from_each_class_in_training_set.h5\n","Epoch 50/50\n","237/237 [==============================] - 0s 2ms/step - loss: 0.6724 - categorical_accuracy: 0.7985 - val_loss: 1.2348 - val_categorical_accuracy: 0.5910\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.60992\n","99/99 [==============================] - 0s 872us/step - loss: 1.1877 - categorical_accuracy: 0.6099\n","Test accuracy on target dataset = 0.6099178791046143\n","99/99 [==============================] - 0s 699us/step\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 75 percent samples from each class in training set \n","==============================================================================================================\n","\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d_19 (Conv2D)              (None, 20, 20, 64)   4160        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 20, 20, 64)   256         conv2d_19[0][0]                  \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 20, 20, 64)   0           batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","lambda_9 (Lambda)               (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_11 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_13 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","lambda_15 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 20, 20, 8)    584         lambda_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 20, 20, 8)    584         lambda_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 20, 20, 8)    584         lambda_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 20, 20, 8)    584         lambda_15[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_8 (Lambda)               (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_22 (Conv2D)              (None, 20, 20, 8)    584         conv2d_21[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_10 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 20, 20, 8)    584         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_12 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 20, 20, 8)    584         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_14 (Lambda)              (None, 20, 20, 8)    0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 20, 20, 8)    584         conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_20 (Conv2D)              (None, 20, 20, 8)    584         lambda_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_23 (Conv2D)              (None, 20, 20, 8)    584         conv2d_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_24 (Conv2D)              (None, 20, 20, 8)    584         lambda_10[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 20, 20, 8)    584         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 20, 20, 8)    584         lambda_12[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 20, 20, 8)    584         conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 20, 20, 8)    584         lambda_14[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 20, 20, 8)    584         conv2d_34[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 20, 20, 64)   0           conv2d_20[0][0]                  \n","                                                                 conv2d_23[0][0]                  \n","                                                                 conv2d_24[0][0]                  \n","                                                                 conv2d_27[0][0]                  \n","                                                                 conv2d_28[0][0]                  \n","                                                                 conv2d_31[0][0]                  \n","                                                                 conv2d_32[0][0]                  \n","                                                                 conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 20, 20, 64)   256         concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 20, 20, 64)   0           batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","tf.reshape_2 (TFOpLambda)       (None, 20, 20, 8, 8) 0           activation_7[0][0]               \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_1 (TFOpL (None, 20, 20, 8, 8) 0           tf.reshape_2[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_1 (TFOpLambda)       (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose_1[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 20, 20, 64)   256         tf.reshape_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 20, 20, 64)   0           batch_normalization_7[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 20, 20, 128)  8320        activation_8[0][0]               \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 20, 20, 128)  8320        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 20, 20, 128)  512         conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 20, 20, 128)  512         conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 20, 20, 128)  0           batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 20, 20, 128)  0           batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 20, 20, 128)  0           activation_9[0][0]               \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 20, 20, 128)  0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_1 (Glo (None, 128)          0           activation_11[0][0]              \n","==================================================================================================\n","Total params: 31,936\n","Trainable params: 31,040\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n","Total number of samples is 7906.\n","\n","unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in training set is 5927.\n","Samples per class in training set: [1026  392  242  547  267  627 1668  345  813]\n","\n","unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in test set is 1979.\n","Samples per class in test set: [342 131  81 183  89 210 556 115 272]\n","\n","X_train_transfer => (5927, 128)\n","X_test_transfer  => (1979, 128)\n","y_train => (5927, 9)\n","y_test  => (1979, 9)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_5 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc9 (Dense)                  (None, 9)                 2313      \n","=================================================================\n","Total params: 35,337\n","Trainable params: 35,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","297/297 [==============================] - 1s 2ms/step - loss: 2.0595 - categorical_accuracy: 0.3417 - val_loss: 1.4801 - val_categorical_accuracy: 0.4265\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.42648, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.3645 - categorical_accuracy: 0.4735 - val_loss: 1.3896 - val_categorical_accuracy: 0.4901\n","\n","Epoch 00002: val_categorical_accuracy improved from 0.42648 to 0.49015, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 3/50\n","297/297 [==============================] - 0s 1ms/step - loss: 1.2331 - categorical_accuracy: 0.5450 - val_loss: 1.3077 - val_categorical_accuracy: 0.5028\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.49015 to 0.50278, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.1916 - categorical_accuracy: 0.5665 - val_loss: 1.3263 - val_categorical_accuracy: 0.5442\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.50278 to 0.54421, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 5/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.1348 - categorical_accuracy: 0.6022 - val_loss: 1.3232 - val_categorical_accuracy: 0.5392\n","\n","Epoch 00005: val_categorical_accuracy did not improve from 0.54421\n","Epoch 6/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.0974 - categorical_accuracy: 0.6090 - val_loss: 1.3089 - val_categorical_accuracy: 0.5589\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.54421 to 0.55887, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 7/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.0817 - categorical_accuracy: 0.6262 - val_loss: 1.4036 - val_categorical_accuracy: 0.5639\n","\n","Epoch 00007: val_categorical_accuracy improved from 0.55887 to 0.56392, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 8/50\n","297/297 [==============================] - 0s 2ms/step - loss: 1.0402 - categorical_accuracy: 0.6371 - val_loss: 1.4035 - val_categorical_accuracy: 0.5129\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.56392\n","Epoch 9/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9958 - categorical_accuracy: 0.6659 - val_loss: 1.4891 - val_categorical_accuracy: 0.5387\n","\n","Epoch 00009: val_categorical_accuracy did not improve from 0.56392\n","Epoch 10/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9922 - categorical_accuracy: 0.6514 - val_loss: 1.2613 - val_categorical_accuracy: 0.5629\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.56392\n","Epoch 11/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.9485 - categorical_accuracy: 0.6868 - val_loss: 1.2993 - val_categorical_accuracy: 0.5700\n","\n","Epoch 00011: val_categorical_accuracy improved from 0.56392 to 0.56998, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 12/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9364 - categorical_accuracy: 0.6827 - val_loss: 1.2351 - val_categorical_accuracy: 0.5538\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.56998\n","Epoch 13/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9349 - categorical_accuracy: 0.6874 - val_loss: 1.2702 - val_categorical_accuracy: 0.5614\n","\n","Epoch 00013: val_categorical_accuracy did not improve from 0.56998\n","Epoch 14/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9186 - categorical_accuracy: 0.6967 - val_loss: 1.1955 - val_categorical_accuracy: 0.5664\n","\n","Epoch 00014: val_categorical_accuracy did not improve from 0.56998\n","Epoch 15/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.9002 - categorical_accuracy: 0.7069 - val_loss: 1.2052 - val_categorical_accuracy: 0.5568\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.56998\n","Epoch 16/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8667 - categorical_accuracy: 0.7095 - val_loss: 1.2359 - val_categorical_accuracy: 0.5781\n","\n","Epoch 00016: val_categorical_accuracy improved from 0.56998 to 0.57807, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 17/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8700 - categorical_accuracy: 0.7136 - val_loss: 1.1786 - val_categorical_accuracy: 0.5609\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.57807\n","Epoch 18/50\n","297/297 [==============================] - 0s 1ms/step - loss: 0.8455 - categorical_accuracy: 0.7265 - val_loss: 1.2178 - val_categorical_accuracy: 0.5902\n","\n","Epoch 00018: val_categorical_accuracy improved from 0.57807 to 0.59020, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 19/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8321 - categorical_accuracy: 0.7327 - val_loss: 1.2061 - val_categorical_accuracy: 0.5523\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.59020\n","Epoch 20/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8053 - categorical_accuracy: 0.7388 - val_loss: 1.2796 - val_categorical_accuracy: 0.5715\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.59020\n","Epoch 21/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.8081 - categorical_accuracy: 0.7379 - val_loss: 1.2374 - val_categorical_accuracy: 0.5856\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.59020\n","Epoch 22/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7898 - categorical_accuracy: 0.7545 - val_loss: 1.2421 - val_categorical_accuracy: 0.5846\n","\n","Epoch 00022: val_categorical_accuracy did not improve from 0.59020\n","Epoch 23/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7790 - categorical_accuracy: 0.7532 - val_loss: 1.2308 - val_categorical_accuracy: 0.5755\n","\n","Epoch 00023: val_categorical_accuracy did not improve from 0.59020\n","Epoch 24/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7801 - categorical_accuracy: 0.7517 - val_loss: 1.1687 - val_categorical_accuracy: 0.5776\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.59020\n","Epoch 25/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7859 - categorical_accuracy: 0.7510 - val_loss: 1.1999 - val_categorical_accuracy: 0.5796\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.59020\n","Epoch 26/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7497 - categorical_accuracy: 0.7720 - val_loss: 1.2203 - val_categorical_accuracy: 0.5766\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.59020\n","Epoch 27/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7439 - categorical_accuracy: 0.7729 - val_loss: 1.2190 - val_categorical_accuracy: 0.5958\n","\n","Epoch 00027: val_categorical_accuracy improved from 0.59020 to 0.59576, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 28/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7290 - categorical_accuracy: 0.7760 - val_loss: 1.1636 - val_categorical_accuracy: 0.5599\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.59576\n","Epoch 29/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7170 - categorical_accuracy: 0.7775 - val_loss: 1.1219 - val_categorical_accuracy: 0.5614\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.59576\n","Epoch 30/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7128 - categorical_accuracy: 0.7821 - val_loss: 1.1717 - val_categorical_accuracy: 0.5594\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.59576\n","Epoch 31/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7221 - categorical_accuracy: 0.7688 - val_loss: 1.2672 - val_categorical_accuracy: 0.6013\n","\n","Epoch 00031: val_categorical_accuracy improved from 0.59576 to 0.60131, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 32/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.7055 - categorical_accuracy: 0.7865 - val_loss: 1.1259 - val_categorical_accuracy: 0.5740\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.60131\n","Epoch 33/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6901 - categorical_accuracy: 0.7816 - val_loss: 1.1604 - val_categorical_accuracy: 0.5983\n","\n","Epoch 00033: val_categorical_accuracy did not improve from 0.60131\n","Epoch 34/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6956 - categorical_accuracy: 0.7825 - val_loss: 1.2068 - val_categorical_accuracy: 0.5882\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.60131\n","Epoch 35/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6784 - categorical_accuracy: 0.7951 - val_loss: 1.1640 - val_categorical_accuracy: 0.5932\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.60131\n","Epoch 36/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6947 - categorical_accuracy: 0.7840 - val_loss: 1.1435 - val_categorical_accuracy: 0.5841\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.60131\n","Epoch 37/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6682 - categorical_accuracy: 0.7953 - val_loss: 1.1161 - val_categorical_accuracy: 0.6028\n","\n","Epoch 00037: val_categorical_accuracy improved from 0.60131 to 0.60283, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 38/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6535 - categorical_accuracy: 0.8023 - val_loss: 1.2008 - val_categorical_accuracy: 0.5968\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.60283\n","Epoch 39/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6390 - categorical_accuracy: 0.8104 - val_loss: 1.3276 - val_categorical_accuracy: 0.5846\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.60283\n","Epoch 40/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6549 - categorical_accuracy: 0.7916 - val_loss: 1.1095 - val_categorical_accuracy: 0.5755\n","\n","Epoch 00040: val_categorical_accuracy did not improve from 0.60283\n","Epoch 41/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6329 - categorical_accuracy: 0.8062 - val_loss: 1.1447 - val_categorical_accuracy: 0.5821\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.60283\n","Epoch 42/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6206 - categorical_accuracy: 0.8141 - val_loss: 1.1165 - val_categorical_accuracy: 0.5806\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.60283\n","Epoch 43/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6292 - categorical_accuracy: 0.8099 - val_loss: 1.1395 - val_categorical_accuracy: 0.5877\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.60283\n","Epoch 44/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6161 - categorical_accuracy: 0.8120 - val_loss: 1.1423 - val_categorical_accuracy: 0.5968\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.60283\n","Epoch 45/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6049 - categorical_accuracy: 0.8210 - val_loss: 1.1600 - val_categorical_accuracy: 0.6079\n","\n","Epoch 00045: val_categorical_accuracy improved from 0.60283 to 0.60788, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_75_samples_from_each_class_in_training_set.h5\n","Epoch 46/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.5979 - categorical_accuracy: 0.8236 - val_loss: 1.1031 - val_categorical_accuracy: 0.5998\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.60788\n","Epoch 47/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.5991 - categorical_accuracy: 0.8281 - val_loss: 1.1329 - val_categorical_accuracy: 0.5877\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.60788\n","Epoch 48/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.6000 - categorical_accuracy: 0.8206 - val_loss: 1.0736 - val_categorical_accuracy: 0.5958\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.60788\n","Epoch 49/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.5957 - categorical_accuracy: 0.8289 - val_loss: 1.1190 - val_categorical_accuracy: 0.5942\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.60788\n","Epoch 50/50\n","297/297 [==============================] - 0s 2ms/step - loss: 0.5822 - categorical_accuracy: 0.8281 - val_loss: 1.1201 - val_categorical_accuracy: 0.5998\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.60788\n","62/62 [==============================] - 0s 930us/step - loss: 1.1600 - categorical_accuracy: 0.6079\n","Test accuracy on target dataset = 0.6078827977180481\n","62/62 [==============================] - 0s 792us/step\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","\n","=============================================================================================================\n","Model training starts for data with overlap ratio 1.0 and 90 percent samples from each class in training set \n","==============================================================================================================\n","\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 20, 20, 64)   4160        input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 20, 20, 64)   256         conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 20, 20, 64)   0           batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","lambda_17 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_19 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_21 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","lambda_23 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 20, 20, 8)    584         lambda_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 20, 20, 8)    584         lambda_19[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 20, 20, 8)    584         lambda_21[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_52 (Conv2D)              (None, 20, 20, 8)    584         lambda_23[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_16 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 20, 20, 8)    584         conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_18 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 20, 20, 8)    584         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_20 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 20, 20, 8)    584         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_22 (Lambda)              (None, 20, 20, 8)    0           activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_53 (Conv2D)              (None, 20, 20, 8)    584         conv2d_52[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 20, 20, 8)    584         lambda_16[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 20, 20, 8)    584         conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 20, 20, 8)    584         lambda_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 20, 20, 8)    584         conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 20, 20, 8)    584         lambda_20[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 20, 20, 8)    584         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 20, 20, 8)    584         lambda_22[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_54 (Conv2D)              (None, 20, 20, 8)    584         conv2d_53[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 20, 20, 64)   0           conv2d_39[0][0]                  \n","                                                                 conv2d_42[0][0]                  \n","                                                                 conv2d_43[0][0]                  \n","                                                                 conv2d_46[0][0]                  \n","                                                                 conv2d_47[0][0]                  \n","                                                                 conv2d_50[0][0]                  \n","                                                                 conv2d_51[0][0]                  \n","                                                                 conv2d_54[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 20, 20, 64)   256         concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 20, 20, 64)   0           batch_normalization_11[0][0]     \n","__________________________________________________________________________________________________\n","tf.reshape_4 (TFOpLambda)       (None, 20, 20, 8, 8) 0           activation_13[0][0]              \n","__________________________________________________________________________________________________\n","tf.compat.v1.transpose_2 (TFOpL (None, 20, 20, 8, 8) 0           tf.reshape_4[0][0]               \n","__________________________________________________________________________________________________\n","tf.reverse_2 (TFOpLambda)       (None, 20, 20, 8, 8) 0           tf.compat.v1.transpose_2[0][0]   \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 20, 20, 64)   0           tf.reverse_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_12 (BatchNo (None, 20, 20, 64)   256         tf.reshape_5[0][0]               \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 20, 20, 64)   0           batch_normalization_12[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_55 (Conv2D)              (None, 20, 20, 128)  8320        activation_14[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_56 (Conv2D)              (None, 20, 20, 128)  8320        input_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_13 (BatchNo (None, 20, 20, 128)  512         conv2d_55[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_14 (BatchNo (None, 20, 20, 128)  512         conv2d_56[0][0]                  \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 20, 20, 128)  0           batch_normalization_13[0][0]     \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 20, 20, 128)  0           batch_normalization_14[0][0]     \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 20, 20, 128)  0           activation_15[0][0]              \n","                                                                 activation_16[0][0]              \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 20, 20, 128)  0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","global_average_pooling2d_2 (Glo (None, 128)          0           activation_17[0][0]              \n","==================================================================================================\n","Total params: 31,936\n","Trainable params: 31,040\n","Non-trainable params: 896\n","__________________________________________________________________________________________________\n","Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n","Total number of samples is 7906.\n","\n","unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in training set is 7112.\n","Samples per class in training set: [1231  470  290  657  320  753 2001  414  976]\n","\n","unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n","Total number of samples in test set is 794.\n","Samples per class in test set: [137  53  33  73  36  84 223  46 109]\n","\n","X_train_transfer => (7112, 128)\n","X_test_transfer  => (794, 128)\n","y_train => (7112, 9)\n","y_test  => (794, 9)\n","\n","Model: \"fine_tune\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         [(None, 128)]             0         \n","_________________________________________________________________\n","fc_256 (Dense)               (None, 256)               33024     \n","_________________________________________________________________\n","fc9 (Dense)                  (None, 9)                 2313      \n","=================================================================\n","Total params: 35,337\n","Trainable params: 35,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","356/356 [==============================] - 1s 2ms/step - loss: 1.8960 - categorical_accuracy: 0.3974 - val_loss: 1.5130 - val_categorical_accuracy: 0.3501\n","\n","Epoch 00001: val_categorical_accuracy improved from -inf to 0.35013, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 2/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.2110 - categorical_accuracy: 0.5545 - val_loss: 1.4267 - val_categorical_accuracy: 0.3501\n","\n","Epoch 00002: val_categorical_accuracy did not improve from 0.35013\n","Epoch 3/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.1541 - categorical_accuracy: 0.5849 - val_loss: 1.3873 - val_categorical_accuracy: 0.3552\n","\n","Epoch 00003: val_categorical_accuracy improved from 0.35013 to 0.35516, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 4/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.1003 - categorical_accuracy: 0.6186 - val_loss: 1.2784 - val_categorical_accuracy: 0.4547\n","\n","Epoch 00004: val_categorical_accuracy improved from 0.35516 to 0.45466, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 5/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0520 - categorical_accuracy: 0.6346 - val_loss: 1.2824 - val_categorical_accuracy: 0.4509\n","\n","Epoch 00005: val_categorical_accuracy did not improve from 0.45466\n","Epoch 6/50\n","356/356 [==============================] - 0s 1ms/step - loss: 1.0313 - categorical_accuracy: 0.6436 - val_loss: 1.2654 - val_categorical_accuracy: 0.4736\n","\n","Epoch 00006: val_categorical_accuracy improved from 0.45466 to 0.47355, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 7/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9829 - categorical_accuracy: 0.6660 - val_loss: 1.3045 - val_categorical_accuracy: 0.4647\n","\n","Epoch 00007: val_categorical_accuracy did not improve from 0.47355\n","Epoch 8/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9812 - categorical_accuracy: 0.6628 - val_loss: 1.2569 - val_categorical_accuracy: 0.4421\n","\n","Epoch 00008: val_categorical_accuracy did not improve from 0.47355\n","Epoch 9/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9621 - categorical_accuracy: 0.6709 - val_loss: 1.2137 - val_categorical_accuracy: 0.5088\n","\n","Epoch 00009: val_categorical_accuracy improved from 0.47355 to 0.50882, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 10/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9453 - categorical_accuracy: 0.6789 - val_loss: 1.2652 - val_categorical_accuracy: 0.4710\n","\n","Epoch 00010: val_categorical_accuracy did not improve from 0.50882\n","Epoch 11/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.9096 - categorical_accuracy: 0.6929 - val_loss: 1.1946 - val_categorical_accuracy: 0.4887\n","\n","Epoch 00011: val_categorical_accuracy did not improve from 0.50882\n","Epoch 12/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8919 - categorical_accuracy: 0.6976 - val_loss: 1.2558 - val_categorical_accuracy: 0.4685\n","\n","Epoch 00012: val_categorical_accuracy did not improve from 0.50882\n","Epoch 13/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8675 - categorical_accuracy: 0.7178 - val_loss: 1.1998 - val_categorical_accuracy: 0.5214\n","\n","Epoch 00013: val_categorical_accuracy improved from 0.50882 to 0.52141, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 14/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8631 - categorical_accuracy: 0.7162 - val_loss: 1.1512 - val_categorical_accuracy: 0.5479\n","\n","Epoch 00014: val_categorical_accuracy improved from 0.52141 to 0.54786, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 15/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8498 - categorical_accuracy: 0.7265 - val_loss: 1.1634 - val_categorical_accuracy: 0.5378\n","\n","Epoch 00015: val_categorical_accuracy did not improve from 0.54786\n","Epoch 16/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8449 - categorical_accuracy: 0.7254 - val_loss: 1.1645 - val_categorical_accuracy: 0.5353\n","\n","Epoch 00016: val_categorical_accuracy did not improve from 0.54786\n","Epoch 17/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.8184 - categorical_accuracy: 0.7359 - val_loss: 1.2364 - val_categorical_accuracy: 0.4887\n","\n","Epoch 00017: val_categorical_accuracy did not improve from 0.54786\n","Epoch 18/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7887 - categorical_accuracy: 0.7443 - val_loss: 1.1503 - val_categorical_accuracy: 0.5302\n","\n","Epoch 00018: val_categorical_accuracy did not improve from 0.54786\n","Epoch 19/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7798 - categorical_accuracy: 0.7550 - val_loss: 1.1124 - val_categorical_accuracy: 0.5403\n","\n","Epoch 00019: val_categorical_accuracy did not improve from 0.54786\n","Epoch 20/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7745 - categorical_accuracy: 0.7528 - val_loss: 1.2042 - val_categorical_accuracy: 0.5088\n","\n","Epoch 00020: val_categorical_accuracy did not improve from 0.54786\n","Epoch 21/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7677 - categorical_accuracy: 0.7602 - val_loss: 1.1936 - val_categorical_accuracy: 0.5164\n","\n","Epoch 00021: val_categorical_accuracy did not improve from 0.54786\n","Epoch 22/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7505 - categorical_accuracy: 0.7636 - val_loss: 1.1156 - val_categorical_accuracy: 0.5554\n","\n","Epoch 00022: val_categorical_accuracy improved from 0.54786 to 0.55542, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 23/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7438 - categorical_accuracy: 0.7611 - val_loss: 1.0944 - val_categorical_accuracy: 0.5844\n","\n","Epoch 00023: val_categorical_accuracy improved from 0.55542 to 0.58438, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 24/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7409 - categorical_accuracy: 0.7651 - val_loss: 1.0850 - val_categorical_accuracy: 0.5554\n","\n","Epoch 00024: val_categorical_accuracy did not improve from 0.58438\n","Epoch 25/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7220 - categorical_accuracy: 0.7756 - val_loss: 1.2228 - val_categorical_accuracy: 0.5315\n","\n","Epoch 00025: val_categorical_accuracy did not improve from 0.58438\n","Epoch 26/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7040 - categorical_accuracy: 0.7774 - val_loss: 1.1319 - val_categorical_accuracy: 0.5567\n","\n","Epoch 00026: val_categorical_accuracy did not improve from 0.58438\n","Epoch 27/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.7067 - categorical_accuracy: 0.7746 - val_loss: 1.1715 - val_categorical_accuracy: 0.5441\n","\n","Epoch 00027: val_categorical_accuracy did not improve from 0.58438\n","Epoch 28/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6981 - categorical_accuracy: 0.7857 - val_loss: 1.0730 - val_categorical_accuracy: 0.5630\n","\n","Epoch 00028: val_categorical_accuracy did not improve from 0.58438\n","Epoch 29/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6916 - categorical_accuracy: 0.7874 - val_loss: 1.1618 - val_categorical_accuracy: 0.5605\n","\n","Epoch 00029: val_categorical_accuracy did not improve from 0.58438\n","Epoch 30/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6758 - categorical_accuracy: 0.7936 - val_loss: 1.1557 - val_categorical_accuracy: 0.5504\n","\n","Epoch 00030: val_categorical_accuracy did not improve from 0.58438\n","Epoch 31/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6700 - categorical_accuracy: 0.7978 - val_loss: 1.0756 - val_categorical_accuracy: 0.5668\n","\n","Epoch 00031: val_categorical_accuracy did not improve from 0.58438\n","Epoch 32/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6527 - categorical_accuracy: 0.7990 - val_loss: 1.0982 - val_categorical_accuracy: 0.5630\n","\n","Epoch 00032: val_categorical_accuracy did not improve from 0.58438\n","Epoch 33/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6602 - categorical_accuracy: 0.7985 - val_loss: 1.0034 - val_categorical_accuracy: 0.6108\n","\n","Epoch 00033: val_categorical_accuracy improved from 0.58438 to 0.61083, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 34/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6520 - categorical_accuracy: 0.8026 - val_loss: 1.1653 - val_categorical_accuracy: 0.5302\n","\n","Epoch 00034: val_categorical_accuracy did not improve from 0.61083\n","Epoch 35/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6268 - categorical_accuracy: 0.8097 - val_loss: 1.0586 - val_categorical_accuracy: 0.5970\n","\n","Epoch 00035: val_categorical_accuracy did not improve from 0.61083\n","Epoch 36/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6117 - categorical_accuracy: 0.8152 - val_loss: 1.1158 - val_categorical_accuracy: 0.5756\n","\n","Epoch 00036: val_categorical_accuracy did not improve from 0.61083\n","Epoch 37/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6297 - categorical_accuracy: 0.8115 - val_loss: 1.0406 - val_categorical_accuracy: 0.5919\n","\n","Epoch 00037: val_categorical_accuracy did not improve from 0.61083\n","Epoch 38/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6186 - categorical_accuracy: 0.8199 - val_loss: 1.1587 - val_categorical_accuracy: 0.5605\n","\n","Epoch 00038: val_categorical_accuracy did not improve from 0.61083\n","Epoch 39/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6114 - categorical_accuracy: 0.8150 - val_loss: 1.0454 - val_categorical_accuracy: 0.5705\n","\n","Epoch 00039: val_categorical_accuracy did not improve from 0.61083\n","Epoch 40/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5954 - categorical_accuracy: 0.8205 - val_loss: 1.0065 - val_categorical_accuracy: 0.6310\n","\n","Epoch 00040: val_categorical_accuracy improved from 0.61083 to 0.63098, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/SGCNN_7//Trained_models//transferred_models/fine_tune_on_indian_pines_with_overlap_ratio_1.0_and_90_samples_from_each_class_in_training_set.h5\n","Epoch 41/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.6005 - categorical_accuracy: 0.8215 - val_loss: 1.0231 - val_categorical_accuracy: 0.6184\n","\n","Epoch 00041: val_categorical_accuracy did not improve from 0.63098\n","Epoch 42/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5958 - categorical_accuracy: 0.8219 - val_loss: 1.2182 - val_categorical_accuracy: 0.5139\n","\n","Epoch 00042: val_categorical_accuracy did not improve from 0.63098\n","Epoch 43/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5795 - categorical_accuracy: 0.8217 - val_loss: 1.0059 - val_categorical_accuracy: 0.5957\n","\n","Epoch 00043: val_categorical_accuracy did not improve from 0.63098\n","Epoch 44/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5884 - categorical_accuracy: 0.8178 - val_loss: 1.0346 - val_categorical_accuracy: 0.6146\n","\n","Epoch 00044: val_categorical_accuracy did not improve from 0.63098\n","Epoch 45/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5691 - categorical_accuracy: 0.8230 - val_loss: 1.0013 - val_categorical_accuracy: 0.6146\n","\n","Epoch 00045: val_categorical_accuracy did not improve from 0.63098\n","Epoch 46/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5808 - categorical_accuracy: 0.8252 - val_loss: 0.9894 - val_categorical_accuracy: 0.5982\n","\n","Epoch 00046: val_categorical_accuracy did not improve from 0.63098\n","Epoch 47/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5651 - categorical_accuracy: 0.8346 - val_loss: 1.0165 - val_categorical_accuracy: 0.5982\n","\n","Epoch 00047: val_categorical_accuracy did not improve from 0.63098\n","Epoch 48/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5600 - categorical_accuracy: 0.8334 - val_loss: 1.0478 - val_categorical_accuracy: 0.5554\n","\n","Epoch 00048: val_categorical_accuracy did not improve from 0.63098\n","Epoch 49/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5674 - categorical_accuracy: 0.8318 - val_loss: 0.9527 - val_categorical_accuracy: 0.6259\n","\n","Epoch 00049: val_categorical_accuracy did not improve from 0.63098\n","Epoch 50/50\n","356/356 [==============================] - 0s 1ms/step - loss: 0.5484 - categorical_accuracy: 0.8412 - val_loss: 1.0710 - val_categorical_accuracy: 0.5844\n","\n","Epoch 00050: val_categorical_accuracy did not improve from 0.63098\n","25/25 [==============================] - 0s 1ms/step - loss: 1.0065 - categorical_accuracy: 0.6310\n","Test accuracy on target dataset = 0.6309823393821716\n","25/25 [==============================] - 0s 805us/step\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n","=============================================================================================================\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VdgxRQRP0_mp"},"source":["# Transfer Learning results"]},{"cell_type":"code","metadata":{"id":"R_WJcBsGjtRA","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1615766049411,"user_tz":420,"elapsed":289,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"0a4cbbfd-f4ae-4688-ca20-db229cdb290b"},"source":["transfer_results"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Overlap_ratio</th>\n","      <th>Training Samples</th>\n","      <th>Test Samples</th>\n","      <th>Training_Test_Split</th>\n","      <th>Test_Accuracies</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>4740</td>\n","      <td>3166</td>\n","      <td>60</td>\n","      <td>60.99</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>5927</td>\n","      <td>1979</td>\n","      <td>75</td>\n","      <td>60.79</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>7112</td>\n","      <td>794</td>\n","      <td>90</td>\n","      <td>63.10</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Overlap_ratio  Training Samples  ...  Training_Test_Split  Test_Accuracies\n","0              1              4740  ...                   60            60.99\n","1              1              5927  ...                   75            60.79\n","2              1              7112  ...                   90            63.10\n","\n","[3 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"1o6mCjHJ01PG"},"source":["# Classification accuracies per class for each model"]},{"cell_type":"code","metadata":{"id":"rhikZcccng2K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615766186708,"user_tz":420,"elapsed":315,"user":{"displayName":"Shubhankar Kulkarni","photoUrl":"","userId":"06931175960113502123"}},"outputId":"29f16d94-a494-4a08-c261-a03361354d05"},"source":["for cm in confusion_matrixes:\n","  print(cm)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["                 1    2    3    4  ...    7    8    9  classfication_accuracies\n","0               55   15    0   42  ...  381    0    0                     10.04\n","1                0  141    5   11  ...   35    0   18                     67.14\n","2                0    0    0   88  ...    0    0   42                       0.0\n","3                0    0   44  226  ...    0    0   22                      77.4\n","4                0    0    0    0  ...    0    0    0                     100.0\n","5                2    0    0    0  ...  126    0    0                     61.79\n","6                0  110    4   63  ...  713    0    0                     80.11\n","7               80    0    0    9  ...   75   12    0                      6.52\n","8                0    0    0    0  ...    0    0  434                     100.0\n","Total Samples  548  210  130  292  ...  890  184  434                         -\n","\n","[10 rows x 10 columns]\n","                 1    2   3    4  ...    7    8    9  classfication_accuracies\n","0               10  126   0   10  ...  195    0    0                      2.92\n","1                0   58   6    0  ...   64    0    3                     44.27\n","2                0    0   0    0  ...   81    0    0                       0.0\n","3                0    0   0  151  ...    0    0   32                     82.51\n","4                0    0   0    0  ...    0    0    0                     100.0\n","5               38    0   0    0  ...   54    0    0                     56.19\n","6                0   41   8    5  ...  502    0    0                     90.29\n","7               20   38   0    0  ...   54    3    0                      2.61\n","8                0    0   0    0  ...    0    0  272                     100.0\n","Total Samples  342  131  81  183  ...  556  115  272                         -\n","\n","[10 rows x 10 columns]\n","                 1   2   3   4   5   6    7   8    9 classfication_accuracies\n","0               40  30   0  12   0   0   55   0    0                     29.2\n","1                0   9   0   5   0   0   38   0    1                    16.98\n","2                0   0  32   0   0   0    1   0    0                    96.97\n","3                0   0   3  56   0   0    0   0   14                    76.71\n","4                0   0   3   0  33   0    0   0    0                    91.67\n","5               41   0   0   0   0  37    6   0    0                    44.05\n","6                0   7   7  24   0   0  185   0    0                    82.96\n","7               24   3   0   1   0   0   18   0    0                      0.0\n","8                0   0   0   0   0   0    0   0  109                    100.0\n","Total Samples  137  53  33  73  36  84  223  46  109                        -\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_BFKA0crn0Gp"},"source":[""],"execution_count":null,"outputs":[]}]}