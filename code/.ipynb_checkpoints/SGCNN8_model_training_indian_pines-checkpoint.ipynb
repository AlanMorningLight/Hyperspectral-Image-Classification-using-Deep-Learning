{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up google colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9bq_kqWQtg0f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jK95udG-qHAy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/Hyperspectral_Image_Classification/code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vn-wRAH4t3WY"
   },
   "outputs": [],
   "source": [
    "from source_model_utils_indian_pines import *\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ky2Qzuw_qDdS"
   },
   "source": [
    "## Load Indian Pines Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uIndianPines = sio.loadmat('Indian_pines_corrected.mat')\n",
    "gt_IndianPines = sio.loadmat('Indian_pines_gt.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = uIndianPines['indian_pines_corrected']\n",
    "ground_truth = gt_IndianPines['indian_pines_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdMIhVj9qDdb",
    "outputId": "ce55d4d8-a9fc-477d-cd70-05720a0a9865"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAjjOxj3qDdb",
    "outputId": "58ced406-8b9c-45e1-cb55-f38de449eff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WxjgWNGqDdc"
   },
   "source": [
    "## Distrubution of samples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "yFA7eqA7qDdd",
    "outputId": "d5f81f86-2603-4747-9452-81b5a6181859"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class  samples\n",
       "0       0    10776\n",
       "1       1       46\n",
       "2       2     1428\n",
       "3       3      830\n",
       "4       4      237\n",
       "5       5      483\n",
       "6       6      730\n",
       "7       7       28\n",
       "8       8      478\n",
       "9       9       20\n",
       "10     10      972\n",
       "11     11     2455\n",
       "12     12      593\n",
       "13     13      205\n",
       "14     14     1265\n",
       "15     15      386\n",
       "16     16       93"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution = pd.DataFrame(np.unique(ground_truth, return_counts = True))\n",
    "class_distribution = class_distribution.transpose()\n",
    "class_distribution.columns = ['class','samples']\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zTsq-yPqDdd",
    "outputId": "3a9b5c84-0d47-4c02-a45a-95deca2426de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  5,  6,  8, 10, 11, 12, 14], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes , counts = np.unique(ground_truth, return_counts = True)\n",
    "classes = classes[[2,3,5,6,8,10,11,12,14]] ## Dropping classes with small number of samples\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pick samples belonging to all classes\n",
    "\n",
    "def pick_samples_from_class(Class, cube_size, data, ground_truth, cubes, output_class, overlap_ratio, channels):\n",
    "    \n",
    "    ## Get row and column position from ground truth image for class\n",
    "    class_indices = np.where(ground_truth == Class)\n",
    "    \n",
    "    ## Remove border position class samples\n",
    "    class_cube_positions = [[class_indices[0][i], class_indices[1][i]] for i in range(len(class_indices[0])) \n",
    "                        if len(ground_truth) - np.ceil(cube_size / 2) > class_indices[0][i] > np.ceil(cube_size / 2) \n",
    "                        and len(ground_truth[0]) - np.ceil(cube_size / 2) > class_indices[1][i] > np.ceil(cube_size / 2)]\n",
    "    \n",
    "    #print('Length of class positions', len(class_cube_positions))\n",
    "    \n",
    "    extracted_cubes = [[class_cube_positions[0][0], class_cube_positions[0][1]]]\n",
    "    \n",
    "    ## Form the first cube for this class\n",
    "    cubes.append(np.array(data[class_cube_positions[0][0] - int(cube_size / 2):class_cube_positions[0][0] + int(cube_size / 2),\n",
    "                       (class_cube_positions[0][1] - int(cube_size / 2)):class_cube_positions[0][1] + int(cube_size / 2),\n",
    "                         :channels]))\n",
    "    \n",
    "    ## Output class value\n",
    "    output_class.append(Class)\n",
    "        \n",
    "    ## Pick cube/sample if it satisfies the criteria for the overlap ratio\n",
    "    for i in range(1, len(class_cube_positions)):\n",
    "        \n",
    "        distance_vector = [] ## Calculate distance from existing sample to the next candiddate cube sample\n",
    "        \n",
    "        for k in range(len(extracted_cubes)):\n",
    "            \n",
    "            distance = math.sqrt((class_cube_positions[i][0] - extracted_cubes[k][0]) ** 2 + \n",
    "                                 (class_cube_positions[i][1] - extracted_cubes[k][1]) ** 2)\n",
    "            \n",
    "            distance_vector.append(distance)\n",
    "            \n",
    "        if np.min(distance_vector) > int(cube_size * (1 - overlap_ratio)):\n",
    "            \n",
    "            cubes.append(np.array(data[class_cube_positions[i][0] - int(cube_size / 2):class_cube_positions[i][0] + int(cube_size / 2),\n",
    "                                      (class_cube_positions[i][1] - int(cube_size / 2)):class_cube_positions[i][1] + int(cube_size / 2),\n",
    "                                      :channels]))\n",
    "            \n",
    "            output_class.append(Class)\n",
    "            extracted_cubes.append([class_cube_positions[i][0], class_cube_positions[i][1]])\n",
    "            \n",
    "    return cubes, output_class, extracted_cubes\n",
    "\n",
    "## Collect and combine samples from all classes\n",
    "\n",
    "def collect_samples_from_all_classes(classes, cube_size, data, ground_truth, cubes, output_class, overlap_ratio, channels):\n",
    "    \n",
    "    class_samples = []\n",
    "    \n",
    "    for Class in classes:\n",
    "        cubes, output_class, extracted_cubes = pick_samples_from_class(Class, cube_size, data, ground_truth, cubes, \n",
    "                                                                       output_class,overlap_ratio, channels)\n",
    "        class_samples.append(len(extracted_cubes))\n",
    "    \n",
    "    cubes = np.array(cubes)\n",
    "    output_class = np.array(output_class)\n",
    "    \n",
    "    print('Class Samples : ', class_samples)\n",
    "    \n",
    "    return cubes, output_class, class_samples\n",
    "\n",
    "## Prepare Training, Validation & Test Data\n",
    "\n",
    "def training_and_test_set(training_samples_from_each_class, \n",
    "                          class_samples, cubes, output_class):\n",
    "    \n",
    "    class_2_samples = cubes[np.where(output_class == 2)[0]]\n",
    "    class_2_labels = output_class[np.where(output_class == 2)[0]]\n",
    "\n",
    "    class_3_samples = cubes[np.where(output_class == 3)[0]]\n",
    "    class_3_labels = output_class[np.where(output_class == 3)[0]]\n",
    "\n",
    "    class_5_samples = cubes[np.where(output_class == 5)[0]]\n",
    "    class_5_labels = output_class[np.where(output_class == 5)[0]]\n",
    "\n",
    "    class_6_samples = cubes[np.where(output_class == 6)[0]]\n",
    "    class_6_labels = output_class[np.where(output_class == 6)[0]]\n",
    "\n",
    "    class_8_samples = cubes[np.where(output_class == 8)[0]]\n",
    "    class_8_labels = output_class[np.where(output_class == 8)[0]]\n",
    "\n",
    "    class_10_samples = cubes[np.where(output_class == 10)[0]]\n",
    "    class_10_labels = output_class[np.where(output_class == 10)[0]]\n",
    "    \n",
    "    class_11_samples = cubes[np.where(output_class == 11)[0]]\n",
    "    class_11_labels = output_class[np.where(output_class == 11)[0]]\n",
    "    \n",
    "    class_12_samples = cubes[np.where(output_class == 12)[0]]\n",
    "    class_12_labels = output_class[np.where(output_class == 12)[0]]\n",
    "    \n",
    "    class_14_samples = cubes[np.where(output_class == 14)[0]]\n",
    "    class_14_labels = output_class[np.where(output_class == 14)[0]]\n",
    "\n",
    "\n",
    "    class_samples_collection = [class_2_samples, class_3_samples, class_5_samples, class_6_samples,\n",
    "                               class_8_samples, class_10_samples, class_11_samples, class_12_samples, class_14_samples]\n",
    "\n",
    "    class_labels_collection = [class_2_samples, class_3_samples, class_5_samples, class_6_samples,\n",
    "                               class_8_samples, class_10_samples, class_11_samples, class_12_samples, class_14_samples]\n",
    "\n",
    "    # Training & Test Set Arrays\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    # Get Training set size samples from each class\n",
    "    for samples in class_samples_collection:\n",
    "        \n",
    "        X_train.append(samples[0:training_samples_from_each_class])\n",
    "        \n",
    "        X_test.append(samples[training_samples_from_each_class:])\n",
    "        \n",
    "    # Get output labels\n",
    "    for labels in class_labels_collection:\n",
    "        y_train.append(labels[0:training_samples_from_each_class])\n",
    "        \n",
    "        y_test.append(labels[training_samples_from_each_class :])\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis = 0)\n",
    "    X_test = np.concatenate(X_test, axis = 0)\n",
    "\n",
    "    y_train = np.concatenate(y_train, axis = 0)\n",
    "    y_test = np.concatenate(y_test, axis = 0)\n",
    "\n",
    "    \n",
    "    ## Shuffle Training Set\n",
    "    samples_train = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(samples_train)\n",
    "\n",
    "    X_train = X_train[samples_train]\n",
    "    y_train = y_train[samples_train]\n",
    "\n",
    "\n",
    "    ## Shuffle Test Set\n",
    "    samples_test = np.arange(X_test.shape[0])\n",
    "    np.random.shuffle(samples_test)\n",
    "\n",
    "    X_test = X_test[samples_test]\n",
    "    y_test = y_test[samples_test]\n",
    "\n",
    "    # Get counts(samples) of each class in test set\n",
    "    values_test_set, counts_test_set = np.unique(y_test, return_counts = True)\n",
    "    values_training_set, counts_training_set = np.unique(y_train, return_counts = True)\n",
    "\n",
    "\n",
    "    print(\"Samples per class: \" + str(class_samples) + '\\n'\n",
    "          \"Total number of samples is \" + str(np.sum(class_samples)) + '.\\n')\n",
    "    \n",
    "    print(\"unique classes in training set: \" + str(values_training_set) + '\\n'\n",
    "          \"Total number of samples in training set is \" + str(np.sum(counts_training_set)) + '.\\n'\n",
    "          \"Samples per class in training set: \" + str(counts_training_set) + '\\n')\n",
    "\n",
    "    print(\"unique classes in test set: \" + str(values_test_set) + '\\n'\n",
    "          \"Total number of samples in test set is \" + str(np.sum(counts_test_set)) + '.\\n'\n",
    "          \"Samples per class in test set: \" + str(counts_test_set) + '\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    ## one hot encode labels\n",
    "    onehot_encoder = OneHotEncoder(sparse = False)\n",
    "\n",
    "    y_train = y_train.reshape(len(y_train), 1)\n",
    "    y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "    y_train = onehot_encoder.fit_transform(y_train)\n",
    "    y_test = onehot_encoder.fit_transform(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, counts_test_set, class_samples\n",
    "\n",
    "\n",
    "def sample_extraction(classes, cube_size, data, ground_truth, cubes, output_class, training_samples_from_each_class,\n",
    "                      overlap_ratio, channels):\n",
    "    \n",
    "    cubes, output_class, class_samples = collect_samples_from_all_classes(classes, \n",
    "                                                                      cube_size, \n",
    "                                                                      data,  \n",
    "                                                                      ground_truth, \n",
    "                                                                      cubes, \n",
    "                                                                      output_class , \n",
    "                                                                      overlap_ratio, \n",
    "                                                                      channels)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, counts_test_set, class_samples = training_and_test_set(\n",
    "                                                                            training_samples_from_each_class,\n",
    "                                                                            class_samples, \n",
    "                                                                            cubes,\n",
    "                                                                            output_class)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, counts_test_set, class_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Samples :  [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n"
     ]
    }
   ],
   "source": [
    "cubes, output_class, class_samples = collect_samples_from_all_classes(classes = classes, \n",
    "                                                                      cube_size = 20, \n",
    "                                                                      data = data, \n",
    "                                                                      ground_truth = ground_truth, \n",
    "                                                                      cubes = [], \n",
    "                                                                      output_class = [], \n",
    "                                                                      overlap_ratio = 1, \n",
    "                                                                      channels = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  3,  5,  6,  8, 10, 11, 12, 14], dtype=uint8),\n",
       " array([1368,  523,  323,  730,  356,  837, 2224,  460, 1085]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(output_class, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_2_samples = cubes[np.where(output_class == 2)[0]]\n",
    "class_2_labels = output_class[np.where(output_class == 2)[0]]\n",
    "\n",
    "class_3_samples = cubes[np.where(output_class == 3)[0]]\n",
    "class_3_labels = output_class[np.where(output_class == 3)[0]]\n",
    "\n",
    "class_5_samples = cubes[np.where(output_class == 5)[0]]\n",
    "class_5_labels = output_class[np.where(output_class == 5)[0]]\n",
    "\n",
    "class_6_samples = cubes[np.where(output_class == 6)[0]]\n",
    "class_6_labels = output_class[np.where(output_class == 6)[0]]\n",
    "\n",
    "class_8_samples = cubes[np.where(output_class == 8)[0]]\n",
    "class_8_labels = output_class[np.where(output_class == 8)[0]]\n",
    "\n",
    "class_10_samples = cubes[np.where(output_class == 10)[0]]\n",
    "class_10_labels = output_class[np.where(output_class == 10)[0]]\n",
    "\n",
    "class_11_samples = cubes[np.where(output_class == 11)[0]]\n",
    "class_11_labels = output_class[np.where(output_class == 11)[0]]\n",
    "\n",
    "class_12_samples = cubes[np.where(output_class == 12)[0]]\n",
    "class_12_labels = output_class[np.where(output_class == 12)[0]]\n",
    "\n",
    "class_14_samples = cubes[np.where(output_class == 14)[0]]\n",
    "class_14_labels = output_class[np.where(output_class == 14)[0]]\n",
    "\n",
    "\n",
    "class_samples_collection = [class_2_samples, class_3_samples, class_5_samples, class_6_samples,\n",
    "                           class_8_samples, class_10_samples, class_11_samples, class_12_samples, class_14_samples]\n",
    "\n",
    "class_labels_collection = [class_2_labels, class_3_labels, class_5_labels, class_6_labels,\n",
    "                               class_8_labels, class_10_labels, class_11_labels, class_12_labels, class_14_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 20, 20, 64)\n",
      "(6106, 20, 20, 64)\n",
      "(1800,)\n",
      "(6106,)\n"
     ]
    }
   ],
   "source": [
    "# Training & Test Set Arrays\n",
    "X_train = []\n",
    "X_test = []\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# Get Training set size samples from each class\n",
    "for samples in class_samples_collection:\n",
    "\n",
    "    X_train.append(samples[0:200])\n",
    "\n",
    "    X_test.append(samples[200:])\n",
    "\n",
    "# Get output labels\n",
    "for labels in class_labels_collection:\n",
    "    y_train.append(labels[0:200])\n",
    "\n",
    "    y_test.append(labels[200:])\n",
    "\n",
    "X_train = np.concatenate(X_train, axis = 0)\n",
    "X_test = np.concatenate(X_test, axis = 0)\n",
    "\n",
    "y_train = np.concatenate(y_train, axis = 0)\n",
    "y_test = np.concatenate(y_test, axis = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n",
      "Total number of samples is 7906.\n",
      "\n",
      "unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n",
      "Total number of samples in training set is 1800.\n",
      "Samples per class in training set: [200 200 200 200 200 200 200 200 200]\n",
      "\n",
      "unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n",
      "Total number of samples in test set is 6106.\n",
      "Samples per class in test set: [1168  323  123  530  156  637 2024  260  885]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Shuffle Training Set\n",
    "samples_train = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(samples_train)\n",
    "\n",
    "X_train = X_train[samples_train]\n",
    "y_train = y_train[samples_train]\n",
    "\n",
    "\n",
    "## Shuffle Test Set\n",
    "samples_test = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(samples_test)\n",
    "\n",
    "X_test = X_test[samples_test]\n",
    "y_test = y_test[samples_test]\n",
    "\n",
    "# Get counts(samples) of each class in test set\n",
    "values_test_set, counts_test_set = np.unique(y_test, return_counts = True)\n",
    "values_training_set, counts_training_set = np.unique(y_train, return_counts = True)\n",
    "\n",
    "\n",
    "print(\"Samples per class: \" + str(class_samples) + '\\n'\n",
    "      \"Total number of samples is \" + str(np.sum(class_samples)) + '.\\n')\n",
    "\n",
    "print(\"unique classes in training set: \" + str(values_training_set) + '\\n'\n",
    "      \"Total number of samples in training set is \" + str(np.sum(counts_training_set)) + '.\\n'\n",
    "      \"Samples per class in training set: \" + str(counts_training_set) + '\\n')\n",
    "\n",
    "print(\"unique classes in test set: \" + str(values_test_set) + '\\n'\n",
    "      \"Total number of samples in test set is \" + str(np.sum(counts_test_set)) + '.\\n'\n",
    "      \"Samples per class in test set: \" + str(counts_test_set) + '\\n')\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  3,  5,  6,  8, 10, 11, 12, 14], dtype=uint8),\n",
       " array([200, 200, 200, 200, 200, 200, 200, 200, 200]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## one hot encode labels\n",
    "onehot_encoder = OneHotEncoder(sparse = False)\n",
    "\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "y_train = onehot_encoder.fit_transform(y_train)\n",
    "y_test = onehot_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 20, 20, 64)\n",
      "(6106, 20, 20, 64)\n",
      "(1800, 9)\n",
      "(6106, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_PESscnqDde"
   },
   "source": [
    "## Train model with overlap ratio 1 for 100 epochs (600 samples from each class in training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vc1sJwRqDdf",
    "outputId": "367d8c9c-20a2-47f7-f377-847c1cb6ebab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================================================================\n",
      "Model training starts for data with 600 samples from each class in training set\n",
      "==============================================================================================================\n",
      "\n",
      "Class Samples :  [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n",
      "Samples per class: [5975, 15062, 1742, 2854, 1345, 5029, 1330, 3682, 940]\n",
      "Total number of samples is 37959.\n",
      "\n",
      "unique classes in training set: [1 2 3 4 5 6 7 8 9]\n",
      "Total number of samples in training set is 5400.\n",
      "Samples per class in training set: [600 600 600 600 600 600 600 600 600]\n",
      "\n",
      "unique classes in test set: [1 2 3 4 5 6 7 8 9]\n",
      "Total number of samples in test set is 32559.\n",
      "Samples per class in test set: [ 5375 14462  1142  2254   745  4429   730  3082   340]\n",
      "\n",
      "\n",
      "\n",
      "X_train => (5400, 20, 20, 64)\n",
      "X_test  => (32559, 20, 20, 64)\n",
      "y_train => (5400, 9)\n",
      "y_test  => (32559, 9)\n",
      "\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 10, 10, 64)   4160        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 10, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 10, 10, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 10, 10, 8)    584         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 10, 10, 8)    584         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 10, 10, 8)    584         lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 10, 10, 8)    584         lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 10, 8)    584         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 10, 10, 8)    584         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 10, 10, 8)    584         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 10, 10, 8)    584         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 10, 10, 8)    584         lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 10, 10, 8)    584         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 10, 10, 8)    584         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 10, 10, 8)    584         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 10, 10, 8)    584         lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 10, 10, 8)    584         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 10, 10, 8)    584         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 10, 10, 8)    584         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10, 10, 64)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10, 10, 64)   256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 10, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 10, 10, 128)  8320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 10, 10, 128)  8320        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10, 10, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10, 10, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10, 10, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10, 10, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 10, 128)  0           activation_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10, 10, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_256 (Dense)                  (None, 256)          33024       global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 9)            2313        fc_256[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 104,201\n",
      "Trainable params: 103,305\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 4.8897 - categorical_accuracy: 0.1264\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.02691, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 4.8894 - categorical_accuracy: 0.1265 - val_loss: 4.6387 - val_categorical_accuracy: 0.0269\n",
      "Epoch 2/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 4.5962 - categorical_accuracy: 0.1392\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.02691 to 0.05049, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 4.5955 - categorical_accuracy: 0.1396 - val_loss: 4.5721 - val_categorical_accuracy: 0.0505\n",
      "Epoch 3/100\n",
      "210/216 [============================>.] - ETA: 0s - loss: 4.4159 - categorical_accuracy: 0.1549\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.05049 to 0.07061, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 4.4138 - categorical_accuracy: 0.1554 - val_loss: 4.5063 - val_categorical_accuracy: 0.0706\n",
      "Epoch 4/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.2983 - categorical_accuracy: 0.2393\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.07061 to 0.10873, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 4.2983 - categorical_accuracy: 0.2393 - val_loss: 4.4529 - val_categorical_accuracy: 0.1087\n",
      "Epoch 5/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 4.2274 - categorical_accuracy: 0.2993\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.10873 to 0.12700, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 4.2278 - categorical_accuracy: 0.2981 - val_loss: 4.4030 - val_categorical_accuracy: 0.1270\n",
      "Epoch 6/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 4.1739 - categorical_accuracy: 0.3439\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.12700 to 0.14703, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 4.1750 - categorical_accuracy: 0.3426 - val_loss: 4.3543 - val_categorical_accuracy: 0.1470\n",
      "Epoch 7/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 4.1298 - categorical_accuracy: 0.4030\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.14703 to 0.18182, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 4.1303 - categorical_accuracy: 0.4039 - val_loss: 4.3178 - val_categorical_accuracy: 0.1818\n",
      "Epoch 8/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 4.0983 - categorical_accuracy: 0.4931\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.18182 to 0.26364, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 4.0983 - categorical_accuracy: 0.4931 - val_loss: 4.2794 - val_categorical_accuracy: 0.2636\n",
      "Epoch 9/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 4.0687 - categorical_accuracy: 0.5543\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.26364 to 0.36251, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 4.0695 - categorical_accuracy: 0.5539 - val_loss: 4.2388 - val_categorical_accuracy: 0.3625\n",
      "Epoch 10/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 4.0325 - categorical_accuracy: 0.6300\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.36251 to 0.42987, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 4.0317 - categorical_accuracy: 0.6300 - val_loss: 4.2079 - val_categorical_accuracy: 0.4299\n",
      "Epoch 11/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 4.0030 - categorical_accuracy: 0.6530\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.42987 to 0.47130, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 4.0042 - categorical_accuracy: 0.6506 - val_loss: 4.1805 - val_categorical_accuracy: 0.4713\n",
      "Epoch 12/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.9818 - categorical_accuracy: 0.6608\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.47130 to 0.50250, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.9815 - categorical_accuracy: 0.6622 - val_loss: 4.1518 - val_categorical_accuracy: 0.5025\n",
      "Epoch 13/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.9492 - categorical_accuracy: 0.6881\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.50250 to 0.53494, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.9485 - categorical_accuracy: 0.6885 - val_loss: 4.1214 - val_categorical_accuracy: 0.5349\n",
      "Epoch 14/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.9186 - categorical_accuracy: 0.7062\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.53494\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.9186 - categorical_accuracy: 0.7050 - val_loss: 4.1074 - val_categorical_accuracy: 0.5347\n",
      "Epoch 15/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.8970 - categorical_accuracy: 0.7106\n",
      "Epoch 00015: val_categorical_accuracy improved from 0.53494 to 0.54710, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.8966 - categorical_accuracy: 0.7104 - val_loss: 4.0795 - val_categorical_accuracy: 0.5471\n",
      "Epoch 16/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.8704 - categorical_accuracy: 0.7142\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.54710 to 0.55782, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.8697 - categorical_accuracy: 0.7146 - val_loss: 4.0529 - val_categorical_accuracy: 0.5578\n",
      "Epoch 17/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.8455 - categorical_accuracy: 0.7170\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.55782\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.8460 - categorical_accuracy: 0.7167 - val_loss: 4.0373 - val_categorical_accuracy: 0.5555\n",
      "Epoch 18/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.8251 - categorical_accuracy: 0.7319\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.55782\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.8246 - categorical_accuracy: 0.7315 - val_loss: 4.0201 - val_categorical_accuracy: 0.5526\n",
      "Epoch 19/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.8036 - categorical_accuracy: 0.7253\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.55782\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.8022 - categorical_accuracy: 0.7256 - val_loss: 3.9996 - val_categorical_accuracy: 0.5551\n",
      "Epoch 20/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.7829 - categorical_accuracy: 0.7340\n",
      "Epoch 00020: val_categorical_accuracy improved from 0.55782 to 0.56024, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.7823 - categorical_accuracy: 0.7343 - val_loss: 3.9771 - val_categorical_accuracy: 0.5602\n",
      "Epoch 21/100\n",
      "210/216 [============================>.] - ETA: 0s - loss: 3.7569 - categorical_accuracy: 0.7480\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.56024\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.7567 - categorical_accuracy: 0.7491 - val_loss: 3.9730 - val_categorical_accuracy: 0.5473\n",
      "Epoch 22/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.7403 - categorical_accuracy: 0.7574\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.56024\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.7412 - categorical_accuracy: 0.7561 - val_loss: 3.9503 - val_categorical_accuracy: 0.5572\n",
      "Epoch 23/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.7203 - categorical_accuracy: 0.7581\n",
      "Epoch 00023: val_categorical_accuracy improved from 0.56024 to 0.56043, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.7201 - categorical_accuracy: 0.7580 - val_loss: 3.9315 - val_categorical_accuracy: 0.5604\n",
      "Epoch 24/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.7036 - categorical_accuracy: 0.7524\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.56043\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.7036 - categorical_accuracy: 0.7524 - val_loss: 3.9179 - val_categorical_accuracy: 0.5599\n",
      "Epoch 25/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.6779 - categorical_accuracy: 0.7764\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.56043\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.6786 - categorical_accuracy: 0.7752 - val_loss: 3.9062 - val_categorical_accuracy: 0.5577\n",
      "Epoch 26/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.6608 - categorical_accuracy: 0.7763\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.56043\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.6609 - categorical_accuracy: 0.7770 - val_loss: 3.8892 - val_categorical_accuracy: 0.5598\n",
      "Epoch 27/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.6403 - categorical_accuracy: 0.7725\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.56043\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.6397 - categorical_accuracy: 0.7728 - val_loss: 3.8774 - val_categorical_accuracy: 0.5574\n",
      "Epoch 28/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 3.6190 - categorical_accuracy: 0.7849\n",
      "Epoch 00028: val_categorical_accuracy improved from 0.56043 to 0.56095, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.6190 - categorical_accuracy: 0.7854 - val_loss: 3.8595 - val_categorical_accuracy: 0.5610\n",
      "Epoch 29/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.6035 - categorical_accuracy: 0.7835\n",
      "Epoch 00029: val_categorical_accuracy improved from 0.56095 to 0.56482, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.6035 - categorical_accuracy: 0.7835 - val_loss: 3.8470 - val_categorical_accuracy: 0.5648\n",
      "Epoch 30/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.5835 - categorical_accuracy: 0.7909\n",
      "Epoch 00030: val_categorical_accuracy improved from 0.56482 to 0.56596, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.5839 - categorical_accuracy: 0.7909 - val_loss: 3.8318 - val_categorical_accuracy: 0.5660\n",
      "Epoch 31/100\n",
      "210/216 [============================>.] - ETA: 0s - loss: 3.5677 - categorical_accuracy: 0.7910\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.56596\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.5692 - categorical_accuracy: 0.7907 - val_loss: 3.8269 - val_categorical_accuracy: 0.5595\n",
      "Epoch 32/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.5541 - categorical_accuracy: 0.7951\n",
      "Epoch 00032: val_categorical_accuracy improved from 0.56596 to 0.56854, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 3.5537 - categorical_accuracy: 0.7965 - val_loss: 3.8065 - val_categorical_accuracy: 0.5685\n",
      "Epoch 33/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.5293 - categorical_accuracy: 0.8028\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.56854\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.5279 - categorical_accuracy: 0.8030 - val_loss: 3.7988 - val_categorical_accuracy: 0.5647\n",
      "Epoch 34/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.5141 - categorical_accuracy: 0.8002\n",
      "Epoch 00034: val_categorical_accuracy improved from 0.56854 to 0.56946, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.5141 - categorical_accuracy: 0.8002 - val_loss: 3.7839 - val_categorical_accuracy: 0.5695\n",
      "Epoch 35/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.5021 - categorical_accuracy: 0.7992\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.56946\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.5013 - categorical_accuracy: 0.7996 - val_loss: 3.7709 - val_categorical_accuracy: 0.5666\n",
      "Epoch 36/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.4860 - categorical_accuracy: 0.8043\n",
      "Epoch 00036: val_categorical_accuracy improved from 0.56946 to 0.57422, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 3.4839 - categorical_accuracy: 0.8050 - val_loss: 3.7558 - val_categorical_accuracy: 0.5742\n",
      "Epoch 37/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 3.4662 - categorical_accuracy: 0.8134\n",
      "Epoch 00037: val_categorical_accuracy improved from 0.57422 to 0.57425, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 8s 35ms/step - loss: 3.4662 - categorical_accuracy: 0.8137 - val_loss: 3.7453 - val_categorical_accuracy: 0.5742\n",
      "Epoch 38/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.4587 - categorical_accuracy: 0.8041\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.57425\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.4588 - categorical_accuracy: 0.8041 - val_loss: 3.7451 - val_categorical_accuracy: 0.5620\n",
      "Epoch 39/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.4351 - categorical_accuracy: 0.8174\n",
      "Epoch 00039: val_categorical_accuracy improved from 0.57425 to 0.57446, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.4349 - categorical_accuracy: 0.8172 - val_loss: 3.7241 - val_categorical_accuracy: 0.5745\n",
      "Epoch 40/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.4154 - categorical_accuracy: 0.8244\n",
      "Epoch 00040: val_categorical_accuracy improved from 0.57446 to 0.57959, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.4154 - categorical_accuracy: 0.8244 - val_loss: 3.7103 - val_categorical_accuracy: 0.5796\n",
      "Epoch 41/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.4033 - categorical_accuracy: 0.8176\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.57959\n",
      "216/216 [==============================] - 7s 31ms/step - loss: 3.4032 - categorical_accuracy: 0.8172 - val_loss: 3.7013 - val_categorical_accuracy: 0.5773\n",
      "Epoch 42/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.3932 - categorical_accuracy: 0.8207\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.57959\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.3932 - categorical_accuracy: 0.8207 - val_loss: 3.7020 - val_categorical_accuracy: 0.5729\n",
      "Epoch 43/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.3820 - categorical_accuracy: 0.8207\n",
      "Epoch 00043: val_categorical_accuracy improved from 0.57959 to 0.58162, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.3798 - categorical_accuracy: 0.8211 - val_loss: 3.6855 - val_categorical_accuracy: 0.5816\n",
      "Epoch 44/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.3656 - categorical_accuracy: 0.8278\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.58162\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.3639 - categorical_accuracy: 0.8283 - val_loss: 3.6749 - val_categorical_accuracy: 0.5797\n",
      "Epoch 45/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.3516 - categorical_accuracy: 0.8183\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.58162\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.3521 - categorical_accuracy: 0.8178 - val_loss: 3.6704 - val_categorical_accuracy: 0.5785\n",
      "Epoch 46/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.3361 - categorical_accuracy: 0.8304\n",
      "Epoch 00046: val_categorical_accuracy improved from 0.58162 to 0.58319, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.3360 - categorical_accuracy: 0.8294 - val_loss: 3.6587 - val_categorical_accuracy: 0.5832\n",
      "Epoch 47/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 3.3203 - categorical_accuracy: 0.8331\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.58319\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.3204 - categorical_accuracy: 0.8326 - val_loss: 3.6507 - val_categorical_accuracy: 0.5828\n",
      "Epoch 48/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.3102 - categorical_accuracy: 0.8320\n",
      "Epoch 00048: val_categorical_accuracy improved from 0.58319 to 0.58423, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.3102 - categorical_accuracy: 0.8319 - val_loss: 3.6320 - val_categorical_accuracy: 0.5842\n",
      "Epoch 49/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.3002 - categorical_accuracy: 0.8336\n",
      "Epoch 00049: val_categorical_accuracy improved from 0.58423 to 0.58958, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 3.2992 - categorical_accuracy: 0.8341 - val_loss: 3.6391 - val_categorical_accuracy: 0.5896\n",
      "Epoch 50/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.2848 - categorical_accuracy: 0.8357\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.58958\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.2844 - categorical_accuracy: 0.8361 - val_loss: 3.6271 - val_categorical_accuracy: 0.5862\n",
      "Epoch 51/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 3.2671 - categorical_accuracy: 0.8452\n",
      "Epoch 00051: val_categorical_accuracy improved from 0.58958 to 0.59044, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 3.2673 - categorical_accuracy: 0.8452 - val_loss: 3.6191 - val_categorical_accuracy: 0.5904\n",
      "Epoch 52/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 3.2572 - categorical_accuracy: 0.8437\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.59044\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.2569 - categorical_accuracy: 0.8441 - val_loss: 3.6078 - val_categorical_accuracy: 0.5883\n",
      "Epoch 53/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.2379 - categorical_accuracy: 0.8482\n",
      "Epoch 00053: val_categorical_accuracy improved from 0.59044 to 0.59348, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.2373 - categorical_accuracy: 0.8480 - val_loss: 3.6041 - val_categorical_accuracy: 0.5935\n",
      "Epoch 54/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.2335 - categorical_accuracy: 0.8447\n",
      "Epoch 00054: val_categorical_accuracy improved from 0.59348 to 0.59661, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.2342 - categorical_accuracy: 0.8437 - val_loss: 3.5904 - val_categorical_accuracy: 0.5966\n",
      "Epoch 55/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.2213 - categorical_accuracy: 0.8517\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.59661\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.2213 - categorical_accuracy: 0.8517 - val_loss: 3.5906 - val_categorical_accuracy: 0.5918\n",
      "Epoch 56/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.2167 - categorical_accuracy: 0.8417\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.59661\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.2144 - categorical_accuracy: 0.8437 - val_loss: 3.5766 - val_categorical_accuracy: 0.5965\n",
      "Epoch 57/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.1984 - categorical_accuracy: 0.8515\n",
      "Epoch 00057: val_categorical_accuracy improved from 0.59661 to 0.60029, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.1978 - categorical_accuracy: 0.8524 - val_loss: 3.5765 - val_categorical_accuracy: 0.6003\n",
      "Epoch 58/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.1801 - categorical_accuracy: 0.8543\n",
      "Epoch 00058: val_categorical_accuracy improved from 0.60029 to 0.60358, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.1801 - categorical_accuracy: 0.8543 - val_loss: 3.5614 - val_categorical_accuracy: 0.6036\n",
      "Epoch 59/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.1760 - categorical_accuracy: 0.8530\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.60358\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.1758 - categorical_accuracy: 0.8535 - val_loss: 3.5559 - val_categorical_accuracy: 0.6005\n",
      "Epoch 60/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.1576 - categorical_accuracy: 0.8580\n",
      "Epoch 00060: val_categorical_accuracy improved from 0.60358 to 0.60490, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 3.1581 - categorical_accuracy: 0.8585 - val_loss: 3.5496 - val_categorical_accuracy: 0.6049\n",
      "Epoch 61/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.1538 - categorical_accuracy: 0.8513\n",
      "Epoch 00061: val_categorical_accuracy did not improve from 0.60490\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.1538 - categorical_accuracy: 0.8513 - val_loss: 3.5483 - val_categorical_accuracy: 0.5980\n",
      "Epoch 62/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.1495 - categorical_accuracy: 0.8551\n",
      "Epoch 00062: val_categorical_accuracy did not improve from 0.60490\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.1505 - categorical_accuracy: 0.8546 - val_loss: 3.5404 - val_categorical_accuracy: 0.5974\n",
      "Epoch 63/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.1313 - categorical_accuracy: 0.8564\n",
      "Epoch 00063: val_categorical_accuracy improved from 0.60490 to 0.60923, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.1309 - categorical_accuracy: 0.8561 - val_loss: 3.5199 - val_categorical_accuracy: 0.6092\n",
      "Epoch 64/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.1165 - categorical_accuracy: 0.8603\n",
      "Epoch 00064: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.1163 - categorical_accuracy: 0.8607 - val_loss: 3.5298 - val_categorical_accuracy: 0.6047\n",
      "Epoch 65/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 3.1045 - categorical_accuracy: 0.8668\n",
      "Epoch 00065: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.1042 - categorical_accuracy: 0.8665 - val_loss: 3.5142 - val_categorical_accuracy: 0.6072\n",
      "Epoch 66/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.1080 - categorical_accuracy: 0.8513\n",
      "Epoch 00066: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.1066 - categorical_accuracy: 0.8517 - val_loss: 3.4972 - val_categorical_accuracy: 0.6086\n",
      "Epoch 67/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.0791 - categorical_accuracy: 0.8645\n",
      "Epoch 00067: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.0796 - categorical_accuracy: 0.8652 - val_loss: 3.5026 - val_categorical_accuracy: 0.6059\n",
      "Epoch 68/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.0718 - categorical_accuracy: 0.8711\n",
      "Epoch 00068: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.0702 - categorical_accuracy: 0.8713 - val_loss: 3.5164 - val_categorical_accuracy: 0.5957\n",
      "Epoch 69/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.0627 - categorical_accuracy: 0.8692\n",
      "Epoch 00069: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.0632 - categorical_accuracy: 0.8680 - val_loss: 3.4920 - val_categorical_accuracy: 0.6058\n",
      "Epoch 70/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 3.0681 - categorical_accuracy: 0.8619\n",
      "Epoch 00070: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.0676 - categorical_accuracy: 0.8619 - val_loss: 3.4966 - val_categorical_accuracy: 0.6048\n",
      "Epoch 71/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 3.0455 - categorical_accuracy: 0.8644\n",
      "Epoch 00071: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.0455 - categorical_accuracy: 0.8644 - val_loss: 3.4798 - val_categorical_accuracy: 0.6079\n",
      "Epoch 72/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 3.0396 - categorical_accuracy: 0.8682\n",
      "Epoch 00072: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.0410 - categorical_accuracy: 0.8681 - val_loss: 3.4777 - val_categorical_accuracy: 0.6079\n",
      "Epoch 73/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 3.0175 - categorical_accuracy: 0.8777\n",
      "Epoch 00073: val_categorical_accuracy did not improve from 0.60923\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 3.0179 - categorical_accuracy: 0.8783 - val_loss: 3.4781 - val_categorical_accuracy: 0.6082\n",
      "Epoch 74/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.0151 - categorical_accuracy: 0.8706\n",
      "Epoch 00074: val_categorical_accuracy improved from 0.60923 to 0.61212, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.0150 - categorical_accuracy: 0.8704 - val_loss: 3.4617 - val_categorical_accuracy: 0.6121\n",
      "Epoch 75/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 3.0110 - categorical_accuracy: 0.8736\n",
      "Epoch 00075: val_categorical_accuracy improved from 0.61212 to 0.61252, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 3.0119 - categorical_accuracy: 0.8737 - val_loss: 3.4558 - val_categorical_accuracy: 0.6125\n",
      "Epoch 76/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.9933 - categorical_accuracy: 0.8739\n",
      "Epoch 00076: val_categorical_accuracy improved from 0.61252 to 0.61366, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.9933 - categorical_accuracy: 0.8739 - val_loss: 3.4480 - val_categorical_accuracy: 0.6137\n",
      "Epoch 77/100\n",
      "210/216 [============================>.] - ETA: 0s - loss: 2.9870 - categorical_accuracy: 0.8743\n",
      "Epoch 00077: val_categorical_accuracy did not improve from 0.61366\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.9848 - categorical_accuracy: 0.8756 - val_loss: 3.4523 - val_categorical_accuracy: 0.6112\n",
      "Epoch 78/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 2.9810 - categorical_accuracy: 0.8761\n",
      "Epoch 00078: val_categorical_accuracy improved from 0.61366 to 0.61439, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.9808 - categorical_accuracy: 0.8767 - val_loss: 3.4340 - val_categorical_accuracy: 0.6144\n",
      "Epoch 79/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.9681 - categorical_accuracy: 0.8806\n",
      "Epoch 00079: val_categorical_accuracy did not improve from 0.61439\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.9681 - categorical_accuracy: 0.8806 - val_loss: 3.4455 - val_categorical_accuracy: 0.6084\n",
      "Epoch 80/100\n",
      "213/216 [============================>.] - ETA: 0s - loss: 2.9552 - categorical_accuracy: 0.8785\n",
      "Epoch 00080: val_categorical_accuracy did not improve from 0.61439\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.9552 - categorical_accuracy: 0.8785 - val_loss: 3.4398 - val_categorical_accuracy: 0.6118\n",
      "Epoch 81/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 2.9450 - categorical_accuracy: 0.8863\n",
      "Epoch 00081: val_categorical_accuracy improved from 0.61439 to 0.61700, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 35ms/step - loss: 2.9446 - categorical_accuracy: 0.8867 - val_loss: 3.4178 - val_categorical_accuracy: 0.6170\n",
      "Epoch 82/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 2.9381 - categorical_accuracy: 0.8815\n",
      "Epoch 00082: val_categorical_accuracy did not improve from 0.61700\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.9377 - categorical_accuracy: 0.8819 - val_loss: 3.4230 - val_categorical_accuracy: 0.6145\n",
      "Epoch 83/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 2.9339 - categorical_accuracy: 0.8845\n",
      "Epoch 00083: val_categorical_accuracy improved from 0.61700 to 0.61839, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.9337 - categorical_accuracy: 0.8848 - val_loss: 3.4078 - val_categorical_accuracy: 0.6184\n",
      "Epoch 84/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 2.9254 - categorical_accuracy: 0.8836\n",
      "Epoch 00084: val_categorical_accuracy improved from 0.61839 to 0.61891, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 34ms/step - loss: 2.9254 - categorical_accuracy: 0.8839 - val_loss: 3.4067 - val_categorical_accuracy: 0.6189\n",
      "Epoch 85/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.9138 - categorical_accuracy: 0.8867\n",
      "Epoch 00085: val_categorical_accuracy did not improve from 0.61891\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.9138 - categorical_accuracy: 0.8867 - val_loss: 3.4102 - val_categorical_accuracy: 0.6161\n",
      "Epoch 86/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 2.9009 - categorical_accuracy: 0.8850\n",
      "Epoch 00086: val_categorical_accuracy did not improve from 0.61891\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.9015 - categorical_accuracy: 0.8844 - val_loss: 3.4066 - val_categorical_accuracy: 0.6159\n",
      "Epoch 87/100\n",
      "210/216 [============================>.] - ETA: 0s - loss: 2.9052 - categorical_accuracy: 0.8810\n",
      "Epoch 00087: val_categorical_accuracy improved from 0.61891 to 0.62081, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.9045 - categorical_accuracy: 0.8813 - val_loss: 3.3866 - val_categorical_accuracy: 0.6208\n",
      "Epoch 88/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.8946 - categorical_accuracy: 0.8817\n",
      "Epoch 00088: val_categorical_accuracy improved from 0.62081 to 0.62447, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.8946 - categorical_accuracy: 0.8817 - val_loss: 3.3742 - val_categorical_accuracy: 0.6245\n",
      "Epoch 89/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 2.8802 - categorical_accuracy: 0.8880\n",
      "Epoch 00089: val_categorical_accuracy did not improve from 0.62447\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.8793 - categorical_accuracy: 0.8885 - val_loss: 3.3731 - val_categorical_accuracy: 0.6239\n",
      "Epoch 90/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.8750 - categorical_accuracy: 0.8907\n",
      "Epoch 00090: val_categorical_accuracy improved from 0.62447 to 0.62450, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.8750 - categorical_accuracy: 0.8907 - val_loss: 3.3731 - val_categorical_accuracy: 0.6245\n",
      "Epoch 91/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 2.8698 - categorical_accuracy: 0.8919\n",
      "Epoch 00091: val_categorical_accuracy did not improve from 0.62450\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.8691 - categorical_accuracy: 0.8924 - val_loss: 3.3735 - val_categorical_accuracy: 0.6234\n",
      "Epoch 92/100\n",
      "211/216 [============================>.] - ETA: 0s - loss: 2.8593 - categorical_accuracy: 0.8885\n",
      "Epoch 00092: val_categorical_accuracy improved from 0.62450 to 0.62474, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.8569 - categorical_accuracy: 0.8894 - val_loss: 3.3615 - val_categorical_accuracy: 0.6247\n",
      "Epoch 93/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.8411 - categorical_accuracy: 0.8941\n",
      "Epoch 00093: val_categorical_accuracy improved from 0.62474 to 0.62788, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.8411 - categorical_accuracy: 0.8941 - val_loss: 3.3587 - val_categorical_accuracy: 0.6279\n",
      "Epoch 94/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 2.8415 - categorical_accuracy: 0.8887\n",
      "Epoch 00094: val_categorical_accuracy did not improve from 0.62788\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.8399 - categorical_accuracy: 0.8896 - val_loss: 3.3564 - val_categorical_accuracy: 0.6249\n",
      "Epoch 95/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 2.8333 - categorical_accuracy: 0.8942\n",
      "Epoch 00095: val_categorical_accuracy did not improve from 0.62788\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.8333 - categorical_accuracy: 0.8933 - val_loss: 3.3521 - val_categorical_accuracy: 0.6259\n",
      "Epoch 96/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.8265 - categorical_accuracy: 0.8965\n",
      "Epoch 00096: val_categorical_accuracy improved from 0.62788 to 0.63058, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//trained_models//full_models/pavia_as_source_with 600 samples_from_each_class_in_training_set.h5\n",
      "216/216 [==============================] - 7s 33ms/step - loss: 2.8265 - categorical_accuracy: 0.8965 - val_loss: 3.3299 - val_categorical_accuracy: 0.6306\n",
      "Epoch 97/100\n",
      "212/216 [============================>.] - ETA: 0s - loss: 2.8247 - categorical_accuracy: 0.8921\n",
      "Epoch 00097: val_categorical_accuracy did not improve from 0.63058\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.8248 - categorical_accuracy: 0.8919 - val_loss: 3.3354 - val_categorical_accuracy: 0.6278\n",
      "Epoch 98/100\n",
      "214/216 [============================>.] - ETA: 0s - loss: 2.8151 - categorical_accuracy: 0.8931\n",
      "Epoch 00098: val_categorical_accuracy did not improve from 0.63058\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.8162 - categorical_accuracy: 0.8920 - val_loss: 3.3334 - val_categorical_accuracy: 0.6259\n",
      "Epoch 99/100\n",
      "216/216 [==============================] - ETA: 0s - loss: 2.7984 - categorical_accuracy: 0.8998\n",
      "Epoch 00099: val_categorical_accuracy did not improve from 0.63058\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.7984 - categorical_accuracy: 0.8998 - val_loss: 3.3341 - val_categorical_accuracy: 0.6269\n",
      "Epoch 100/100\n",
      "215/216 [============================>.] - ETA: 0s - loss: 2.7850 - categorical_accuracy: 0.9029\n",
      "Epoch 00100: val_categorical_accuracy did not improve from 0.63058\n",
      "216/216 [==============================] - 7s 32ms/step - loss: 2.7849 - categorical_accuracy: 0.9030 - val_loss: 3.3266 - val_categorical_accuracy: 0.6241\n",
      "1018/1018 [==============================] - 5s 5ms/step - loss: 3.3266 - categorical_accuracy: 0.6241\n",
      "Test Accuracy =  0.6240670680999756\n",
      "1018/1018 [==============================] - 4s 4ms/step\n",
      "Confusion Matrix for Training Set Size 600 [[3263    0   22 1218    0    0  445  256  171]\n",
      " [   0 9665  650 2672    0 1258    0  213    4]\n",
      " [   0    0  574    0    0    0    1  542   25]\n",
      " [  34   46   12 1895    0   11   11   17  228]\n",
      " [   0    0    0    0  706    0    0    0   39]\n",
      " [   0 2259    4    0    0 2164    0    0    2]\n",
      " [  95    0    0    2    0    0  633    0    0]\n",
      " [  31    0 1289   55    0    1  136 1192  378]\n",
      " [   0    0    0   77    0   26    0   10  227]]\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 10, 10, 64)   4160        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10, 10, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 10, 10, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 10, 10, 8)    584         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 10, 10, 8)    584         lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 10, 10, 8)    584         lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 10, 10, 8)    584         lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 10, 8)    584         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 10, 10, 8)    584         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 10, 10, 8)    584         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 10, 10, 8)    0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 10, 10, 8)    584         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 10, 10, 8)    584         lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 10, 10, 8)    584         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 10, 10, 8)    584         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 10, 10, 8)    584         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 10, 10, 8)    584         lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 10, 10, 8)    584         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 10, 10, 8)    584         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 10, 10, 8)    584         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10, 10, 64)   0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10, 10, 64)   256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10, 10, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 10, 10, 128)  8320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 10, 10, 128)  8320        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10, 10, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10, 10, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10, 10, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10, 10, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10, 10, 128)  0           activation_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10, 10, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 68,864\n",
      "Trainable params: 67,968\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "indian_pines__source_model_with_overlap_ratio_1_and_5_epochs = Training(training_set_size = [200],\n",
    "                                                                 classes = classes,\n",
    "                                                                 cube_size = 20,\n",
    "                                                                 overlap_ratio = 1,\n",
    "                                                                 data = data,\n",
    "                                                                 ground_truth = ground_truth,\n",
    "                                                                 batch_size = 20,\n",
    "                                                                 channels = 64,\n",
    "                                                                 epochs = 5,\n",
    "                                                                 Verbosity = 1,\n",
    "                                                                 accuracies = [],\n",
    "                                                                 learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDR2MFQDqDdg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SGCNN8_model_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
