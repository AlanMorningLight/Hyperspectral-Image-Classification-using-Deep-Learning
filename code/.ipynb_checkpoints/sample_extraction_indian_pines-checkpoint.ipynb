{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tensorflow.keras.layers import Input, Add, Dense, ReLU, Activation, ZeroPadding3D, Lambda, BatchNormalization \n",
    "from tensorflow.keras.layers import Flatten, Conv3D, Conv2D, concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hyperspectral Dataset - Pavia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uIndianPines = sio.loadmat('Indian_pines_corrected.mat')\n",
    "gt_IndianPines = sio.loadmat('Indian_pines_gt.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = uIndianPines['indian_pines_corrected']\n",
    "ground_truth = gt_IndianPines['indian_pines_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 145)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distrubution of samples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class  samples\n",
       "0       0    10776\n",
       "1       1       46\n",
       "2       2     1428\n",
       "3       3      830\n",
       "4       4      237\n",
       "5       5      483\n",
       "6       6      730\n",
       "7       7       28\n",
       "8       8      478\n",
       "9       9       20\n",
       "10     10      972\n",
       "11     11     2455\n",
       "12     12      593\n",
       "13     13      205\n",
       "14     14     1265\n",
       "15     15      386\n",
       "16     16       93"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_distribution = pd.DataFrame(np.unique(ground_truth, return_counts = True))\n",
    "class_distribution = class_distribution.transpose()\n",
    "class_distribution.columns = ['class','samples']\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes , counts = np.unique(ground_truth, return_counts = True)\n",
    "classes = classes[1:] ## Not considering background\n",
    "classes = classes[[1,2,4,5,7,9,10,11,13]] ## Dropping classes with small number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  5,  6,  8, 10, 11, 12, 14], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick samples belonging to all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pick_samples_from_class(Class, cube_size, data, ground_truth, cubes, output_class, overlap_ratio, channels):\n",
    "    \n",
    "    ## Get row and column position from ground truth image for class\n",
    "    class_indices = np.where(ground_truth == Class)\n",
    "    \n",
    "    ## Remove border position class samples\n",
    "    class_cube_positions = [[class_indices[0][i], class_indices[1][i]] for i in range(len(class_indices[0])) \n",
    "                        if len(ground_truth) - np.ceil(cube_size / 2) > class_indices[0][i] > np.ceil(cube_size / 2) \n",
    "                        and len(ground_truth[0]) - np.ceil(cube_size / 2) > class_indices[1][i] > np.ceil(cube_size / 2)]\n",
    "    \n",
    "    #print('Length of class positions', len(class_cube_positions))\n",
    "    \n",
    "    extracted_cubes = [[class_cube_positions[0][0], class_cube_positions[0][1]]]\n",
    "    \n",
    "    ## Form the first cube for this class\n",
    "    cubes.append(np.array(data[class_cube_positions[0][0] - int(cube_size / 2):class_cube_positions[0][0] + int(cube_size / 2),\n",
    "                       (class_cube_positions[0][1] - int(cube_size / 2)):class_cube_positions[0][1] + int(cube_size / 2),\n",
    "                         :channels]))\n",
    "    \n",
    "    ## Output class value\n",
    "    output_class.append(Class)\n",
    "        \n",
    "    ## Pick cube/sample if it satisfies the criteria for the overlap ratio\n",
    "    for i in range(1, len(class_cube_positions)):\n",
    "        \n",
    "        distance_vector = [] ## Calculate distance from existing sample to the next candiddate cube sample\n",
    "        \n",
    "        for k in range(len(extracted_cubes)):\n",
    "            \n",
    "            distance = math.sqrt((class_cube_positions[i][0] - extracted_cubes[k][0]) ** 2 + \n",
    "                                 (class_cube_positions[i][1] - extracted_cubes[k][1]) ** 2)\n",
    "            \n",
    "            distance_vector.append(distance)\n",
    "            \n",
    "        if np.min(distance_vector) > int(cube_size * (1 - overlap_ratio)):\n",
    "            \n",
    "            cubes.append(np.array(data[class_cube_positions[i][0] - int(cube_size / 2):class_cube_positions[i][0] + int(cube_size / 2),\n",
    "                                      (class_cube_positions[i][1] - int(cube_size / 2)):class_cube_positions[i][1] + int(cube_size / 2),\n",
    "                                      :channels]))\n",
    "            \n",
    "            output_class.append(Class)\n",
    "            extracted_cubes.append([class_cube_positions[i][0], class_cube_positions[i][1]])\n",
    "            \n",
    "    return cubes, output_class, extracted_cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and combine samples from all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_samples_from_all_classes(classes, cube_size, data, ground_truth, cubes, output_class, overlap_ratio, channels):\n",
    "    \n",
    "    class_samples = []\n",
    "    \n",
    "    for Class in classes:\n",
    "        cubes, output_class, extracted_cubes = pick_samples_from_class(Class, cube_size, data, ground_truth, cubes, \n",
    "                                                                       output_class,overlap_ratio, channels)\n",
    "        class_samples.append(len(extracted_cubes))\n",
    "    \n",
    "    cubes = np.array(cubes)\n",
    "    output_class = np.array(output_class)\n",
    "    \n",
    "    print('Class Samples : ', class_samples)\n",
    "    \n",
    "    return cubes, output_class, class_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubes, output_class, class_samples = collect_samples_from_all_classes(classes = classes, \n",
    "                                                                      cube_size = 19, \n",
    "                                                                      data = data, \n",
    "                                                                      ground_truth = ground_truth, \n",
    "                                                                      cubes = [], \n",
    "                                                                      output_class = [], \n",
    "                                                                      overlap_ratio = 1, \n",
    "                                                                      channels = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_and_test_set(training_samples_from_each_class, \n",
    "                          class_samples, cubes, output_class):\n",
    "    \n",
    "    class_2_samples = cubes[np.where(output_class == 2)[0]]\n",
    "    class_2_labels = output_class[np.where(output_class == 2)[0]]\n",
    "\n",
    "    class_3_samples = cubes[np.where(output_class == 3)[0]]\n",
    "    class_3_labels = output_class[np.where(output_class == 3)[0]]\n",
    "\n",
    "    class_5_samples = cubes[np.where(output_class == 5)[0]]\n",
    "    class_5_labels = output_class[np.where(output_class == 5)[0]]\n",
    "\n",
    "    class_6_samples = cubes[np.where(output_class == 6)[0]]\n",
    "    class_6_labels = output_class[np.where(output_class == 6)[0]]\n",
    "\n",
    "    class_8_samples = cubes[np.where(output_class == 8)[0]]\n",
    "    class_8_labels = output_class[np.where(output_class == 8)[0]]\n",
    "\n",
    "    class_10_samples = cubes[np.where(output_class == 10)[0]]\n",
    "    class_10_labels = output_class[np.where(output_class == 10)[0]]\n",
    "    \n",
    "    class_11_samples = cubes[np.where(output_class == 11)[0]]\n",
    "    class_11_labels = output_class[np.where(output_class == 11)[0]]\n",
    "    \n",
    "    class_12_samples = cubes[np.where(output_class == 12)[0]]\n",
    "    class_12_labels = output_class[np.where(output_class == 12)[0]]\n",
    "    \n",
    "    class_14_samples = cubes[np.where(output_class == 14)[0]]\n",
    "    class_14_labels = output_class[np.where(output_class == 14)[0]]\n",
    "\n",
    "\n",
    "    #print(len(class_1_samples), len(class_1_labels))\n",
    "    #print(len(class_2_samples), len(class_2_labels))\n",
    "    #print(len(class_3_samples), len(class_3_labels))\n",
    "    #print(len(class_4_samples), len(class_4_labels))\n",
    "    #print(len(class_5_samples), len(class_5_labels))\n",
    "    #print(len(class_6_samples), len(class_6_labels))\n",
    "    #print(len(class_7_samples), len(class_7_labels))\n",
    "    #print(len(class_8_samples), len(class_8_labels))\n",
    "    #print(len(class_9_samples), len(class_9_labels))\n",
    "\n",
    "    class_samples_collection = [class_2_samples, class_3_samples, class_5_samples, class_6_samples,\n",
    "                               class_8_samples, class_10_samples, class_11_samples, class_12_samples, class_14_samples]\n",
    "\n",
    "    class_labels_collection = [class_2_samples, class_3_samples, class_5_samples, class_6_samples,\n",
    "                               class_8_samples, class_10_samples, class_11_samples, class_12_samples, class_14_samples]\n",
    "\n",
    "    # Training & Test Set Arrays\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "\n",
    "    # Get Training set size samples from each class\n",
    "    for samples in class_samples_collection:\n",
    "        \n",
    "        X_train.append(samples[0:training_samples_from_each_class])\n",
    "        \n",
    "        X_test.append(samples[training_samples_from_each_class:])\n",
    "        \n",
    "    # Get output labels\n",
    "    for labels in class_labels_collection:\n",
    "        y_train.append(labels[0:training_samples_from_each_class])\n",
    "        \n",
    "        y_test.append(labels[training_samples_from_each_class :])\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis = 0)\n",
    "    X_test = np.concatenate(X_test, axis = 0)\n",
    "\n",
    "    y_train = np.concatenate(y_train, axis = 0)\n",
    "    y_test = np.concatenate(y_test, axis = 0)\n",
    "\n",
    "    \n",
    "    ## Shuffle Training Set\n",
    "    samples_train = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(samples_train)\n",
    "\n",
    "    X_train = X_train[samples_train]\n",
    "    y_train = y_train[samples_train]\n",
    "\n",
    "\n",
    "    ## Shuffle Test Set\n",
    "    samples_test = np.arange(X_test.shape[0])\n",
    "    np.random.shuffle(samples_test)\n",
    "\n",
    "    X_test = X_test[samples_test]\n",
    "    y_test = y_test[samples_test]\n",
    "\n",
    "    # Get counts(samples) of each class in test set\n",
    "    values_test_set, counts_test_set = np.unique(y_test, return_counts = True)\n",
    "    values_training_set, counts_training_set = np.unique(y_train, return_counts = True)\n",
    "\n",
    "\n",
    "    print(\"Samples per class: \" + str(class_samples) + '\\n'\n",
    "          \"Total number of samples is \" + str(np.sum(class_samples)) + '.\\n')\n",
    "    \n",
    "    print(\"unique classes in training set: \" + str(values_training_set) + '\\n'\n",
    "          \"Total number of samples in training set is \" + str(np.sum(counts_training_set)) + '.\\n'\n",
    "          \"Samples per class in training set: \" + str(counts_training_set) + '\\n')\n",
    "\n",
    "    print(\"unique classes in test set: \" + str(values_test_set) + '\\n'\n",
    "          \"Total number of samples in test set is \" + str(np.sum(counts_test_set)) + '.\\n'\n",
    "          \"Samples per class in test set: \" + str(counts_test_set) + '\\n')\n",
    "    print('\\n')\n",
    "\n",
    "    ## one hot encode labels\n",
    "    onehot_encoder = OneHotEncoder(sparse = False)\n",
    "\n",
    "    y_train = y_train.reshape(len(y_train), 1)\n",
    "    y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "    y_train = onehot_encoder.fit_transform(y_train)\n",
    "    y_test = onehot_encoder.fit_transform(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, counts_test_set, class_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_extraction(classes, cube_size, data, ground_truth, cubes, output_class, training_samples_from_each_class,\n",
    "                      validation_samples_from_each_class, test_samples_from_each_class, overlap_ratio, channels):\n",
    "    \n",
    "    cubes, output_class, class_samples = collect_samples_from_all_classes(classes, \n",
    "                                                                      cube_size, \n",
    "                                                                      data,  \n",
    "                                                                      ground_truth, \n",
    "                                                                      cubes, \n",
    "                                                                      output_class , \n",
    "                                                                      overlap_ratio, \n",
    "                                                                      channels)\n",
    "    \n",
    "    X_train, X_val, X_test, y_train, y_val, y_test, counts, class_samples = training_and_test_set(\n",
    "                                                                            training_samples_from_each_class,\n",
    "                                                                            validation_samples_from_each_class,\n",
    "                                                                            test_samples_from_each_class,\n",
    "                                                                            class_samples, \n",
    "                                                                            cubes,\n",
    "                                                                            output_class)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test, counts, class_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training and test data by extracting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, class_samples, counts = sample_extraction(classes = classes, \n",
    "                                                                            cube_size = 20, \n",
    "                                                                            data = data, \n",
    "                                                                            ground_truth = ground_truth, \n",
    "                                                                            cubes = [], \n",
    "                                                                            output_class = [], \n",
    "                                                                            for_training_set = 150,\n",
    "                                                                            overlap_ratio = 1, \n",
    "                                                                            channels = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initial_convolution_block(samples):\n",
    "    \n",
    "    X = Conv2D(64, (3, 3), strides = (2, 2), padding = 'same', name = 'conv_initial',\n",
    "               input_shape = (20, 20, 64))(samples)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = ReLU()(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cardinality = 8 #Paths\n",
    "def group_convolution(y, channels):\n",
    "    \n",
    "    assert not channels % cardinality\n",
    "    \n",
    "    d = channels // cardinality\n",
    "\n",
    "    # in a grouped convolution layer, input and output channels are divided into `cardinality` groups,\n",
    "    # and convolutions are separately performed within each group\n",
    "    \n",
    "    groups = []\n",
    "    \n",
    "    for j in range(cardinality):\n",
    "        \n",
    "        if j % 2 == 0:\n",
    "            \n",
    "            no_dilation = Lambda(lambda z: z[:, :, :, j * d:j * d + d])(y)\n",
    "            groups.append(Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same')(no_dilation))\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            dilation_group = Lambda(lambda z: z[:, :, :, j * d:j * d + d])(y)\n",
    "            x = Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same', dilation_rate = 1)(dilation_group)\n",
    "            x = Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same', dilation_rate = 3)(x)\n",
    "            x = Conv2D(d, kernel_size=(3, 3), strides = (1,1), padding='same', dilation_rate = 5)(x)\n",
    "            groups.append(x)\n",
    "\n",
    "            \n",
    "    # the grouped convolutional layer concatenates them as the outputs of the layer\n",
    "    y = concatenate(groups)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def SG_Unit(X):\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "    l2_ = 0.01\n",
    "    X = Conv2D(64, (1, 1), kernel_regularizer = regularizers.l2(l2_), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = group_convolution(X, 64)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X = Conv2D(128, (1, 1), kernel_regularizer = regularizers.l2(l2_), padding=\"same\")(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    X_shortcut = Conv2D(128, (1, 1), kernel_regularizer = regularizers.l2(l2_), padding=\"same\")(X_shortcut)\n",
    "    X_shortcut = BatchNormalization()(X_shortcut)\n",
    "    X_shortcut = Activation('relu')(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model(input_shape, classes):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    X = initial_convolution_block(X_input)\n",
    "    X = SG_Unit(X)\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    X = Dense(256, input_dim = X.shape, activation='relu', name = 'fc_256', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = Dense(classes, input_dim = X.shape, activation = 'softmax')(X)\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training(training_set_size,\n",
    "             validation_samples_from_each_class,\n",
    "             test_samples_from_each_class,\n",
    "             classes,\n",
    "             cube_size,\n",
    "             overlap_ratio,\n",
    "             data,\n",
    "             ground_truth,\n",
    "             batch_size,\n",
    "             channels,\n",
    "             epochs,\n",
    "             Verbosity,\n",
    "             accuracies,\n",
    "             learning_rate):\n",
    "    \n",
    "    for i in range(len(training_set_size)):\n",
    "        \n",
    "        print(\"\\n===========================================================================================================\\n\"\n",
    "              \"Model training starts for data with \" + str(int(training_set_size[i])) + \" samples from each class in training set\\n\"\n",
    "              \"==============================================================================================================\\n\")\n",
    "\n",
    "\n",
    "\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, counts, class_samples = sample_extraction(classes = classes, \n",
    "                                                                                cube_size = cube_size, \n",
    "                                                                                data = data, \n",
    "                                                                                ground_truth = ground_truth, \n",
    "                                                                                cubes = [], \n",
    "                                                                                output_class = [], \n",
    "                                                                                training_samples_from_each_class = training_set_size[i],\n",
    "                                                                                validation_samples_from_each_class = validation_samples_from_each_class,\n",
    "                                                                                test_samples_from_each_class = test_samples_from_each_class,\n",
    "                                                                                overlap_ratio = overlap_ratio, \n",
    "                                                                                channels = channels)\n",
    "        print('X_train => ' + str(X_train.shape) + '\\n' +\n",
    "              'X_val =>' + str(X_val.shape) + '\\n' +\n",
    "              'X_test  => ' + str(X_test.shape) + '\\n' +\n",
    "              'y_train => ' + str(y_train.shape) + '\\n' +\n",
    "              'y_val =>' + str(y_val.shape) + '\\n' +\n",
    "              'y_test  => ' + str(y_test.shape) + '\\n')\n",
    "\n",
    "        X_train = np.array(X_train).astype(np.float32)\n",
    "        X_val = np.array(X_val).astype(np.float32)\n",
    "        X_test = np.array(X_test).astype(np.float32)\n",
    "\n",
    "        model_to_train = model(input_shape = X_train[0].shape, classes = len(classes))\n",
    "        model_to_train.summary()\n",
    "\n",
    "        # save best model\n",
    "        model_checkpoint = ModelCheckpoint('pavia_as_source_with ' \n",
    "                                           + str(int(training_set_size[i])) \n",
    "                                           + ' samples_from_each_class_in_training_set.h5',\n",
    "                                            monitor = 'val_categorical_accuracy', \n",
    "                                            verbose = 1, \n",
    "                                            save_best_only = True)\n",
    "\n",
    "        model_to_train.compile(optimizer = keras.optimizers.SGD(learning_rate = learning_rate), \n",
    "                                                     loss = 'categorical_crossentropy', \n",
    "                                                     metrics = ['categorical_accuracy'])\n",
    "\n",
    "        model_to_train.fit(X_train, y_train, \n",
    "                          epochs = epochs, \n",
    "                          batch_size = batch_size,\n",
    "                          #validation_split = 0.2,\n",
    "                          validation_data = (X_val, y_val),\n",
    "                          verbose = Verbosity, \n",
    "                          callbacks = [model_checkpoint])\n",
    "\n",
    "        evaluation = model_to_train.evaluate(X_test, y_test)\n",
    "        print(\"Test Accuracy = \", evaluation[1])\n",
    "\n",
    "        y_pred = model_to_train.predict(X_test, verbose = 1)\n",
    "        confusion_matrix = sklearn.metrics.confusion_matrix(np.argmax(y_test, axis = 1), np.argmax(y_pred, axis = 1))\n",
    "        \n",
    "        print(\"Confusion Matrix for Training Set Size \" + str(training_set_size[i]), confusion_matrix)\n",
    "\n",
    "        accuracies.append(evaluation[1] * 100)\n",
    "\n",
    "    print(model_to_train.layers)\n",
    "                        \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = Training(training_set_size = [600],\n",
    "                  validation_samples_from_each_class = 150,\n",
    "                  test_samples_from_each_class = 150,\n",
    "                  classes = classes,\n",
    "                  cube_size = 20,\n",
    "                  overlap_ratio = 1,\n",
    "                  data = data,\n",
    "                  ground_truth = ground_truth,\n",
    "                  batch_size = 25,\n",
    "                  channels = 64,\n",
    "                  epochs = 50,\n",
    "                  Verbosity = 1,\n",
    "                  accuracies = [],\n",
    "                  learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
