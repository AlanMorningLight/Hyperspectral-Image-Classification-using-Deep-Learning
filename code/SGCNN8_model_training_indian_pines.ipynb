{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGCNN8_model_training_indian_pines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_nG8NlV-lz-"
      },
      "source": [
        "## Set up google colab environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bq_kqWQtg0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93182e48-782a-4e8d-d674-afc74b5eb166"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK95udG-qHAy"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Hyperspectral_Image_Classification/code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn-wRAH4t3WY"
      },
      "source": [
        "from utils import *\n",
        "import scipy.io as sio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky2Qzuw_qDdS"
      },
      "source": [
        "## Load Indian Pines Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svwF-yzh-l0N"
      },
      "source": [
        "uIndianPines = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/code/Datasets/Indian_pines_corrected.mat')\n",
        "gt_IndianPines = sio.loadmat('/content/drive/My Drive/Hyperspectral_Image_Classification/code/Datasets/Indian_pines_gt.mat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGpYj4P-l0N"
      },
      "source": [
        "data = uIndianPines['indian_pines_corrected']\n",
        "ground_truth = gt_IndianPines['indian_pines_gt']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdMIhVj9qDdb",
        "outputId": "c4f7dbdb-70cd-4790-c991-0aa1f8ae0b09"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145, 145, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAjjOxj3qDdb",
        "outputId": "3196903f-0057-44c2-d7a0-df07e10fc3e8"
      },
      "source": [
        "ground_truth.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145, 145)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WxjgWNGqDdc"
      },
      "source": [
        "## Distrubution of samples for each class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "yFA7eqA7qDdd",
        "outputId": "2d85d2dd-fc76-469b-c78f-518e17fe022f"
      },
      "source": [
        "class_distribution = pd.DataFrame(np.unique(ground_truth, return_counts = True))\n",
        "class_distribution = class_distribution.transpose()\n",
        "class_distribution.columns = ['class','samples']\n",
        "class_distribution"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>samples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>10776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>2455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>1265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    class  samples\n",
              "0       0    10776\n",
              "1       1       46\n",
              "2       2     1428\n",
              "3       3      830\n",
              "4       4      237\n",
              "5       5      483\n",
              "6       6      730\n",
              "7       7       28\n",
              "8       8      478\n",
              "9       9       20\n",
              "10     10      972\n",
              "11     11     2455\n",
              "12     12      593\n",
              "13     13      205\n",
              "14     14     1265\n",
              "15     15      386\n",
              "16     16       93"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zTsq-yPqDdd",
        "outputId": "a72ca01b-6373-42ca-dda7-1760b82d5af6"
      },
      "source": [
        "classes , counts = np.unique(ground_truth, return_counts = True)\n",
        "classes = classes[[2,3,5,6,8,10,11,12,14]] ## Dropping classes with small number of samples\n",
        "classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  3,  5,  6,  8, 10, 11, 12, 14], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_PESscnqDde"
      },
      "source": [
        "## Train model with overlap ratio 1 for 100 epochs (200 samples from each class in training set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vc1sJwRqDdf",
        "outputId": "6086d1af-78ee-44cb-c334-aea6479fe4b5"
      },
      "source": [
        "indian_pines_source_model_with_overlap_ratio_1_and_100_epochs = pretrain_source_models(training_set_size = [200],\n",
        "                                                                                       test_samples_from_each_class = 50,\n",
        "                                                                                        classes = classes,\n",
        "                                                                                        cube_size = 20,\n",
        "                                                                                        overlap_ratio = 1,\n",
        "                                                                                        data = data,\n",
        "                                                                                        ground_truth = ground_truth,\n",
        "                                                                                        batch_size = 20,\n",
        "                                                                                        channels = 64,\n",
        "                                                                                        epochs = 100,\n",
        "                                                                                        Verbosity = 1,\n",
        "                                                                                        accuracies = [],\n",
        "                                                                                        learning_rate = 0.0001,\n",
        "                                                                                        source = 'indian_pines')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=============================================================================================================\n",
            "Model training starts for data with 200 samples from each class in training set\n",
            "==============================================================================================================\n",
            "\n",
            "Class Samples :  [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n",
            "Samples per class: [1368, 523, 323, 730, 356, 837, 2224, 460, 1085]\n",
            "Total number of samples is 7906.\n",
            "\n",
            "unique classes in training set: [ 2  3  5  6  8 10 11 12 14]\n",
            "Total number of samples in training set is 1800.\n",
            "Samples per class in training set: [200 200 200 200 200 200 200 200 200]\n",
            "\n",
            "unique classes in test set: [ 2  3  5  6  8 10 11 12 14]\n",
            "Total number of samples in test set is 450.\n",
            "Samples per class in test set: [50 50 50 50 50 50 50 50 50]\n",
            "\n",
            "\n",
            "\n",
            "X_train => (1800, 20, 20, 64)\n",
            "X_test  => (450, 20, 20, 64)\n",
            "y_train => (1800, 9)\n",
            "y_test  => (450, 9)\n",
            "\n",
            "Groud convolution output  (None, 5, 5, 64)\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 5, 5, 128)    0           activation_2[0][0]               \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "final_fully_connected (Dense)   (None, 256)          33024       global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 9)            2313        final_fully_connected[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 104,201\n",
            "Trainable params: 103,305\n",
            "Non-trainable params: 896\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 4.7976 - categorical_accuracy: 0.1131\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.09778, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 14ms/step - loss: 4.8028 - categorical_accuracy: 0.1111 - val_loss: 5.0868 - val_categorical_accuracy: 0.0978\n",
            "Epoch 2/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 4.7194 - categorical_accuracy: 0.1087\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.09778 to 0.11111, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.7113 - categorical_accuracy: 0.1111 - val_loss: 4.7010 - val_categorical_accuracy: 0.1111\n",
            "Epoch 3/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 4.6328 - categorical_accuracy: 0.1093\n",
            "Epoch 00003: val_categorical_accuracy did not improve from 0.11111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.6355 - categorical_accuracy: 0.1094 - val_loss: 4.6376 - val_categorical_accuracy: 0.0956\n",
            "Epoch 4/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 4.5822 - categorical_accuracy: 0.1100\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.11111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.5807 - categorical_accuracy: 0.1117 - val_loss: 4.5958 - val_categorical_accuracy: 0.0844\n",
            "Epoch 5/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 4.5415 - categorical_accuracy: 0.1262\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.11111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.5379 - categorical_accuracy: 0.1289 - val_loss: 4.5641 - val_categorical_accuracy: 0.0933\n",
            "Epoch 6/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 4.5039 - categorical_accuracy: 0.1494\n",
            "Epoch 00006: val_categorical_accuracy did not improve from 0.11111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.5060 - categorical_accuracy: 0.1467 - val_loss: 4.5357 - val_categorical_accuracy: 0.1022\n",
            "Epoch 7/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 4.4661 - categorical_accuracy: 0.1859\n",
            "Epoch 00007: val_categorical_accuracy improved from 0.11111 to 0.11778, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.4704 - categorical_accuracy: 0.1856 - val_loss: 4.5128 - val_categorical_accuracy: 0.1178\n",
            "Epoch 8/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 4.4425 - categorical_accuracy: 0.2250\n",
            "Epoch 00008: val_categorical_accuracy improved from 0.11778 to 0.14444, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.4418 - categorical_accuracy: 0.2278 - val_loss: 4.4865 - val_categorical_accuracy: 0.1444\n",
            "Epoch 9/100\n",
            "83/90 [==========================>...] - ETA: 0s - loss: 4.4189 - categorical_accuracy: 0.2705\n",
            "Epoch 00009: val_categorical_accuracy improved from 0.14444 to 0.16000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.4209 - categorical_accuracy: 0.2694 - val_loss: 4.4631 - val_categorical_accuracy: 0.1600\n",
            "Epoch 10/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 4.3937 - categorical_accuracy: 0.3188\n",
            "Epoch 00010: val_categorical_accuracy improved from 0.16000 to 0.19111, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.3954 - categorical_accuracy: 0.3172 - val_loss: 4.4420 - val_categorical_accuracy: 0.1911\n",
            "Epoch 11/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 4.3768 - categorical_accuracy: 0.3565\n",
            "Epoch 00011: val_categorical_accuracy improved from 0.19111 to 0.20000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.3778 - categorical_accuracy: 0.3572 - val_loss: 4.4217 - val_categorical_accuracy: 0.2000\n",
            "Epoch 12/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 4.3584 - categorical_accuracy: 0.3894\n",
            "Epoch 00012: val_categorical_accuracy improved from 0.20000 to 0.24000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.3587 - categorical_accuracy: 0.3911 - val_loss: 4.4021 - val_categorical_accuracy: 0.2400\n",
            "Epoch 13/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 4.3414 - categorical_accuracy: 0.4133\n",
            "Epoch 00013: val_categorical_accuracy improved from 0.24000 to 0.25333, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.3414 - categorical_accuracy: 0.4133 - val_loss: 4.3854 - val_categorical_accuracy: 0.2533\n",
            "Epoch 14/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 4.3212 - categorical_accuracy: 0.4311\n",
            "Epoch 00014: val_categorical_accuracy improved from 0.25333 to 0.28444, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.3212 - categorical_accuracy: 0.4311 - val_loss: 4.3712 - val_categorical_accuracy: 0.2844\n",
            "Epoch 15/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 4.3079 - categorical_accuracy: 0.4333\n",
            "Epoch 00015: val_categorical_accuracy improved from 0.28444 to 0.29556, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 4.3090 - categorical_accuracy: 0.4344 - val_loss: 4.3559 - val_categorical_accuracy: 0.2956\n",
            "Epoch 16/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 4.2937 - categorical_accuracy: 0.4580\n",
            "Epoch 00016: val_categorical_accuracy improved from 0.29556 to 0.31556, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 4.2915 - categorical_accuracy: 0.4617 - val_loss: 4.3409 - val_categorical_accuracy: 0.3156\n",
            "Epoch 17/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 4.2726 - categorical_accuracy: 0.4706\n",
            "Epoch 00017: val_categorical_accuracy improved from 0.31556 to 0.32889, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.2726 - categorical_accuracy: 0.4706 - val_loss: 4.3258 - val_categorical_accuracy: 0.3289\n",
            "Epoch 18/100\n",
            "83/90 [==========================>...] - ETA: 0s - loss: 4.2639 - categorical_accuracy: 0.4747\n",
            "Epoch 00018: val_categorical_accuracy improved from 0.32889 to 0.34222, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.2614 - categorical_accuracy: 0.4728 - val_loss: 4.3108 - val_categorical_accuracy: 0.3422\n",
            "Epoch 19/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 4.2420 - categorical_accuracy: 0.4866\n",
            "Epoch 00019: val_categorical_accuracy improved from 0.34222 to 0.35333, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.2409 - categorical_accuracy: 0.4856 - val_loss: 4.2957 - val_categorical_accuracy: 0.3533\n",
            "Epoch 20/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 4.2310 - categorical_accuracy: 0.4863\n",
            "Epoch 00020: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.2310 - categorical_accuracy: 0.4889 - val_loss: 4.2824 - val_categorical_accuracy: 0.3467\n",
            "Epoch 21/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 4.2178 - categorical_accuracy: 0.4977\n",
            "Epoch 00021: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.2170 - categorical_accuracy: 0.4967 - val_loss: 4.2697 - val_categorical_accuracy: 0.3511\n",
            "Epoch 22/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 4.2046 - categorical_accuracy: 0.5018\n",
            "Epoch 00022: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.2013 - categorical_accuracy: 0.5056 - val_loss: 4.2542 - val_categorical_accuracy: 0.3444\n",
            "Epoch 23/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 4.1876 - categorical_accuracy: 0.5068\n",
            "Epoch 00023: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.1860 - categorical_accuracy: 0.5089 - val_loss: 4.2399 - val_categorical_accuracy: 0.3400\n",
            "Epoch 24/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 4.1710 - categorical_accuracy: 0.5125\n",
            "Epoch 00024: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.1714 - categorical_accuracy: 0.5128 - val_loss: 4.2261 - val_categorical_accuracy: 0.3378\n",
            "Epoch 25/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 4.1597 - categorical_accuracy: 0.5294\n",
            "Epoch 00025: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.1591 - categorical_accuracy: 0.5289 - val_loss: 4.2109 - val_categorical_accuracy: 0.3444\n",
            "Epoch 26/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 4.1452 - categorical_accuracy: 0.5399\n",
            "Epoch 00026: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.1465 - categorical_accuracy: 0.5367 - val_loss: 4.1993 - val_categorical_accuracy: 0.3444\n",
            "Epoch 27/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 4.1330 - categorical_accuracy: 0.5324\n",
            "Epoch 00027: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.1314 - categorical_accuracy: 0.5344 - val_loss: 4.1866 - val_categorical_accuracy: 0.3467\n",
            "Epoch 28/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 4.1201 - categorical_accuracy: 0.5455\n",
            "Epoch 00028: val_categorical_accuracy did not improve from 0.35333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 4.1201 - categorical_accuracy: 0.5456 - val_loss: 4.1735 - val_categorical_accuracy: 0.3422\n",
            "Epoch 29/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 4.1092 - categorical_accuracy: 0.5455\n",
            "Epoch 00029: val_categorical_accuracy improved from 0.35333 to 0.36000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 12ms/step - loss: 4.1089 - categorical_accuracy: 0.5467 - val_loss: 4.1573 - val_categorical_accuracy: 0.3600\n",
            "Epoch 30/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 4.0965 - categorical_accuracy: 0.5567\n",
            "Epoch 00030: val_categorical_accuracy improved from 0.36000 to 0.36667, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.0965 - categorical_accuracy: 0.5567 - val_loss: 4.1447 - val_categorical_accuracy: 0.3667\n",
            "Epoch 31/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 4.0819 - categorical_accuracy: 0.5702\n",
            "Epoch 00031: val_categorical_accuracy improved from 0.36667 to 0.36889, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.0818 - categorical_accuracy: 0.5717 - val_loss: 4.1344 - val_categorical_accuracy: 0.3689\n",
            "Epoch 32/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 4.0659 - categorical_accuracy: 0.5816\n",
            "Epoch 00032: val_categorical_accuracy improved from 0.36889 to 0.38222, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.0651 - categorical_accuracy: 0.5839 - val_loss: 4.1230 - val_categorical_accuracy: 0.3822\n",
            "Epoch 33/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 4.0591 - categorical_accuracy: 0.5826\n",
            "Epoch 00033: val_categorical_accuracy improved from 0.38222 to 0.40000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.0591 - categorical_accuracy: 0.5828 - val_loss: 4.1116 - val_categorical_accuracy: 0.4000\n",
            "Epoch 34/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 4.0368 - categorical_accuracy: 0.5948\n",
            "Epoch 00034: val_categorical_accuracy improved from 0.40000 to 0.41111, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.0367 - categorical_accuracy: 0.5950 - val_loss: 4.1012 - val_categorical_accuracy: 0.4111\n",
            "Epoch 35/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 4.0311 - categorical_accuracy: 0.5977\n",
            "Epoch 00035: val_categorical_accuracy improved from 0.41111 to 0.42000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 4.0314 - categorical_accuracy: 0.5956 - val_loss: 4.0895 - val_categorical_accuracy: 0.4200\n",
            "Epoch 36/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 4.0223 - categorical_accuracy: 0.6101\n",
            "Epoch 00036: val_categorical_accuracy improved from 0.42000 to 0.43333, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 4.0220 - categorical_accuracy: 0.6089 - val_loss: 4.0772 - val_categorical_accuracy: 0.4333\n",
            "Epoch 37/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 4.0136 - categorical_accuracy: 0.6040\n",
            "Epoch 00037: val_categorical_accuracy improved from 0.43333 to 0.44222, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 4.0107 - categorical_accuracy: 0.6050 - val_loss: 4.0666 - val_categorical_accuracy: 0.4422\n",
            "Epoch 38/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.9977 - categorical_accuracy: 0.6045\n",
            "Epoch 00038: val_categorical_accuracy did not improve from 0.44222\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.9975 - categorical_accuracy: 0.6033 - val_loss: 4.0578 - val_categorical_accuracy: 0.4400\n",
            "Epoch 39/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.9872 - categorical_accuracy: 0.6150\n",
            "Epoch 00039: val_categorical_accuracy improved from 0.44222 to 0.45333, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.9872 - categorical_accuracy: 0.6150 - val_loss: 4.0470 - val_categorical_accuracy: 0.4533\n",
            "Epoch 40/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.9758 - categorical_accuracy: 0.6194\n",
            "Epoch 00040: val_categorical_accuracy improved from 0.45333 to 0.45778, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.9758 - categorical_accuracy: 0.6194 - val_loss: 4.0379 - val_categorical_accuracy: 0.4578\n",
            "Epoch 41/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 3.9667 - categorical_accuracy: 0.6218\n",
            "Epoch 00041: val_categorical_accuracy improved from 0.45778 to 0.46444, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.9646 - categorical_accuracy: 0.6256 - val_loss: 4.0291 - val_categorical_accuracy: 0.4644\n",
            "Epoch 42/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 3.9564 - categorical_accuracy: 0.6241\n",
            "Epoch 00042: val_categorical_accuracy improved from 0.46444 to 0.48000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.9581 - categorical_accuracy: 0.6211 - val_loss: 4.0180 - val_categorical_accuracy: 0.4800\n",
            "Epoch 43/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.9465 - categorical_accuracy: 0.6337\n",
            "Epoch 00043: val_categorical_accuracy did not improve from 0.48000\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.9473 - categorical_accuracy: 0.6317 - val_loss: 4.0057 - val_categorical_accuracy: 0.4778\n",
            "Epoch 44/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 3.9359 - categorical_accuracy: 0.6429\n",
            "Epoch 00044: val_categorical_accuracy improved from 0.48000 to 0.50222, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.9331 - categorical_accuracy: 0.6456 - val_loss: 3.9955 - val_categorical_accuracy: 0.5022\n",
            "Epoch 45/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.9189 - categorical_accuracy: 0.6528\n",
            "Epoch 00045: val_categorical_accuracy improved from 0.50222 to 0.50889, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.9195 - categorical_accuracy: 0.6533 - val_loss: 3.9873 - val_categorical_accuracy: 0.5089\n",
            "Epoch 46/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.9131 - categorical_accuracy: 0.6447\n",
            "Epoch 00046: val_categorical_accuracy improved from 0.50889 to 0.51111, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.9139 - categorical_accuracy: 0.6444 - val_loss: 3.9778 - val_categorical_accuracy: 0.5111\n",
            "Epoch 47/100\n",
            "83/90 [==========================>...] - ETA: 0s - loss: 3.9068 - categorical_accuracy: 0.6512\n",
            "Epoch 00047: val_categorical_accuracy did not improve from 0.51111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.9104 - categorical_accuracy: 0.6444 - val_loss: 3.9690 - val_categorical_accuracy: 0.5089\n",
            "Epoch 48/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 3.8903 - categorical_accuracy: 0.6569\n",
            "Epoch 00048: val_categorical_accuracy did not improve from 0.51111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.8928 - categorical_accuracy: 0.6550 - val_loss: 3.9585 - val_categorical_accuracy: 0.5089\n",
            "Epoch 49/100\n",
            "83/90 [==========================>...] - ETA: 0s - loss: 3.8812 - categorical_accuracy: 0.6608\n",
            "Epoch 00049: val_categorical_accuracy did not improve from 0.51111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.8807 - categorical_accuracy: 0.6600 - val_loss: 3.9466 - val_categorical_accuracy: 0.5089\n",
            "Epoch 50/100\n",
            "83/90 [==========================>...] - ETA: 0s - loss: 3.8664 - categorical_accuracy: 0.6669\n",
            "Epoch 00050: val_categorical_accuracy improved from 0.51111 to 0.51333, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.8676 - categorical_accuracy: 0.6667 - val_loss: 3.9376 - val_categorical_accuracy: 0.5133\n",
            "Epoch 51/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.8555 - categorical_accuracy: 0.6645\n",
            "Epoch 00051: val_categorical_accuracy improved from 0.51333 to 0.52889, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.8551 - categorical_accuracy: 0.6644 - val_loss: 3.9306 - val_categorical_accuracy: 0.5289\n",
            "Epoch 52/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.8487 - categorical_accuracy: 0.6727\n",
            "Epoch 00052: val_categorical_accuracy improved from 0.52889 to 0.53556, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.8473 - categorical_accuracy: 0.6728 - val_loss: 3.9201 - val_categorical_accuracy: 0.5356\n",
            "Epoch 53/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.8491 - categorical_accuracy: 0.6775\n",
            "Epoch 00053: val_categorical_accuracy improved from 0.53556 to 0.55111, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.8485 - categorical_accuracy: 0.6783 - val_loss: 3.9121 - val_categorical_accuracy: 0.5511\n",
            "Epoch 54/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.8161 - categorical_accuracy: 0.6876\n",
            "Epoch 00054: val_categorical_accuracy did not improve from 0.55111\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.8189 - categorical_accuracy: 0.6861 - val_loss: 3.9024 - val_categorical_accuracy: 0.5422\n",
            "Epoch 55/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 3.8314 - categorical_accuracy: 0.6667\n",
            "Epoch 00055: val_categorical_accuracy did not improve from 0.55111\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.8335 - categorical_accuracy: 0.6656 - val_loss: 3.8927 - val_categorical_accuracy: 0.5467\n",
            "Epoch 56/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.8051 - categorical_accuracy: 0.6912\n",
            "Epoch 00056: val_categorical_accuracy improved from 0.55111 to 0.55556, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.8099 - categorical_accuracy: 0.6856 - val_loss: 3.8829 - val_categorical_accuracy: 0.5556\n",
            "Epoch 57/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 3.7890 - categorical_accuracy: 0.6989\n",
            "Epoch 00057: val_categorical_accuracy improved from 0.55556 to 0.57556, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.7910 - categorical_accuracy: 0.6961 - val_loss: 3.8716 - val_categorical_accuracy: 0.5756\n",
            "Epoch 58/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 3.7882 - categorical_accuracy: 0.6892\n",
            "Epoch 00058: val_categorical_accuracy improved from 0.57556 to 0.57778, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.7869 - categorical_accuracy: 0.6894 - val_loss: 3.8634 - val_categorical_accuracy: 0.5778\n",
            "Epoch 59/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.7799 - categorical_accuracy: 0.7017\n",
            "Epoch 00059: val_categorical_accuracy improved from 0.57778 to 0.58667, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.7788 - categorical_accuracy: 0.7022 - val_loss: 3.8535 - val_categorical_accuracy: 0.5867\n",
            "Epoch 60/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.7655 - categorical_accuracy: 0.6938\n",
            "Epoch 00060: val_categorical_accuracy did not improve from 0.58667\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.7658 - categorical_accuracy: 0.6933 - val_loss: 3.8463 - val_categorical_accuracy: 0.5800\n",
            "Epoch 61/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.7518 - categorical_accuracy: 0.7189\n",
            "Epoch 00061: val_categorical_accuracy did not improve from 0.58667\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.7518 - categorical_accuracy: 0.7189 - val_loss: 3.8397 - val_categorical_accuracy: 0.5822\n",
            "Epoch 62/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.7492 - categorical_accuracy: 0.7058\n",
            "Epoch 00062: val_categorical_accuracy did not improve from 0.58667\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.7478 - categorical_accuracy: 0.7083 - val_loss: 3.8306 - val_categorical_accuracy: 0.5867\n",
            "Epoch 63/100\n",
            "83/90 [==========================>...] - ETA: 0s - loss: 3.7454 - categorical_accuracy: 0.7072\n",
            "Epoch 00063: val_categorical_accuracy did not improve from 0.58667\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.7433 - categorical_accuracy: 0.7083 - val_loss: 3.8213 - val_categorical_accuracy: 0.5867\n",
            "Epoch 64/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.7363 - categorical_accuracy: 0.7122\n",
            "Epoch 00064: val_categorical_accuracy improved from 0.58667 to 0.58889, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.7330 - categorical_accuracy: 0.7139 - val_loss: 3.8158 - val_categorical_accuracy: 0.5889\n",
            "Epoch 65/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.7214 - categorical_accuracy: 0.7311\n",
            "Epoch 00065: val_categorical_accuracy did not improve from 0.58889\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.7214 - categorical_accuracy: 0.7311 - val_loss: 3.8074 - val_categorical_accuracy: 0.5889\n",
            "Epoch 66/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.7176 - categorical_accuracy: 0.7300\n",
            "Epoch 00066: val_categorical_accuracy improved from 0.58889 to 0.59333, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.7230 - categorical_accuracy: 0.7267 - val_loss: 3.7960 - val_categorical_accuracy: 0.5933\n",
            "Epoch 67/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 3.7123 - categorical_accuracy: 0.7391\n",
            "Epoch 00067: val_categorical_accuracy improved from 0.59333 to 0.59778, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.7123 - categorical_accuracy: 0.7361 - val_loss: 3.7915 - val_categorical_accuracy: 0.5978\n",
            "Epoch 68/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.6957 - categorical_accuracy: 0.7360\n",
            "Epoch 00068: val_categorical_accuracy improved from 0.59778 to 0.60444, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.6959 - categorical_accuracy: 0.7389 - val_loss: 3.7835 - val_categorical_accuracy: 0.6044\n",
            "Epoch 69/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.6811 - categorical_accuracy: 0.7394\n",
            "Epoch 00069: val_categorical_accuracy improved from 0.60444 to 0.61111, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.6832 - categorical_accuracy: 0.7356 - val_loss: 3.7759 - val_categorical_accuracy: 0.6111\n",
            "Epoch 70/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.6795 - categorical_accuracy: 0.7388\n",
            "Epoch 00070: val_categorical_accuracy did not improve from 0.61111\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.6787 - categorical_accuracy: 0.7406 - val_loss: 3.7699 - val_categorical_accuracy: 0.6022\n",
            "Epoch 71/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 3.6756 - categorical_accuracy: 0.7420\n",
            "Epoch 00071: val_categorical_accuracy did not improve from 0.61111\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.6752 - categorical_accuracy: 0.7422 - val_loss: 3.7630 - val_categorical_accuracy: 0.6022\n",
            "Epoch 72/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.6540 - categorical_accuracy: 0.7624\n",
            "Epoch 00072: val_categorical_accuracy improved from 0.61111 to 0.62222, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.6566 - categorical_accuracy: 0.7567 - val_loss: 3.7524 - val_categorical_accuracy: 0.6222\n",
            "Epoch 73/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 3.6530 - categorical_accuracy: 0.7619\n",
            "Epoch 00073: val_categorical_accuracy did not improve from 0.62222\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.6541 - categorical_accuracy: 0.7611 - val_loss: 3.7499 - val_categorical_accuracy: 0.6044\n",
            "Epoch 74/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.6415 - categorical_accuracy: 0.7640\n",
            "Epoch 00074: val_categorical_accuracy did not improve from 0.62222\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.6438 - categorical_accuracy: 0.7644 - val_loss: 3.7387 - val_categorical_accuracy: 0.6200\n",
            "Epoch 75/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.6347 - categorical_accuracy: 0.7641\n",
            "Epoch 00075: val_categorical_accuracy improved from 0.62222 to 0.62444, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.6391 - categorical_accuracy: 0.7617 - val_loss: 3.7283 - val_categorical_accuracy: 0.6244\n",
            "Epoch 76/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.6373 - categorical_accuracy: 0.7622\n",
            "Epoch 00076: val_categorical_accuracy did not improve from 0.62444\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.6349 - categorical_accuracy: 0.7628 - val_loss: 3.7259 - val_categorical_accuracy: 0.6200\n",
            "Epoch 77/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.6188 - categorical_accuracy: 0.7634\n",
            "Epoch 00077: val_categorical_accuracy improved from 0.62444 to 0.62667, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.6176 - categorical_accuracy: 0.7661 - val_loss: 3.7168 - val_categorical_accuracy: 0.6267\n",
            "Epoch 78/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 3.6157 - categorical_accuracy: 0.7696\n",
            "Epoch 00078: val_categorical_accuracy improved from 0.62667 to 0.63333, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.6145 - categorical_accuracy: 0.7683 - val_loss: 3.7066 - val_categorical_accuracy: 0.6333\n",
            "Epoch 79/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.6014 - categorical_accuracy: 0.7718\n",
            "Epoch 00079: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.6010 - categorical_accuracy: 0.7711 - val_loss: 3.7069 - val_categorical_accuracy: 0.6222\n",
            "Epoch 80/100\n",
            "87/90 [============================>.] - ETA: 0s - loss: 3.5994 - categorical_accuracy: 0.7690\n",
            "Epoch 00080: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.5983 - categorical_accuracy: 0.7700 - val_loss: 3.7002 - val_categorical_accuracy: 0.6267\n",
            "Epoch 81/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.5947 - categorical_accuracy: 0.7733\n",
            "Epoch 00081: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5947 - categorical_accuracy: 0.7733 - val_loss: 3.6989 - val_categorical_accuracy: 0.6178\n",
            "Epoch 82/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.5865 - categorical_accuracy: 0.7647\n",
            "Epoch 00082: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5849 - categorical_accuracy: 0.7667 - val_loss: 3.6925 - val_categorical_accuracy: 0.6222\n",
            "Epoch 83/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.5730 - categorical_accuracy: 0.7741\n",
            "Epoch 00083: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5741 - categorical_accuracy: 0.7733 - val_loss: 3.6793 - val_categorical_accuracy: 0.6311\n",
            "Epoch 84/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 3.5726 - categorical_accuracy: 0.7732\n",
            "Epoch 00084: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5745 - categorical_accuracy: 0.7767 - val_loss: 3.6770 - val_categorical_accuracy: 0.6200\n",
            "Epoch 85/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.5590 - categorical_accuracy: 0.7750\n",
            "Epoch 00085: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5590 - categorical_accuracy: 0.7750 - val_loss: 3.6717 - val_categorical_accuracy: 0.6222\n",
            "Epoch 86/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 3.5522 - categorical_accuracy: 0.7804\n",
            "Epoch 00086: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 8ms/step - loss: 3.5572 - categorical_accuracy: 0.7783 - val_loss: 3.6634 - val_categorical_accuracy: 0.6244\n",
            "Epoch 87/100\n",
            "86/90 [===========================>..] - ETA: 0s - loss: 3.5466 - categorical_accuracy: 0.7890\n",
            "Epoch 00087: val_categorical_accuracy did not improve from 0.63333\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5432 - categorical_accuracy: 0.7922 - val_loss: 3.6591 - val_categorical_accuracy: 0.6289\n",
            "Epoch 88/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 3.5362 - categorical_accuracy: 0.7923\n",
            "Epoch 00088: val_categorical_accuracy improved from 0.63333 to 0.63556, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.5353 - categorical_accuracy: 0.7978 - val_loss: 3.6504 - val_categorical_accuracy: 0.6356\n",
            "Epoch 89/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.5267 - categorical_accuracy: 0.7899\n",
            "Epoch 00089: val_categorical_accuracy did not improve from 0.63556\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5251 - categorical_accuracy: 0.7900 - val_loss: 3.6486 - val_categorical_accuracy: 0.6311\n",
            "Epoch 90/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 3.5292 - categorical_accuracy: 0.7881\n",
            "Epoch 00090: val_categorical_accuracy did not improve from 0.63556\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5299 - categorical_accuracy: 0.7878 - val_loss: 3.6448 - val_categorical_accuracy: 0.6289\n",
            "Epoch 91/100\n",
            "84/90 [===========================>..] - ETA: 0s - loss: 3.5250 - categorical_accuracy: 0.7815\n",
            "Epoch 00091: val_categorical_accuracy did not improve from 0.63556\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5235 - categorical_accuracy: 0.7844 - val_loss: 3.6388 - val_categorical_accuracy: 0.6267\n",
            "Epoch 92/100\n",
            "88/90 [============================>.] - ETA: 0s - loss: 3.5145 - categorical_accuracy: 0.7983\n",
            "Epoch 00092: val_categorical_accuracy improved from 0.63556 to 0.64000, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5127 - categorical_accuracy: 0.7989 - val_loss: 3.6291 - val_categorical_accuracy: 0.6400\n",
            "Epoch 93/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.5115 - categorical_accuracy: 0.7950\n",
            "Epoch 00093: val_categorical_accuracy did not improve from 0.64000\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.5115 - categorical_accuracy: 0.7950 - val_loss: 3.6243 - val_categorical_accuracy: 0.6356\n",
            "Epoch 94/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.4966 - categorical_accuracy: 0.7967\n",
            "Epoch 00094: val_categorical_accuracy did not improve from 0.64000\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.4966 - categorical_accuracy: 0.7967 - val_loss: 3.6176 - val_categorical_accuracy: 0.6311\n",
            "Epoch 95/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.4967 - categorical_accuracy: 0.7994\n",
            "Epoch 00095: val_categorical_accuracy did not improve from 0.64000\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.4961 - categorical_accuracy: 0.7978 - val_loss: 3.6157 - val_categorical_accuracy: 0.6356\n",
            "Epoch 96/100\n",
            "85/90 [===========================>..] - ETA: 0s - loss: 3.4832 - categorical_accuracy: 0.7982\n",
            "Epoch 00096: val_categorical_accuracy improved from 0.64000 to 0.64444, saving model to /content/drive/My Drive/Hyperspectral_Image_Classification/code//Trained_models//full_models/indian_pines_as_source_with_200 samples_from_each_class_in_training_set.h5\n",
            "90/90 [==============================] - 1s 10ms/step - loss: 3.4837 - categorical_accuracy: 0.7956 - val_loss: 3.6101 - val_categorical_accuracy: 0.6444\n",
            "Epoch 97/100\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.4843 - categorical_accuracy: 0.8000\n",
            "Epoch 00097: val_categorical_accuracy did not improve from 0.64444\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.4843 - categorical_accuracy: 0.8000 - val_loss: 3.6080 - val_categorical_accuracy: 0.6289\n",
            "Epoch 98/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.4779 - categorical_accuracy: 0.7961\n",
            "Epoch 00098: val_categorical_accuracy did not improve from 0.64444\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.4802 - categorical_accuracy: 0.7939 - val_loss: 3.6040 - val_categorical_accuracy: 0.6378\n",
            "Epoch 99/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.4564 - categorical_accuracy: 0.8152\n",
            "Epoch 00099: val_categorical_accuracy did not improve from 0.64444\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.4572 - categorical_accuracy: 0.8144 - val_loss: 3.5966 - val_categorical_accuracy: 0.6422\n",
            "Epoch 100/100\n",
            "89/90 [============================>.] - ETA: 0s - loss: 3.4636 - categorical_accuracy: 0.8062\n",
            "Epoch 00100: val_categorical_accuracy did not improve from 0.64444\n",
            "90/90 [==============================] - 1s 9ms/step - loss: 3.4634 - categorical_accuracy: 0.8083 - val_loss: 3.5933 - val_categorical_accuracy: 0.6311\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.5933 - categorical_accuracy: 0.6311\n",
            "Test Accuracy =  0.6311110854148865\n",
            "15/15 [==============================] - 0s 4ms/step\n",
            "Confusion Matrix for Training Set Size 200 [[37 10  0  0  0  0  0  3  0]\n",
            " [ 0 18 24  6  0  0  0  0  2]\n",
            " [ 0  0 35 15  0  0  0  0  0]\n",
            " [ 0  0  2 48  0  0  0  0  0]\n",
            " [ 0  0  0  0 50  0  0  0  0]\n",
            " [ 4 29  0  0  0 10  0  1  6]\n",
            " [ 0  2  0 13 23  8  0  0  4]\n",
            " [ 0 12  0  1  0  0  0 37  0]\n",
            " [ 0  0  1  0  0  0  0  0 49]]\n",
            "Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 20, 20, 64)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_initial (Conv2D)           (None, 10, 10, 64)   36928       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 10, 10, 64)   256         conv_initial[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "re_lu (ReLU)                    (None, 10, 10, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "MaxPooling_0 (MaxPooling2D)     (None, 5, 5, 64)     0           re_lu[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 5, 5, 64)     4160        MaxPooling_0[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 5, 5, 64)     256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 5, 5, 64)     0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 5, 5, 8)      584         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 5, 5, 8)      584         lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 5, 5, 8)      584         lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 5, 5, 8)      584         lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 5, 5, 8)      584         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 5, 5, 8)      584         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 5, 5, 8)      584         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 5, 5, 8)      0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 5, 5, 8)      584         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 5, 5, 8)      584         lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 5, 5, 8)      584         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 5, 5, 8)      584         lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 5, 5, 8)      584         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 5, 5, 8)      584         lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 5, 5, 8)      584         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 5, 5, 8)      584         lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 5, 5, 8)      584         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5, 5, 64)     0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 5, 5, 64)     256         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 5, 5, 64)     0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 5, 5, 128)    8320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 5, 5, 128)    8320        MaxPooling_0[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 5, 5, 128)    512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 5, 5, 128)    512         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 5, 5, 128)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 5, 5, 128)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 5, 5, 128)    0           activation_2[0][0]               \n",
            "                                                                 activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 5, 5, 128)    0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 128)          0           activation_4[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 68,864\n",
            "Trainable params: 67,968\n",
            "Non-trainable params: 896\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDR2MFQDqDdg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}